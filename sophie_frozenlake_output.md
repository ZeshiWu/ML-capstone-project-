Output 1: Average over 100 episodes - Reward: 0.0
-----------------------------
| time/              |      |
|    fps             | 581  |
|    iterations      | 1    |
|    time_elapsed    | 3    |
|    total_timesteps | 2048 |
-----------------------------
Output 2: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 491         |
|    iterations           | 2           |
|    time_elapsed         | 8           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009321687 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -4.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00896     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 0.0146      |
-----------------------------------------
Output 3: Average over 100 episodes - Reward: 0.13
-----------------------------------------
| time/                   |             |
|    fps                  | 510         |
|    iterations           | 3           |
|    time_elapsed         | 12          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012359478 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.0994      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.0199      |
-----------------------------------------
Output 4: Average over 100 episodes - Reward: 0.2
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 4           |
|    time_elapsed         | 15          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.018152526 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0148     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 0.0595      |
-----------------------------------------
Output 5: Average over 100 episodes - Reward: 0.38
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 5           |
|    time_elapsed         | 19          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.017712172 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00747    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0493     |
|    value_loss           | 0.0997      |
-----------------------------------------
Output 6: Average over 100 episodes - Reward: 0.67
----------------------------------------
| time/                   |            |
|    fps                  | 533        |
|    iterations           | 6          |
|    time_elapsed         | 23         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.02646886 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0203    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.051     |
|    value_loss           | 0.126      |
----------------------------------------
Output 7: Average over 100 episodes - Reward: 0.85
-----------------------------------------
| time/                   |             |
|    fps                  | 538         |
|    iterations           | 7           |
|    time_elapsed         | 26          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.030173749 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00837     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.0914      |
-----------------------------------------
Output 8: Average over 100 episodes - Reward: 0.92
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 8           |
|    time_elapsed         | 30          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.016236085 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.751      |
|    explained_variance   | -0.0577     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0125     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.0487      |
-----------------------------------------
Output 9: Average over 100 episodes - Reward: 0.91
-----------------------------------------
| time/                   |             |
|    fps                  | 542         |
|    iterations           | 9           |
|    time_elapsed         | 33          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012887418 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0.075       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00145    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.0418      |
-----------------------------------------
Output 10: Average over 100 episodes - Reward: 0.93
----------------------------------------
| time/                   |            |
|    fps                  | 542        |
|    iterations           | 10         |
|    time_elapsed         | 37         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.01151163 |
|    clip_fraction        | 0.0963     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.549     |
|    explained_variance   | 0.0444     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0119     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.0418     |
----------------------------------------
Output 11: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 11          |
|    time_elapsed         | 41          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011159407 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.451      |
|    explained_variance   | 0.0262      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00837     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.0166      |
-----------------------------------------
Output 12: Average over 100 episodes - Reward: 0.95
-----------------------------------------
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 12          |
|    time_elapsed         | 45          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.024604848 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.338      |
|    explained_variance   | 0.0399      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0109      |
-----------------------------------------
Output 13: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 13          |
|    time_elapsed         | 48          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.011726507 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 0.0269      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0233     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00942    |
|    value_loss           | 0.0197      |
-----------------------------------------
Output 14: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 548         |
|    iterations           | 14          |
|    time_elapsed         | 52          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.014662791 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.251      |
|    explained_variance   | 0.0219      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0229      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 0.00194     |
-----------------------------------------
Output 15: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 15           |
|    time_elapsed         | 55           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0078750625 |
|    clip_fraction        | 0.0819       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.233       |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00718     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 0.00194      |
------------------------------------------
Output 16: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 16           |
|    time_elapsed         | 59           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0025280765 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0176      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00855     |
|    value_loss           | 1.36e-05     |
------------------------------------------
Output 17: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 551         |
|    iterations           | 17          |
|    time_elapsed         | 63          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.009340372 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.102       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00509     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.00319     |
-----------------------------------------
Output 18: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 18          |
|    time_elapsed         | 67          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.003326573 |
|    clip_fraction        | 0.00791     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.013      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 9.63e-06    |
-----------------------------------------
Output 19: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 19           |
|    time_elapsed         | 71           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0065471856 |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0143      |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.0104      |
|    value_loss           | 0.00195      |
------------------------------------------
Output 20: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 20           |
|    time_elapsed         | 74           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0022320712 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00381     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 4.99e-06     |
------------------------------------------
Output 21: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 21           |
|    time_elapsed         | 78           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0010030152 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00476     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 3.66e-07     |
------------------------------------------
Output 22: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 22           |
|    time_elapsed         | 82           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0014223877 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.108       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00171      |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 3.23e-06     |
------------------------------------------
Output 23: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 544           |
|    iterations           | 23            |
|    time_elapsed         | 86            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00095772103 |
|    clip_fraction        | 0.00654       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.11         |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000899     |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.0024       |
|    value_loss           | 1.25e-06      |
-------------------------------------------
Output 24: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 24          |
|    time_elapsed         | 90          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008844548 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0921     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 2.9e-09     |
-----------------------------------------
Output 25: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 542           |
|    iterations           | 25            |
|    time_elapsed         | 94            |
|    total_timesteps      | 51200         |
| train/                  |               |
|    approx_kl            | 0.00085654494 |
|    clip_fraction        | 0.00825       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0848       |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -7.94e-05     |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.00298      |
|    value_loss           | 7.66e-07      |
-------------------------------------------
Output 26: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 26          |
|    time_elapsed         | 98          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.016539123 |
|    clip_fraction        | 0.0735      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0016     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 1.97e-06    |
-----------------------------------------
Output 27: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 27           |
|    time_elapsed         | 101          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0016396644 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00597     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 3.46e-07     |
------------------------------------------
Output 28: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 28           |
|    time_elapsed         | 105          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0045873467 |
|    clip_fraction        | 0.0555       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.114       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0315      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00187     |
|    value_loss           | 9.29e-08     |
------------------------------------------
Output 29: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 29           |
|    time_elapsed         | 109          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0010067772 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000663     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00289     |
|    value_loss           | 1.16e-06     |
------------------------------------------
Output 30: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 30           |
|    time_elapsed         | 113          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0017343315 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000611    |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 1.26e-06     |
------------------------------------------
Output 31: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 31          |
|    time_elapsed         | 117         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.005733989 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0144     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.000258   |
|    value_loss           | 2.68e-07    |
-----------------------------------------
Output 32: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 32          |
|    time_elapsed         | 121         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.009974135 |
|    clip_fraction        | 0.0561      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00219     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 1.07e-06    |
-----------------------------------------
Output 33: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 33           |
|    time_elapsed         | 125          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0006366095 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.142       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00198     |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 5.18e-07     |
------------------------------------------
Output 34: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 34           |
|    time_elapsed         | 128          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0040280893 |
|    clip_fraction        | 0.0763       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.142       |
|    explained_variance   | 0.545        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00438     |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.000672     |
------------------------------------------
Output 35: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 35          |
|    time_elapsed         | 132         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.008923295 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0113      |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 2.71e-06    |
-----------------------------------------
Output 36: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 36           |
|    time_elapsed         | 135          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0057754107 |
|    clip_fraction        | 0.065        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000785     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 2.13e-06     |
------------------------------------------
Output 37: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 37           |
|    time_elapsed         | 139          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0042172717 |
|    clip_fraction        | 0.0571       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00533     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000842    |
|    value_loss           | 1.76e-08     |
------------------------------------------
Output 38: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 542         |
|    iterations           | 38          |
|    time_elapsed         | 143         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.014924299 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0227     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 1.95e-06    |
-----------------------------------------
Output 39: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 542         |
|    iterations           | 39          |
|    time_elapsed         | 147         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.000541655 |
|    clip_fraction        | 0.00693     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0757     |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000895   |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.000469   |
|    value_loss           | 2.34e-08    |
-----------------------------------------
Output 40: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 40           |
|    time_elapsed         | 150          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0007697933 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0815      |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000982    |
|    n_updates            | 390          |
|    policy_gradient_loss | 0.000521     |
|    value_loss           | 3.08e-11     |
------------------------------------------
Output 41: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 41           |
|    time_elapsed         | 154          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0004983704 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0767      |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00109     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 2.3e-07      |
------------------------------------------
Output 42: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 42          |
|    time_elapsed         | 158         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.009262267 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0564     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000162   |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000384   |
|    value_loss           | 1.65e-09    |
-----------------------------------------
Output 43: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 545           |
|    iterations           | 43            |
|    time_elapsed         | 161           |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00062545587 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0469       |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00162      |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.0021       |
|    value_loss           | 1.16e-06      |
-------------------------------------------
Output 44: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 44          |
|    time_elapsed         | 165         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.007503272 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0863     |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00953    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.000134   |
|    value_loss           | 6e-09       |
-----------------------------------------
Output 45: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 45          |
|    time_elapsed         | 168         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.023681324 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0427     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0583     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 1.33e-11    |
-----------------------------------------
Output 46: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 46         |
|    time_elapsed         | 172        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.00057453 |
|    clip_fraction        | 0.00273    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0404    |
|    explained_variance   | 0.135      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000146  |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.00196   |
|    value_loss           | 0.00208    |
----------------------------------------
Output 47: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 548         |
|    iterations           | 47          |
|    time_elapsed         | 175         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.019220963 |
|    clip_fraction        | 0.0439      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.082      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0133      |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.000163   |
|    value_loss           | 3.79e-06    |
-----------------------------------------
Output 48: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 48          |
|    time_elapsed         | 180         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.040310755 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0634     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0417     |
|    value_loss           | 1.75e-08    |
-----------------------------------------
Output 49: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 49          |
|    time_elapsed         | 184         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.004138008 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.166       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00137    |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00586    |
|    value_loss           | 0.00167     |
-----------------------------------------
Output 50: Average over 100 episodes - Reward: 0.8
-----------------------------------------
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 50          |
|    time_elapsed         | 188         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.018567778 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0248     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 2.42e-06    |
-----------------------------------------
Output 51: Average over 100 episodes - Reward: 0.96
---------------------------------------
| time/                   |           |
|    fps                  | 542       |
|    iterations           | 51        |
|    time_elapsed         | 192       |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.0466288 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.179    |
|    explained_variance   | 0.00963   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0216    |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.0407   |
|    value_loss           | 0.109     |
---------------------------------------
Output 52: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 543        |
|    iterations           | 52         |
|    time_elapsed         | 196        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.05229365 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0978    |
|    explained_variance   | -0.157     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00666   |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0276    |
|    value_loss           | 0.0365     |
----------------------------------------
Output 53: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 543        |
|    iterations           | 53         |
|    time_elapsed         | 199        |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.27187753 |
|    clip_fraction        | 0.0982     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0708    |
|    explained_variance   | -2.47      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0838    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0265    |
|    value_loss           | 0.000113   |
----------------------------------------
Output 54: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 54          |
|    time_elapsed         | 203         |
|    total_timesteps      | 110592      |
| train/                  |             |
|    approx_kl            | 0.025883295 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.445      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0629     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0565     |
|    value_loss           | 0.000805    |
-----------------------------------------
Output 55: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 55          |
|    time_elapsed         | 207         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.020221418 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.394      |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0505     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0553     |
|    value_loss           | 0.00048     |
-----------------------------------------
Output 56: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 56          |
|    time_elapsed         | 211         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.015575903 |
|    clip_fraction        | 0.418       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.329      |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0417     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0582     |
|    value_loss           | 0.00024     |
-----------------------------------------
Output 57: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 57         |
|    time_elapsed         | 216        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.01814416 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.257     |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.033     |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.000134   |
----------------------------------------
Output 58: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 538         |
|    iterations           | 58          |
|    time_elapsed         | 220         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.017703665 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.177      |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0498     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0377     |
|    value_loss           | 0.00252     |
-----------------------------------------
Output 59: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 59           |
|    time_elapsed         | 225          |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0018399425 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.826        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000734     |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.0158      |
|    value_loss           | 2.65e-05     |
------------------------------------------
Output 60: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 60          |
|    time_elapsed         | 229         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.072003245 |
|    clip_fraction        | 0.0529      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0666     |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 2.89e-05    |
-----------------------------------------
Output 61: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 61          |
|    time_elapsed         | 235         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.035436414 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0856     |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0396     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 3.73e-06    |
-----------------------------------------
Output 62: Average over 100 episodes - Reward: 0.92
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 62          |
|    time_elapsed         | 241         |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.005337301 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 4.77e-07    |
-----------------------------------------
Output 63: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 523        |
|    iterations           | 63         |
|    time_elapsed         | 246        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.03359917 |
|    clip_fraction        | 0.0811     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0762    |
|    explained_variance   | 0.014      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00914   |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0221    |
|    value_loss           | 0.0437     |
----------------------------------------
Output 64: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 64          |
|    time_elapsed         | 251         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.015275711 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0281     |
|    explained_variance   | -0.257      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00462    |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 0.00392     |
-----------------------------------------
Output 65: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 65            |
|    time_elapsed         | 254           |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.00014866202 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0161       |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.41e-05      |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.000585     |
|    value_loss           | 8.52e-06      |
-------------------------------------------
Output 66: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 66            |
|    time_elapsed         | 258           |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 0.00047223264 |
|    clip_fraction        | 0.00273       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0.665         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000337      |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.00342      |
|    value_loss           | 5.87e-05      |
-------------------------------------------
Output 67: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 67            |
|    time_elapsed         | 262           |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 9.2193455e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00885      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000274      |
|    n_updates            | 660           |
|    policy_gradient_loss | -0.000559     |
|    value_loss           | 3.91e-07      |
-------------------------------------------
Output 68: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 68            |
|    time_elapsed         | 265           |
|    total_timesteps      | 139264        |
| train/                  |               |
|    approx_kl            | 0.00015750702 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00575      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.38e-05     |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.00203      |
|    value_loss           | 6.47e-07      |
-------------------------------------------
Output 69: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 69            |
|    time_elapsed         | 269           |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 2.2470776e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00585      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00144      |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.000336     |
|    value_loss           | 8.55e-10      |
-------------------------------------------
Output 70: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 70           |
|    time_elapsed         | 272          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0017848094 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00687     |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00179     |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 1.03e-07     |
------------------------------------------
Output 71: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 71           |
|    time_elapsed         | 276          |
|    total_timesteps      | 145408       |
| train/                  |              |
|    approx_kl            | 0.0003652146 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0063      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000331     |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00219     |
|    value_loss           | 8.19e-07     |
------------------------------------------
Output 72: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 72            |
|    time_elapsed         | 279           |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 0.00021480443 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00405      |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -7.82e-05     |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000795     |
|    value_loss           | 2.35e-07      |
-------------------------------------------
Output 73: Average over 53 episodes - Reward: 0.9622641509433962
----------------------------------------
| time/                   |            |
|    fps                  | 526        |
|    iterations           | 73         |
|    time_elapsed         | 283        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.45978245 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0698    |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0772    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0336    |
|    value_loss           | 7.4e-10    |
----------------------------------------
Output 74: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 527        |
|    iterations           | 74         |
|    time_elapsed         | 287        |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.12862726 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.461     |
|    explained_variance   | 0.00231    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0174    |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0242    |
|    value_loss           | 0.00268    |
----------------------------------------
Output 75: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 75          |
|    time_elapsed         | 291         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.008243544 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0.117       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 0.00153     |
-----------------------------------------
Output 76: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 76          |
|    time_elapsed         | 294         |
|    total_timesteps      | 155648      |
| train/                  |             |
|    approx_kl            | 0.026573837 |
|    clip_fraction        | 0.438       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0709     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0619     |
|    value_loss           | 0.000553    |
-----------------------------------------
Output 77: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 528        |
|    iterations           | 77         |
|    time_elapsed         | 298        |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.06549874 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.256     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0561    |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0581    |
|    value_loss           | 0.000283   |
----------------------------------------
Output 78: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 78          |
|    time_elapsed         | 302         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.016886093 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0546     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 8.15e-05    |
-----------------------------------------
Output 79: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 527        |
|    iterations           | 79         |
|    time_elapsed         | 306        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.03433241 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0718    |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0251    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.031     |
|    value_loss           | 3.78e-05   |
----------------------------------------
Output 80: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 80          |
|    time_elapsed         | 311         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.027212624 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0219     |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00841    |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 9.51e-06    |
-----------------------------------------
Output 81: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 81            |
|    time_elapsed         | 315           |
|    total_timesteps      | 165888        |
| train/                  |               |
|    approx_kl            | 0.00033224118 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0119       |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00351      |
|    n_updates            | 800           |
|    policy_gradient_loss | -0.000264     |
|    value_loss           | 4.6e-07       |
-------------------------------------------
Output 82: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 82            |
|    time_elapsed         | 319           |
|    total_timesteps      | 167936        |
| train/                  |               |
|    approx_kl            | 0.00073159643 |
|    clip_fraction        | 0.00425       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.28e-05     |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.00504      |
|    value_loss           | 2.08e-06      |
-------------------------------------------
Output 83: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 83           |
|    time_elapsed         | 324          |
|    total_timesteps      | 169984       |
| train/                  |              |
|    approx_kl            | 0.0005519813 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00854     |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000134    |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 1.5e-06      |
------------------------------------------
Output 84: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 84            |
|    time_elapsed         | 327           |
|    total_timesteps      | 172032        |
| train/                  |               |
|    approx_kl            | 0.00032585693 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00666      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000166     |
|    n_updates            | 830           |
|    policy_gradient_loss | -0.00131      |
|    value_loss           | 4.74e-07      |
-------------------------------------------
Output 85: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 85            |
|    time_elapsed         | 331           |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 0.00023629851 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00902      |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0003        |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.00201      |
|    value_loss           | 7.63e-07      |
-------------------------------------------
Output 86: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 86            |
|    time_elapsed         | 335           |
|    total_timesteps      | 176128        |
| train/                  |               |
|    approx_kl            | 0.00044725052 |
|    clip_fraction        | 0.00352       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.356         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000177      |
|    n_updates            | 850           |
|    policy_gradient_loss | -0.00354      |
|    value_loss           | 9.83e-06      |
-------------------------------------------
Output 87: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 87          |
|    time_elapsed         | 339         |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.006098825 |
|    clip_fraction        | 0.00332     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0047     |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 3.06e-07    |
-----------------------------------------
Output 88: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 88            |
|    time_elapsed         | 343           |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 1.7523416e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00401      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000321     |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000201     |
|    value_loss           | 1.69e-09      |
-------------------------------------------
Output 89: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 89          |
|    time_elapsed         | 347         |
|    total_timesteps      | 182272      |
| train/                  |             |
|    approx_kl            | 0.030337682 |
|    clip_fraction        | 0.00693     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0092     |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0372     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00334    |
|    value_loss           | 4.46e-07    |
-----------------------------------------
Output 90: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 90          |
|    time_elapsed         | 351         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.005886345 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.0322      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0191      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 0.0117      |
-----------------------------------------
Output 91: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 91          |
|    time_elapsed         | 355         |
|    total_timesteps      | 186368      |
| train/                  |             |
|    approx_kl            | 0.002097738 |
|    clip_fraction        | 0.016       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.0942      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00513     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 0.00195     |
-----------------------------------------
Output 92: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 92           |
|    time_elapsed         | 359          |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0021182909 |
|    clip_fraction        | 0.0585       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00482     |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 1.88e-06     |
------------------------------------------
Output 93: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 93          |
|    time_elapsed         | 363         |
|    total_timesteps      | 190464      |
| train/                  |             |
|    approx_kl            | 0.003432044 |
|    clip_fraction        | 0.0963      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000398    |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 4.88e-08    |
-----------------------------------------
Output 94: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 94           |
|    time_elapsed         | 367          |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 0.0034878012 |
|    clip_fraction        | 0.0455       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0206       |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 3.32e-11     |
------------------------------------------
Output 95: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 95           |
|    time_elapsed         | 370          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.0014720141 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000471    |
|    n_updates            | 940          |
|    policy_gradient_loss | -3.38e-05    |
|    value_loss           | 2.08e-13     |
------------------------------------------
Output 96: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 96           |
|    time_elapsed         | 374          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 9.533175e-05 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.55e-06     |
|    n_updates            | 950          |
|    policy_gradient_loss | 0.000951     |
|    value_loss           | 2.55e-15     |
------------------------------------------
Output 97: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 97           |
|    time_elapsed         | 378          |
|    total_timesteps      | 198656       |
| train/                  |              |
|    approx_kl            | 0.0028868674 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0119      |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 0.00198      |
------------------------------------------
Output 98: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 98          |
|    time_elapsed         | 381         |
|    total_timesteps      | 200704      |
| train/                  |             |
|    approx_kl            | 0.001927211 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00321    |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 3.13e-06    |
-----------------------------------------
Output 99: Average over 100 episodes - Reward: 0.48
----------------------------------------
| time/                   |            |
|    fps                  | 526        |
|    iterations           | 99         |
|    time_elapsed         | 385        |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.06015517 |
|    clip_fraction        | 0.0983     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.17      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0265    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 2.94e-07   |
----------------------------------------
Output 100: Average over 100 episodes - Reward: 0.81
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 100         |
|    time_elapsed         | 389         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.073347434 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.255      |
|    explained_variance   | 0.00716     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0509      |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0484     |
|    value_loss           | 0.169       |
-----------------------------------------
Output 101: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 526       |
|    iterations           | 101       |
|    time_elapsed         | 392       |
|    total_timesteps      | 206848    |
| train/                  |           |
|    approx_kl            | 0.1024346 |
|    clip_fraction        | 0.173     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.179    |
|    explained_variance   | -0.053    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0124   |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.0421   |
|    value_loss           | 0.0873    |
---------------------------------------
Output 102: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 102          |
|    time_elapsed         | 396          |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 0.0075094206 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | -8.49        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00595     |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00718     |
|    value_loss           | 0.000764     |
------------------------------------------
Output 103: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 103         |
|    time_elapsed         | 400         |
|    total_timesteps      | 210944      |
| train/                  |             |
|    approx_kl            | 0.057779532 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0665     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 7.45e-06    |
-----------------------------------------
Output 104: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 525        |
|    iterations           | 104        |
|    time_elapsed         | 405        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.02890331 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.282     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0271    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.000123   |
----------------------------------------
Output 105: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 105         |
|    time_elapsed         | 409         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.053700514 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 5.52e-05    |
-----------------------------------------
Output 106: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 525       |
|    iterations           | 106       |
|    time_elapsed         | 413       |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.0726821 |
|    clip_fraction        | 0.0688    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.155    |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0315   |
|    n_updates            | 1050      |
|    policy_gradient_loss | -0.0214   |
|    value_loss           | 1.19e-05  |
---------------------------------------
Output 107: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 107         |
|    time_elapsed         | 417         |
|    total_timesteps      | 219136      |
| train/                  |             |
|    approx_kl            | 0.015470378 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 1.05e-06    |
-----------------------------------------
Output 108: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 108          |
|    time_elapsed         | 421          |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 0.0043997588 |
|    clip_fraction        | 0.0753       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.139       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000754    |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00362     |
|    value_loss           | 2.27e-07     |
------------------------------------------
Output 109: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 109          |
|    time_elapsed         | 425          |
|    total_timesteps      | 223232       |
| train/                  |              |
|    approx_kl            | 0.0031100498 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.13        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0126      |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 2.26e-07     |
------------------------------------------
Output 110: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 110         |
|    time_elapsed         | 428         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.001311864 |
|    clip_fraction        | 0.0128      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.0763      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00542     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 0.00387     |
-----------------------------------------
Output 111: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 111          |
|    time_elapsed         | 432          |
|    total_timesteps      | 227328       |
| train/                  |              |
|    approx_kl            | 0.0069368607 |
|    clip_fraction        | 0.0932       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.0436       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0184      |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00788     |
|    value_loss           | 0.00772      |
------------------------------------------
Output 112: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 112          |
|    time_elapsed         | 436          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0014606285 |
|    clip_fraction        | 0.00688      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.86e-05     |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.000102    |
|    value_loss           | 5.26e-06     |
------------------------------------------
Output 113: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 113          |
|    time_elapsed         | 440          |
|    total_timesteps      | 231424       |
| train/                  |              |
|    approx_kl            | 0.0011450573 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0108      |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00228     |
|    value_loss           | 0.00194      |
------------------------------------------
Output 114: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 526        |
|    iterations           | 114        |
|    time_elapsed         | 443        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.02139366 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00143   |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 1.98e-06   |
----------------------------------------
Output 115: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 115          |
|    time_elapsed         | 447          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0016451557 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0208      |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00273     |
|    value_loss           | 1.14e-08     |
------------------------------------------
Output 116: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 116         |
|    time_elapsed         | 451         |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.002748047 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00692     |
|    n_updates            | 1150        |
|    policy_gradient_loss | 0.000257    |
|    value_loss           | 4.73e-11    |
-----------------------------------------
Output 117: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 117          |
|    time_elapsed         | 455          |
|    total_timesteps      | 239616       |
| train/                  |              |
|    approx_kl            | 0.0036054915 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00424     |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00329     |
|    value_loss           | 2.33e-07     |
------------------------------------------
Output 118: Average over 100 episodes - Reward: 0.66
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 118         |
|    time_elapsed         | 459         |
|    total_timesteps      | 241664      |
| train/                  |             |
|    approx_kl            | 0.017215833 |
|    clip_fraction        | 0.081       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0421     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 3.28e-10    |
-----------------------------------------
Output 119: Average over 100 episodes - Reward: 0.93
---------------------------------------
| time/                   |           |
|    fps                  | 524       |
|    iterations           | 119       |
|    time_elapsed         | 464       |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.0496531 |
|    clip_fraction        | 0.239     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.215    |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0419    |
|    n_updates            | 1180      |
|    policy_gradient_loss | -0.0423   |
|    value_loss           | 0.144     |
---------------------------------------
Output 120: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 120         |
|    time_elapsed         | 468         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.051617544 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | -0.149      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00259    |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.0439      |
-----------------------------------------
Output 121: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 524        |
|    iterations           | 121        |
|    time_elapsed         | 472        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.07296798 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.183     |
|    explained_variance   | -0.573     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0727    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 0.00209    |
----------------------------------------
Output 122: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 122         |
|    time_elapsed         | 476         |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.022237718 |
|    clip_fraction        | 0.445       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0574     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0595     |
|    value_loss           | 0.000293    |
-----------------------------------------
Output 123: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 515        |
|    iterations           | 123        |
|    time_elapsed         | 488        |
|    total_timesteps      | 251904     |
| train/                  |            |
|    approx_kl            | 0.03452906 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0958    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0605    |
|    value_loss           | 0.00014    |
----------------------------------------
Output 124: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 124         |
|    time_elapsed         | 494         |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.032406606 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.038      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0501     |
|    value_loss           | 7.32e-05    |
-----------------------------------------
Output 125: Average over 100 episodes - Reward: 0.96
-----------------------------------------
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 125         |
|    time_elapsed         | 498         |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.012753302 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0326     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 2.01e-05    |
-----------------------------------------
Output 126: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 512       |
|    iterations           | 126       |
|    time_elapsed         | 503       |
|    total_timesteps      | 258048    |
| train/                  |           |
|    approx_kl            | 0.0116098 |
|    clip_fraction        | 0.0742    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.19     |
|    explained_variance   | 0.0191    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00943   |
|    n_updates            | 1250      |
|    policy_gradient_loss | -0.00966  |
|    value_loss           | 0.0257    |
---------------------------------------
Output 127: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 512          |
|    iterations           | 127          |
|    time_elapsed         | 507          |
|    total_timesteps      | 260096       |
| train/                  |              |
|    approx_kl            | 0.0072780615 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0.0251       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00247      |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.0053      |
|    value_loss           | 0.00383      |
------------------------------------------
Output 128: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 512        |
|    iterations           | 128        |
|    time_elapsed         | 511        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.00440746 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.209     |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00153   |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.00609   |
|    value_loss           | 1.56e-05   |
----------------------------------------
Output 129: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 512         |
|    iterations           | 129         |
|    time_elapsed         | 515         |
|    total_timesteps      | 264192      |
| train/                  |             |
|    approx_kl            | 0.005655849 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.137       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0328      |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 0.00193     |
-----------------------------------------
Output 130: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 512         |
|    iterations           | 130         |
|    time_elapsed         | 519         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.007954779 |
|    clip_fraction        | 0.0916      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00475    |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 2.76e-06    |
-----------------------------------------
Output 131: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 512          |
|    iterations           | 131          |
|    time_elapsed         | 523          |
|    total_timesteps      | 268288       |
| train/                  |              |
|    approx_kl            | 0.0011284873 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000561    |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 8.29e-07     |
------------------------------------------
Output 132: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 512          |
|    iterations           | 132          |
|    time_elapsed         | 527          |
|    total_timesteps      | 270336       |
| train/                  |              |
|    approx_kl            | 0.0077514625 |
|    clip_fraction        | 0.0508       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.0763       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00126      |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00516     |
|    value_loss           | 0.00387      |
------------------------------------------
Output 133: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 512         |
|    iterations           | 133         |
|    time_elapsed         | 531         |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.023631785 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.174      |
|    explained_variance   | 0.0988      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0283      |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.00195     |
-----------------------------------------
Output 134: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 513        |
|    iterations           | 134        |
|    time_elapsed         | 534        |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.00442267 |
|    clip_fraction        | 0.0668     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0275    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 5.2e-05    |
----------------------------------------
Output 135: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 135         |
|    time_elapsed         | 538         |
|    total_timesteps      | 276480      |
| train/                  |             |
|    approx_kl            | 0.028299801 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0485     |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0438     |
|    value_loss           | 4.2e-05     |
-----------------------------------------
Output 136: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 136         |
|    time_elapsed         | 542         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.051473666 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.209      |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0161     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.00264     |
-----------------------------------------
Output 137: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 514          |
|    iterations           | 137          |
|    time_elapsed         | 545          |
|    total_timesteps      | 280576       |
| train/                  |              |
|    approx_kl            | 0.0061823674 |
|    clip_fraction        | 0.0994       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.777        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00237      |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00552     |
|    value_loss           | 2.68e-06     |
------------------------------------------
Output 138: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 138         |
|    time_elapsed         | 549         |
|    total_timesteps      | 282624      |
| train/                  |             |
|    approx_kl            | 0.015359763 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0248     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 6.79e-07    |
-----------------------------------------
Output 139: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 139         |
|    time_elapsed         | 553         |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.016176555 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.058      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0505     |
|    value_loss           | 4.25e-05    |
-----------------------------------------
Output 140: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 513        |
|    iterations           | 140        |
|    time_elapsed         | 557        |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.07156935 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0642    |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 3.86e-05   |
----------------------------------------
Output 141: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 513          |
|    iterations           | 141          |
|    time_elapsed         | 561          |
|    total_timesteps      | 288768       |
| train/                  |              |
|    approx_kl            | 0.0057819746 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.132       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.016       |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00899     |
|    value_loss           | 8.88e-07     |
------------------------------------------
Output 142: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 142         |
|    time_elapsed         | 565         |
|    total_timesteps      | 290816      |
| train/                  |             |
|    approx_kl            | 0.010374842 |
|    clip_fraction        | 0.0575      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 4.74e-07    |
-----------------------------------------
Output 143: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 514          |
|    iterations           | 143          |
|    time_elapsed         | 569          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0010524376 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0822      |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00543      |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 0.00199      |
------------------------------------------
Output 144: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 144         |
|    time_elapsed         | 573         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.013367552 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0128      |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 3.74e-06    |
-----------------------------------------
Output 145: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 514          |
|    iterations           | 145          |
|    time_elapsed         | 576          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0098536955 |
|    clip_fraction        | 0.0721       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0352       |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 1.75e-08     |
------------------------------------------
Output 146: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 146         |
|    time_elapsed         | 580         |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.023273146 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0684     |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00533     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 0.00197     |
-----------------------------------------
Output 147: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 147         |
|    time_elapsed         | 584         |
|    total_timesteps      | 301056      |
| train/                  |             |
|    approx_kl            | 0.058688648 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00273     |
|    n_updates            | 1460        |
|    policy_gradient_loss | 0.0235      |
|    value_loss           | 0.00189     |
-----------------------------------------
Output 148: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 148         |
|    time_elapsed         | 588         |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.005878003 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | -1.84       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0129     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00794    |
|    value_loss           | 7.43e-05    |
-----------------------------------------
Output 149: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 515       |
|    iterations           | 149       |
|    time_elapsed         | 592       |
|    total_timesteps      | 305152    |
| train/                  |           |
|    approx_kl            | 0.0434833 |
|    clip_fraction        | 0.0756    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.109    |
|    explained_variance   | 0.89      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.018    |
|    n_updates            | 1480      |
|    policy_gradient_loss | -0.0207   |
|    value_loss           | 3.05e-05  |
---------------------------------------
Output 150: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 150         |
|    time_elapsed         | 595         |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.038209245 |
|    clip_fraction        | 0.0361      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0455     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0173     |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 4.33e-06    |
-----------------------------------------
Output 151: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 151         |
|    time_elapsed         | 599         |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.012694922 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0579     |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.00196     |
-----------------------------------------
Output 152: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 515          |
|    iterations           | 152          |
|    time_elapsed         | 603          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0024129115 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.673        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0251      |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 3.33e-05     |
------------------------------------------
Output 153: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 515       |
|    iterations           | 153       |
|    time_elapsed         | 607       |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0536049 |
|    clip_fraction        | 0.198     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0344   |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0364   |
|    n_updates            | 1520      |
|    policy_gradient_loss | -0.0328   |
|    value_loss           | 2.87e-05  |
---------------------------------------
Output 154: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 516         |
|    iterations           | 154         |
|    time_elapsed         | 610         |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.011702733 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00945    |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00619    |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00987    |
|    value_loss           | 3.33e-06    |
-----------------------------------------
Output 155: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 516          |
|    iterations           | 155          |
|    time_elapsed         | 614          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0011227893 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00828     |
|    explained_variance   | 0.319        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000112    |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 0.00095      |
------------------------------------------
Output 156: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 156         |
|    time_elapsed         | 617         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.012612118 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0818     |
|    explained_variance   | 0.0606      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.083      |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.00549     |
-----------------------------------------
Output 157: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 157         |
|    time_elapsed         | 621         |
|    total_timesteps      | 321536      |
| train/                  |             |
|    approx_kl            | 0.030372893 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000454   |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 2.68e-05    |
-----------------------------------------
Output 158: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 158         |
|    time_elapsed         | 625         |
|    total_timesteps      | 323584      |
| train/                  |             |
|    approx_kl            | 0.039923433 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0472     |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00669    |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.000433    |
-----------------------------------------
Output 159: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 159          |
|    time_elapsed         | 629          |
|    total_timesteps      | 325632       |
| train/                  |              |
|    approx_kl            | 0.0033517224 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0248      |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00273     |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 2.86e-06     |
------------------------------------------
Output 160: Average over 100 episodes - Reward: 0.45
-----------------------------------------
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 160         |
|    time_elapsed         | 632         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.067208424 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0376     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0247     |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 1.16e-06    |
-----------------------------------------
Output 161: Average over 100 episodes - Reward: 0.82
----------------------------------------
| time/                   |            |
|    fps                  | 517        |
|    iterations           | 161        |
|    time_elapsed         | 636        |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.08026756 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.00569    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0441     |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.174      |
----------------------------------------
Output 162: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 517        |
|    iterations           | 162        |
|    time_elapsed         | 640        |
|    total_timesteps      | 331776     |
| train/                  |            |
|    approx_kl            | 0.20818406 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | -0.102     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00212    |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.106      |
----------------------------------------
Output 163: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 163          |
|    time_elapsed         | 644          |
|    total_timesteps      | 333824       |
| train/                  |              |
|    approx_kl            | 0.0045146616 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.262       |
|    explained_variance   | -11.9        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00199     |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.000936    |
|    value_loss           | 0.00113      |
------------------------------------------
Output 164: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 164         |
|    time_elapsed         | 648         |
|    total_timesteps      | 335872      |
| train/                  |             |
|    approx_kl            | 0.018600766 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.397       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0233      |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 8e-05       |
-----------------------------------------
Output 165: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 165        |
|    time_elapsed         | 651        |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.06472302 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0767    |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0532    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0428    |
|    value_loss           | 4.71e-05   |
----------------------------------------
Output 166: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 166         |
|    time_elapsed         | 655         |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.008217967 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0377     |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0287     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 2.86e-06    |
-----------------------------------------
Output 167: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 167          |
|    time_elapsed         | 659          |
|    total_timesteps      | 342016       |
| train/                  |              |
|    approx_kl            | 0.0057613505 |
|    clip_fraction        | 0.0083       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0186      |
|    explained_variance   | 0.0765       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00673     |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.00304     |
|    value_loss           | 0.00381      |
------------------------------------------
Output 168: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 168         |
|    time_elapsed         | 663         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.031996954 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0917     |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0551     |
|    n_updates            | 1670        |
|    policy_gradient_loss | 0.0921      |
|    value_loss           | 1.16e-06    |
-----------------------------------------
Output 169: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 169         |
|    time_elapsed         | 666         |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.056223758 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0553     |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0337     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 4.42e-05    |
-----------------------------------------
Output 170: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 170        |
|    time_elapsed         | 671        |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.01946678 |
|    clip_fraction        | 0.00762    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00927   |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0296    |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 3.95e-06   |
----------------------------------------
Output 171: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 171         |
|    time_elapsed         | 675         |
|    total_timesteps      | 350208      |
| train/                  |             |
|    approx_kl            | 0.007025689 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.041      |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0134     |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00193     |
-----------------------------------------
Output 172: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 172         |
|    time_elapsed         | 678         |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.093694836 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0139     |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 2.37e-05    |
-----------------------------------------
Output 173: Average over 47 episodes - Reward: 0.9574468085106383
----------------------------------------
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 173        |
|    time_elapsed         | 682        |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.50095356 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0874    |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0696    |
|    value_loss           | 1.04e-07   |
----------------------------------------
Output 174: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 174         |
|    time_elapsed         | 685         |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.119256936 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.0173      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.00254     |
-----------------------------------------
Output 175: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 175         |
|    time_elapsed         | 689         |
|    total_timesteps      | 358400      |
| train/                  |             |
|    approx_kl            | 0.014967771 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0369     |
|    value_loss           | 0.00258     |
-----------------------------------------
Output 176: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 520        |
|    iterations           | 176        |
|    time_elapsed         | 692        |
|    total_timesteps      | 360448     |
| train/                  |            |
|    approx_kl            | 0.02341931 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0637    |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.000743   |
----------------------------------------
Output 177: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 177         |
|    time_elapsed         | 696         |
|    total_timesteps      | 362496      |
| train/                  |             |
|    approx_kl            | 0.018157467 |
|    clip_fraction        | 0.403       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.327      |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0803     |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0584     |
|    value_loss           | 0.000246    |
-----------------------------------------
Output 178: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 520        |
|    iterations           | 178        |
|    time_elapsed         | 700        |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.04281885 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.224     |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0611    |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.00013    |
----------------------------------------
Output 179: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 179         |
|    time_elapsed         | 703         |
|    total_timesteps      | 366592      |
| train/                  |             |
|    approx_kl            | 0.040553372 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0546     |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.0428     |
|    value_loss           | 3.24e-05    |
-----------------------------------------
Output 180: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 180          |
|    time_elapsed         | 707          |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.0037690555 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0741      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.028       |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 3.42e-06     |
------------------------------------------
Output 181: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 181         |
|    time_elapsed         | 711         |
|    total_timesteps      | 370688      |
| train/                  |             |
|    approx_kl            | 0.024535496 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0248     |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00238    |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 3.27e-06    |
-----------------------------------------
Output 182: Average over 100 episodes - Reward: 0.3
----------------------------------------
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 182        |
|    time_elapsed         | 714        |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.12737618 |
|    clip_fraction        | 0.0549     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0406    |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0159    |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 4.64e-07   |
----------------------------------------
Output 183: Average over 88 episodes - Reward: 0.6477272727272727
----------------------------------------
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 183        |
|    time_elapsed         | 718        |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.12885761 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.0166     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0194     |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.134      |
----------------------------------------
Output 184: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 184        |
|    time_elapsed         | 722        |
|    total_timesteps      | 376832     |
| train/                  |            |
|    approx_kl            | 0.31469417 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.0598     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.112     |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.0252     |
----------------------------------------
Output 185: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 185        |
|    time_elapsed         | 726        |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.06563662 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.156      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0479    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.00599    |
----------------------------------------
Output 186: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 186          |
|    time_elapsed         | 730          |
|    total_timesteps      | 380928       |
| train/                  |              |
|    approx_kl            | 0.0072224806 |
|    clip_fraction        | 0.213        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | -0.178       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.022       |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.0243      |
|    value_loss           | 0.00018      |
------------------------------------------
Output 187: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 187         |
|    time_elapsed         | 734         |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.009987278 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.175       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0548     |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.00167     |
-----------------------------------------
Output 188: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 520        |
|    iterations           | 188        |
|    time_elapsed         | 739        |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.03174333 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0662    |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0514    |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.026     |
|    value_loss           | 3.35e-05   |
----------------------------------------
Output 189: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 520        |
|    iterations           | 189        |
|    time_elapsed         | 743        |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.03383645 |
|    clip_fraction        | 0.0181     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0169    |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0045    |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.0016     |
----------------------------------------
Output 190: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 190           |
|    time_elapsed         | 748           |
|    total_timesteps      | 389120        |
| train/                  |               |
|    approx_kl            | 0.00065978325 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00635      |
|    explained_variance   | -0.0257       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000548     |
|    n_updates            | 1890          |
|    policy_gradient_loss | -0.0036       |
|    value_loss           | 0.00341       |
-------------------------------------------
Output 191: Average over 38 episodes - Reward: 0.6842105263157895
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 191         |
|    time_elapsed         | 752         |
|    total_timesteps      | 391168      |
| train/                  |             |
|    approx_kl            | 0.113623604 |
|    clip_fraction        | 0.0163      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0147     |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0364     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 2e-06       |
-----------------------------------------
Output 192: Average over 84 episodes - Reward: 0.9880952380952381
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 192         |
|    time_elapsed         | 757         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.021599311 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | -0.351      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0433     |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.0151      |
-----------------------------------------
Output 193: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 193         |
|    time_elapsed         | 761         |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.011709952 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0568     |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.015       |
-----------------------------------------
Output 194: Average over 100 episodes - Reward: 0.38
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 194         |
|    time_elapsed         | 766         |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.030335478 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.58       |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0489     |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.0114      |
-----------------------------------------
Output 195: Average over 100 episodes - Reward: 0.84
----------------------------------------
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 195        |
|    time_elapsed         | 770        |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.05961606 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.511     |
|    explained_variance   | -0.0407    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0263     |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.03      |
|    value_loss           | 0.13       |
----------------------------------------
Output 196: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 196         |
|    time_elapsed         | 773         |
|    total_timesteps      | 401408      |
| train/                  |             |
|    approx_kl            | 0.052203607 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | -0.0343     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0252     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.0616      |
-----------------------------------------
Output 197: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 197         |
|    time_elapsed         | 777         |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.009525564 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.0852      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0538     |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.00929     |
-----------------------------------------
Output 198: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 198         |
|    time_elapsed         | 781         |
|    total_timesteps      | 405504      |
| train/                  |             |
|    approx_kl            | 0.012403684 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | -0.0377     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0432     |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 0.00117     |
-----------------------------------------
Output 199: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 199         |
|    time_elapsed         | 786         |
|    total_timesteps      | 407552      |
| train/                  |             |
|    approx_kl            | 0.092901215 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.201       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0548     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0422     |
|    value_loss           | 0.00258     |
-----------------------------------------
Output 200: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 200         |
|    time_elapsed         | 790         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.002041428 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0261     |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 4.05e-05    |
-----------------------------------------
Output 201: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 201         |
|    time_elapsed         | 795         |
|    total_timesteps      | 411648      |
| train/                  |             |
|    approx_kl            | 0.036064602 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.046      |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0435     |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.00195     |
-----------------------------------------
Output 202: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 202           |
|    time_elapsed         | 799           |
|    total_timesteps      | 413696        |
| train/                  |               |
|    approx_kl            | 0.00060761574 |
|    clip_fraction        | 0.00503       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0309       |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0089       |
|    n_updates            | 2010          |
|    policy_gradient_loss | -0.00274      |
|    value_loss           | 5.92e-06      |
-------------------------------------------
Output 203: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 203           |
|    time_elapsed         | 803           |
|    total_timesteps      | 415744        |
| train/                  |               |
|    approx_kl            | 0.00027976814 |
|    clip_fraction        | 0.0064        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0191       |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0296       |
|    n_updates            | 2020          |
|    policy_gradient_loss | -0.00634      |
|    value_loss           | 3.84e-06      |
-------------------------------------------
Output 204: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 204           |
|    time_elapsed         | 807           |
|    total_timesteps      | 417792        |
| train/                  |               |
|    approx_kl            | 0.00028694046 |
|    clip_fraction        | 0.00337       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00135       |
|    n_updates            | 2030          |
|    policy_gradient_loss | -0.00402      |
|    value_loss           | 1.78e-06      |
-------------------------------------------
Output 205: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 205          |
|    time_elapsed         | 811          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.0050301924 |
|    clip_fraction        | 0.00288      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00727     |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000892    |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00544     |
|    value_loss           | 1.34e-06     |
------------------------------------------
Output 206: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 206           |
|    time_elapsed         | 814           |
|    total_timesteps      | 421888        |
| train/                  |               |
|    approx_kl            | 0.00019012712 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00494      |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000116     |
|    n_updates            | 2050          |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 7.7e-07       |
-------------------------------------------
Output 207: Average over 100 episodes - Reward: 0.15
----------------------------------------
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 207        |
|    time_elapsed         | 818        |
|    total_timesteps      | 423936     |
| train/                  |            |
|    approx_kl            | 0.17273842 |
|    clip_fraction        | 0.0612     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0322    |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0339    |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 6.98e-10   |
----------------------------------------
Output 208: Average over 100 episodes - Reward: 0.72
----------------------------------------
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 208        |
|    time_elapsed         | 821        |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.22339553 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.0149     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0114     |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.114      |
----------------------------------------
Output 209: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 518       |
|    iterations           | 209       |
|    time_elapsed         | 825       |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.3540065 |
|    clip_fraction        | 0.252     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0543   |
|    explained_variance   | -0.253    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.016    |
|    n_updates            | 2080      |
|    policy_gradient_loss | -0.0562   |
|    value_loss           | 0.112     |
---------------------------------------
Output 210: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 210         |
|    time_elapsed         | 829         |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.013122258 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.273      |
|    explained_variance   | -12.7       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00932    |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 0.002       |
-----------------------------------------
Output 211: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 211        |
|    time_elapsed         | 832        |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.07171131 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.174     |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0409    |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0409    |
|    value_loss           | 0.0002     |
----------------------------------------
Output 212: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 212         |
|    time_elapsed         | 836         |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 0.046704784 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0409     |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.036      |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 3.47e-05    |
-----------------------------------------
Output 213: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 213          |
|    time_elapsed         | 840          |
|    total_timesteps      | 436224       |
| train/                  |              |
|    approx_kl            | 0.0017617152 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0236      |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000457     |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00879     |
|    value_loss           | 4.9e-06      |
------------------------------------------
Output 214: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 214          |
|    time_elapsed         | 844          |
|    total_timesteps      | 438272       |
| train/                  |              |
|    approx_kl            | 0.0011949928 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0202      |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00015     |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.005       |
|    value_loss           | 0.00159      |
------------------------------------------
Output 215: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 215           |
|    time_elapsed         | 848           |
|    total_timesteps      | 440320        |
| train/                  |               |
|    approx_kl            | 0.00027694518 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000321      |
|    n_updates            | 2140          |
|    policy_gradient_loss | -0.00154      |
|    value_loss           | 0.00159       |
-------------------------------------------
Output 216: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 216        |
|    time_elapsed         | 851        |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.11412075 |
|    clip_fraction        | 0.0736     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0561    |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0231    |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 1.46e-06   |
----------------------------------------
Output 217: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 217        |
|    time_elapsed         | 855        |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.05501329 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.391     |
|    explained_variance   | 0.0711     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0583    |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.00124    |
----------------------------------------
Output 218: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 218         |
|    time_elapsed         | 859         |
|    total_timesteps      | 446464      |
| train/                  |             |
|    approx_kl            | 0.018322106 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0367     |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.0437     |
|    value_loss           | 0.000443    |
-----------------------------------------
Output 219: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 219        |
|    time_elapsed         | 862        |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.00960817 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.24      |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0364    |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.00022    |
----------------------------------------
Output 220: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 220         |
|    time_elapsed         | 866         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.008419463 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0294     |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0473     |
|    value_loss           | 7.7e-05     |
-----------------------------------------
Output 221: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 221         |
|    time_elapsed         | 869         |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.031732537 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.054      |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0441     |
|    value_loss           | 3.79e-05    |
-----------------------------------------
Output 222: Average over 100 episodes - Reward: 0.76
----------------------------------------
| time/                   |            |
|    fps                  | 520        |
|    iterations           | 222        |
|    time_elapsed         | 873        |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.09576306 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0693    |
|    explained_variance   | 0.165      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0575    |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.033     |
|    value_loss           | 0.00161    |
----------------------------------------
Output 223: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 223         |
|    time_elapsed         | 876         |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.083250135 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0174     |
|    explained_variance   | 0.0119      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0143      |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0292     |
|    value_loss           | 0.114       |
-----------------------------------------
Output 224: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 224          |
|    time_elapsed         | 880          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0048896396 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00953     |
|    explained_variance   | -0.367       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00142     |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.0035      |
|    value_loss           | 0.00811      |
------------------------------------------
Output 225: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 225          |
|    time_elapsed         | 884          |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 4.186353e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00664     |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000967    |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 1.22e-05     |
------------------------------------------
Output 226: Average over 100 episodes - Reward: 0.98
-------------------------------------------
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 226           |
|    time_elapsed         | 887           |
|    total_timesteps      | 462848        |
| train/                  |               |
|    approx_kl            | 0.00072316977 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00782      |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00481       |
|    n_updates            | 2250          |
|    policy_gradient_loss | 0.00131       |
|    value_loss           | 0.00191       |
-------------------------------------------
Output 227: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 227           |
|    time_elapsed         | 891           |
|    total_timesteps      | 464896        |
| train/                  |               |
|    approx_kl            | 0.00010110959 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00986      |
|    explained_variance   | 0.057         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.012         |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.00156      |
|    value_loss           | 0.00569       |
-------------------------------------------
Output 228: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 228        |
|    time_elapsed         | 894        |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.13767052 |
|    clip_fraction        | 0.0966     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0617    |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 1.46e-06   |
----------------------------------------
Output 229: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 229         |
|    time_elapsed         | 898         |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.023256809 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.052       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0482     |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.00164     |
-----------------------------------------
Output 230: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 230         |
|    time_elapsed         | 901         |
|    total_timesteps      | 471040      |
| train/                  |             |
|    approx_kl            | 0.009908862 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.399      |
|    explained_variance   | 0.196       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.00108     |
-----------------------------------------
Output 231: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 231         |
|    time_elapsed         | 905         |
|    total_timesteps      | 473088      |
| train/                  |             |
|    approx_kl            | 0.014651705 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.277      |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0534     |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.000547    |
-----------------------------------------
Output 232: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 232         |
|    time_elapsed         | 909         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.014023146 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0406     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0428     |
|    value_loss           | 0.000143    |
-----------------------------------------
Output 233: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 233         |
|    time_elapsed         | 912         |
|    total_timesteps      | 477184      |
| train/                  |             |
|    approx_kl            | 0.024725547 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0865     |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.00199     |
-----------------------------------------
Output 234: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 523       |
|    iterations           | 234       |
|    time_elapsed         | 916       |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.0339495 |
|    clip_fraction        | 0.0158    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0164   |
|    explained_variance   | 0.921     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0261   |
|    n_updates            | 2330      |
|    policy_gradient_loss | -0.0169   |
|    value_loss           | 2.29e-05  |
---------------------------------------
Output 235: Average over 82 episodes - Reward: 0.1951219512195122
----------------------------------------
| time/                   |            |
|    fps                  | 523        |
|    iterations           | 235        |
|    time_elapsed         | 919        |
|    total_timesteps      | 481280     |
| train/                  |            |
|    approx_kl            | 0.34094393 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0128    |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 1.19e-06   |
----------------------------------------
Output 236: Average over 59 episodes - Reward: 0.5084745762711864
-----------------------------------------
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 236         |
|    time_elapsed         | 923         |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.014196388 |
|    clip_fraction        | 0.0676      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | -0.0953     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0257      |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.0715      |
-----------------------------------------
Output 237: Average over 49 episodes - Reward: 0.7755102040816326
-----------------------------------------
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 237         |
|    time_elapsed         | 926         |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.008490555 |
|    clip_fraction        | 0.0693      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | -0.0491     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0241      |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.0515      |
-----------------------------------------
Output 238: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 238         |
|    time_elapsed         | 929         |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.040982686 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.292      |
|    explained_variance   | 0.181       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 0.0351      |
-----------------------------------------
Output 239: Average over 100 episodes - Reward: 0.43
----------------------------------------
| time/                   |            |
|    fps                  | 524        |
|    iterations           | 239        |
|    time_elapsed         | 932        |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.03552676 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.284     |
|    explained_variance   | 0.0269     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0121    |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.0188     |
----------------------------------------
Output 240: Average over 100 episodes - Reward: 0.61
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 240          |
|    time_elapsed         | 936          |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0068540196 |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.304       |
|    explained_variance   | -0.0318      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.049        |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.0271      |
|    value_loss           | 0.138        |
------------------------------------------
Output 241: Average over 100 episodes - Reward: 0.86
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 241         |
|    time_elapsed         | 939         |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.015234603 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.284      |
|    explained_variance   | 0.0708      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0507      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.028      |
|    value_loss           | 0.132       |
-----------------------------------------
Output 242: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 242         |
|    time_elapsed         | 942         |
|    total_timesteps      | 495616      |
| train/                  |             |
|    approx_kl            | 0.059879962 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.308      |
|    explained_variance   | 0.0305      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00366     |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0364     |
|    value_loss           | 0.0673      |
-----------------------------------------
Output 243: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 243         |
|    time_elapsed         | 946         |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.035169095 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.389      |
|    explained_variance   | -0.29       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0387     |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.00454     |
-----------------------------------------
Output 244: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 526        |
|    iterations           | 244        |
|    time_elapsed         | 949        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.01202821 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.4       |
|    explained_variance   | 0.0801     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0181    |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.0367    |
|    value_loss           | 0.00285    |
----------------------------------------
Output 245: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 245         |
|    time_elapsed         | 953         |
|    total_timesteps      | 501760      |
| train/                  |             |
|    approx_kl            | 0.023337647 |
|    clip_fraction        | 0.377       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.314      |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0695     |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0484     |
|    value_loss           | 0.00186     |
-----------------------------------------
Overall: Average Reward: 0.9463636363636364
Using cpu device
Logging to ./PPOtensorboard/PPO_2
Output 1: Average over 100 episodes - Reward: 0.02
-----------------------------
| time/              |      |
|    fps             | 910  |
|    iterations      | 1    |
|    time_elapsed    | 2    |
|    total_timesteps | 2048 |
-----------------------------
Output 2: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 637         |
|    iterations           | 2           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011759092 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -1.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0204      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 0.0152      |
-----------------------------------------
Output 3: Average over 100 episodes - Reward: 0.01
-----------------------------------------
| time/                   |             |
|    fps                  | 604         |
|    iterations           | 3           |
|    time_elapsed         | 10          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.018131863 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.127       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0244     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.022       |
-----------------------------------------
Output 4: Average over 100 episodes - Reward: 0.03
-----------------------------------------
| time/                   |             |
|    fps                  | 589         |
|    iterations           | 4           |
|    time_elapsed         | 13          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.013373062 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.17        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0201     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0081     |
|    value_loss           | 0.0177      |
-----------------------------------------
Output 5: Average over 100 episodes - Reward: 0.04
-----------------------------------------
| time/                   |             |
|    fps                  | 584         |
|    iterations           | 5           |
|    time_elapsed         | 17          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.014359696 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00103    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.0189      |
-----------------------------------------
Output 6: Average over 100 episodes - Reward: 0.05
-----------------------------------------
| time/                   |             |
|    fps                  | 567         |
|    iterations           | 6           |
|    time_elapsed         | 21          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.008946655 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.209       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00817    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00636    |
|    value_loss           | 0.0218      |
-----------------------------------------
Output 7: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 560         |
|    iterations           | 7           |
|    time_elapsed         | 25          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009846981 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00539    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.0366      |
-----------------------------------------
Output 8: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 8           |
|    time_elapsed         | 30          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.013054941 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.0709      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.0176      |
-----------------------------------------
Output 9: Average over 100 episodes - Reward: 0.05
-----------------------------------------
| time/                   |             |
|    fps                  | 542         |
|    iterations           | 9           |
|    time_elapsed         | 33          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.007603651 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 0.0203      |
-----------------------------------------
Output 10: Average over 100 episodes - Reward: 0.13
-----------------------------------------
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 10          |
|    time_elapsed         | 37          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.008516686 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.194       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.016       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 0.0378      |
-----------------------------------------
Output 11: Average over 100 episodes - Reward: 0.08
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 11          |
|    time_elapsed         | 41          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.013390456 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.942      |
|    explained_variance   | 0.132       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00452    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.0655      |
-----------------------------------------
Output 12: Average over 100 episodes - Reward: 0.16
-----------------------------------------
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 12          |
|    time_elapsed         | 45          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.006245424 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0111      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 0.0393      |
-----------------------------------------
Output 13: Average over 100 episodes - Reward: 0.13
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 13          |
|    time_elapsed         | 49          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.009041596 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.787      |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0235      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00892    |
|    value_loss           | 0.0541      |
-----------------------------------------
Output 14: Average over 100 episodes - Reward: 0.13
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 14          |
|    time_elapsed         | 52          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.008766474 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.705      |
|    explained_variance   | -0.0146     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.058       |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00669    |
|    value_loss           | 0.0599      |
-----------------------------------------
Output 15: Average over 100 episodes - Reward: 0.15
------------------------------------------
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 15           |
|    time_elapsed         | 56           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0044650272 |
|    clip_fraction        | 0.0604       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.637       |
|    explained_variance   | 0.0925       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0254       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00673     |
|    value_loss           | 0.0458       |
------------------------------------------
Output 16: Average over 100 episodes - Reward: 0.19
------------------------------------------
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 16           |
|    time_elapsed         | 60           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0065571903 |
|    clip_fraction        | 0.064        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0187       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0065      |
|    value_loss           | 0.0544       |
------------------------------------------
Output 17: Average over 96 episodes - Reward: 0.2916666666666667
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 17          |
|    time_elapsed         | 63          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.003240789 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.0775      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0235      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 0.0571      |
-----------------------------------------
Output 18: Average over 98 episodes - Reward: 0.15306122448979592
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 18          |
|    time_elapsed         | 67          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.002738592 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.502      |
|    explained_variance   | 0.154       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0158      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 0.0643      |
-----------------------------------------
Output 19: Average over 86 episodes - Reward: 0.23255813953488372
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 19          |
|    time_elapsed         | 70          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.013955073 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.09        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000393    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 0.0478      |
-----------------------------------------
Output 20: Average over 81 episodes - Reward: 0.32098765432098764
-----------------------------------------
| time/                   |             |
|    fps                  | 555         |
|    iterations           | 20          |
|    time_elapsed         | 73          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.002251668 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0185      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 0.054       |
-----------------------------------------
Output 21: Average over 77 episodes - Reward: 0.4155844155844156
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 21           |
|    time_elapsed         | 77           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0032955778 |
|    clip_fraction        | 0.0442       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.324       |
|    explained_variance   | 0.0584       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0231       |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00482     |
|    value_loss           | 0.0676       |
------------------------------------------
Output 22: Average over 77 episodes - Reward: 0.45454545454545453
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 22           |
|    time_elapsed         | 81           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0017685595 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.0995       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0294       |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00348     |
|    value_loss           | 0.0685       |
------------------------------------------
Output 23: Average over 69 episodes - Reward: 0.42028985507246375
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 23           |
|    time_elapsed         | 85           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0019462605 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.262       |
|    explained_variance   | 0.113        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0146       |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00391     |
|    value_loss           | 0.0703       |
------------------------------------------
Output 24: Average over 59 episodes - Reward: 0.3220338983050847
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 24          |
|    time_elapsed         | 88          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.001574968 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0292      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 0.0605      |
-----------------------------------------
Output 25: Average over 67 episodes - Reward: 0.3582089552238806
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 25           |
|    time_elapsed         | 92           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0015776232 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.0778       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0427       |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.0477       |
------------------------------------------
Output 26: Average over 53 episodes - Reward: 0.5471698113207547
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 26           |
|    time_elapsed         | 96           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0019995533 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.0581       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00982      |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.0054      |
|    value_loss           | 0.0589       |
------------------------------------------
Output 27: Average over 73 episodes - Reward: 0.3698630136986301
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 27           |
|    time_elapsed         | 100          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0014885871 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0165       |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.0535       |
------------------------------------------
Output 28: Average over 59 episodes - Reward: 0.5254237288135594
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 28           |
|    time_elapsed         | 104          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0025128606 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0.124        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0103       |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00444     |
|    value_loss           | 0.0602       |
------------------------------------------
Output 29: Average over 62 episodes - Reward: 0.45161290322580644
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 29           |
|    time_elapsed         | 107          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0017638915 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0.1          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0138       |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00296     |
|    value_loss           | 0.0612       |
------------------------------------------
Output 30: Average over 58 episodes - Reward: 0.46551724137931033
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 30           |
|    time_elapsed         | 111          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0018271065 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.0961       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0187       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 0.0579       |
------------------------------------------
Output 31: Average over 58 episodes - Reward: 0.5689655172413793
------------------------------------------
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 31           |
|    time_elapsed         | 115          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0017547107 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0.07         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0136       |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0027      |
|    value_loss           | 0.0558       |
------------------------------------------
Output 32: Average over 55 episodes - Reward: 0.43636363636363634
------------------------------------------
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 32           |
|    time_elapsed         | 120          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0011948969 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0316       |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00311     |
|    value_loss           | 0.056        |
------------------------------------------
Output 33: Average over 55 episodes - Reward: 0.5636363636363636
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 33           |
|    time_elapsed         | 125          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0018023378 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.0757       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0327       |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 0.0509       |
------------------------------------------
Output 34: Average over 54 episodes - Reward: 0.5555555555555556
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 34           |
|    time_elapsed         | 129          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0016387149 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0402       |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 0.0509       |
------------------------------------------
Output 35: Average over 50 episodes - Reward: 0.44
------------------------------------------
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 35           |
|    time_elapsed         | 133          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0015316681 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.0855       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0182       |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00338     |
|    value_loss           | 0.0544       |
------------------------------------------
Output 36: Average over 55 episodes - Reward: 0.41818181818181815
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 36           |
|    time_elapsed         | 137          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0012915332 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00763      |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 0.0442       |
------------------------------------------
Output 37: Average over 52 episodes - Reward: 0.5
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 37           |
|    time_elapsed         | 141          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0021371394 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0238       |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00388     |
|    value_loss           | 0.0479       |
------------------------------------------
Output 38: Average over 67 episodes - Reward: 0.5074626865671642
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 38           |
|    time_elapsed         | 145          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0010672399 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0157       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00243     |
|    value_loss           | 0.0495       |
------------------------------------------
Output 39: Average over 61 episodes - Reward: 0.6557377049180327
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 39           |
|    time_elapsed         | 148          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0016045854 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.0971       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0259       |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 0.0633       |
------------------------------------------
Output 40: Average over 54 episodes - Reward: 0.6296296296296297
------------------------------------------
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 40           |
|    time_elapsed         | 152          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0012650255 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0222       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00203     |
|    value_loss           | 0.0558       |
------------------------------------------
Output 41: Average over 56 episodes - Reward: 0.6785714285714286
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 41           |
|    time_elapsed         | 155          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0014877559 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0867      |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00302      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 0.0474       |
------------------------------------------
Output 42: Average over 61 episodes - Reward: 0.6229508196721312
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 42            |
|    time_elapsed         | 159           |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.00095942966 |
|    clip_fraction        | 0.0132        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0869       |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0157        |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.00209      |
|    value_loss           | 0.0483        |
-------------------------------------------
Output 43: Average over 59 episodes - Reward: 0.6779661016949152
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 43           |
|    time_elapsed         | 163          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0011996623 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0747      |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0234       |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00306     |
|    value_loss           | 0.0531       |
------------------------------------------
Output 44: Average over 47 episodes - Reward: 0.723404255319149
------------------------------------------
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 44           |
|    time_elapsed         | 168          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0007060744 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0757      |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0216       |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.052        |
------------------------------------------
Output 45: Average over 48 episodes - Reward: 0.5625
------------------------------------------
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 45           |
|    time_elapsed         | 172          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0006596346 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0701      |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00791      |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 0.0361       |
------------------------------------------
Output 46: Average over 48 episodes - Reward: 0.625
------------------------------------------
| time/                   |              |
|    fps                  | 532          |
|    iterations           | 46           |
|    time_elapsed         | 176          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0007465207 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0548      |
|    explained_variance   | 0.183        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0205       |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00266     |
|    value_loss           | 0.0437       |
------------------------------------------
Output 47: Average over 53 episodes - Reward: 0.7735849056603774
-------------------------------------------
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 47            |
|    time_elapsed         | 180           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.00047021804 |
|    clip_fraction        | 0.00962       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0578       |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0201        |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.00227      |
|    value_loss           | 0.0471        |
-------------------------------------------
Output 48: Average over 47 episodes - Reward: 0.5957446808510638
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 48          |
|    time_elapsed         | 185         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.009291668 |
|    clip_fraction        | 0.00659     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.08       |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0116      |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 0.0407      |
-----------------------------------------
Output 49: Average over 55 episodes - Reward: 0.6
-------------------------------------------
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 49            |
|    time_elapsed         | 189           |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00090887287 |
|    clip_fraction        | 0.0209        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.112        |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0129        |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.00234      |
|    value_loss           | 0.0418        |
-------------------------------------------
Output 50: Average over 56 episodes - Reward: 0.6428571428571429
------------------------------------------
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 50           |
|    time_elapsed         | 194          |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0017146834 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0964      |
|    explained_variance   | 0.143        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0251       |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00295     |
|    value_loss           | 0.0467       |
------------------------------------------
Output 51: Average over 45 episodes - Reward: 0.5111111111111111
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 51           |
|    time_elapsed         | 198          |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.0006749025 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0925      |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0203       |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000885    |
|    value_loss           | 0.0459       |
------------------------------------------
Output 52: Average over 54 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 52           |
|    time_elapsed         | 202          |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 0.0013776425 |
|    clip_fraction        | 0.00981      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0766      |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00834      |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000412    |
|    value_loss           | 0.0382       |
------------------------------------------
Output 53: Average over 52 episodes - Reward: 0.8461538461538461
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 53            |
|    time_elapsed         | 206           |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.00094938243 |
|    clip_fraction        | 0.0151        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0683       |
|    explained_variance   | 0.222         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00788       |
|    n_updates            | 520           |
|    policy_gradient_loss | -0.0025       |
|    value_loss           | 0.0486        |
-------------------------------------------
Output 54: Average over 57 episodes - Reward: 0.6666666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 54            |
|    time_elapsed         | 210           |
|    total_timesteps      | 110592        |
| train/                  |               |
|    approx_kl            | 0.00096126297 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0688       |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0183        |
|    n_updates            | 530           |
|    policy_gradient_loss | -0.00117      |
|    value_loss           | 0.0343        |
-------------------------------------------
Output 55: Average over 43 episodes - Reward: 0.6511627906976745
-------------------------------------------
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 55            |
|    time_elapsed         | 213           |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.00060952315 |
|    clip_fraction        | 0.00669       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0564       |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0342        |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.000583     |
|    value_loss           | 0.0459        |
-------------------------------------------
Output 56: Average over 54 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 56           |
|    time_elapsed         | 217          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0005073134 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0539      |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0205       |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.0349       |
------------------------------------------
Output 57: Average over 49 episodes - Reward: 0.7142857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 57           |
|    time_elapsed         | 221          |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.0011366992 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0484      |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0157       |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.000555    |
|    value_loss           | 0.0431       |
------------------------------------------
Output 58: Average over 55 episodes - Reward: 0.5636363636363636
-------------------------------------------
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 58            |
|    time_elapsed         | 225           |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 0.00042007098 |
|    clip_fraction        | 0.00552       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0498       |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0291        |
|    n_updates            | 570           |
|    policy_gradient_loss | -0.000387     |
|    value_loss           | 0.0359        |
-------------------------------------------
Output 59: Average over 57 episodes - Reward: 0.6491228070175439
------------------------------------------
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 59           |
|    time_elapsed         | 229          |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0013147304 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0527      |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0342       |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 0.0453       |
------------------------------------------
Output 60: Average over 55 episodes - Reward: 0.7272727272727273
------------------------------------------
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 60           |
|    time_elapsed         | 232          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0005868763 |
|    clip_fraction        | 0.00688      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0491      |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0292       |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000837    |
|    value_loss           | 0.048        |
------------------------------------------
Output 61: Average over 57 episodes - Reward: 0.6491228070175439
-------------------------------------------
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 61            |
|    time_elapsed         | 236           |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.00049428607 |
|    clip_fraction        | 0.00962       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0452       |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0231        |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.0009       |
|    value_loss           | 0.0458        |
-------------------------------------------
Output 62: Average over 50 episodes - Reward: 0.72
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 62           |
|    time_elapsed         | 240          |
|    total_timesteps      | 126976       |
| train/                  |              |
|    approx_kl            | 0.0013355683 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0449      |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0184       |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.0509       |
------------------------------------------
Output 63: Average over 49 episodes - Reward: 0.673469387755102
-------------------------------------------
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 63            |
|    time_elapsed         | 244           |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.00043953225 |
|    clip_fraction        | 0.00703       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.031        |
|    explained_variance   | 0.153         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0261        |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.000694     |
|    value_loss           | 0.0399        |
-------------------------------------------
Output 64: Average over 49 episodes - Reward: 0.6530612244897959
------------------------------------------
| time/                   |              |
|    fps                  | 529          |
|    iterations           | 64           |
|    time_elapsed         | 247          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0003621345 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0245      |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0231       |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000921    |
|    value_loss           | 0.0442       |
------------------------------------------
Output 65: Average over 54 episodes - Reward: 0.7407407407407407
-------------------------------------------
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 65            |
|    time_elapsed         | 251           |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.00037078318 |
|    clip_fraction        | 0.00596       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0282       |
|    explained_variance   | 0.138         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0206        |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.0011       |
|    value_loss           | 0.0427        |
-------------------------------------------
Output 66: Average over 56 episodes - Reward: 0.6071428571428571
-------------------------------------------
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 66            |
|    time_elapsed         | 256           |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 0.00046620186 |
|    clip_fraction        | 0.00405       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0255       |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.017         |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.000294     |
|    value_loss           | 0.039         |
-------------------------------------------
Output 67: Average over 48 episodes - Reward: 0.7916666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 67            |
|    time_elapsed         | 264           |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 0.00029627225 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0206       |
|    explained_variance   | 0.138         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0248        |
|    n_updates            | 660           |
|    policy_gradient_loss | -0.00163      |
|    value_loss           | 0.0497        |
-------------------------------------------
Output 68: Average over 55 episodes - Reward: 0.7636363636363637
-------------------------------------------
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 68            |
|    time_elapsed         | 268           |
|    total_timesteps      | 139264        |
| train/                  |               |
|    approx_kl            | 0.00083030347 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0143        |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.000876     |
|    value_loss           | 0.0328        |
-------------------------------------------
Output 69: Average over 45 episodes - Reward: 0.7777777777777778
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 69           |
|    time_elapsed         | 272          |
|    total_timesteps      | 141312       |
| train/                  |              |
|    approx_kl            | 0.0008672685 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0164      |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0175       |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 0.038        |
------------------------------------------
Output 70: Average over 57 episodes - Reward: 0.7894736842105263
-------------------------------------------
| time/                   |               |
|    fps                  | 518           |
|    iterations           | 70            |
|    time_elapsed         | 276           |
|    total_timesteps      | 143360        |
| train/                  |               |
|    approx_kl            | 0.00029841604 |
|    clip_fraction        | 0.0042        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0142        |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.00058      |
|    value_loss           | 0.0277        |
-------------------------------------------
Output 71: Average over 58 episodes - Reward: 0.6896551724137931
------------------------------------------
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 71           |
|    time_elapsed         | 279          |
|    total_timesteps      | 145408       |
| train/                  |              |
|    approx_kl            | 4.988801e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0178      |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0197       |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.000142    |
|    value_loss           | 0.0361       |
------------------------------------------
Output 72: Average over 47 episodes - Reward: 0.7446808510638298
-------------------------------------------
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 72            |
|    time_elapsed         | 283           |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 0.00042893848 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0209       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0297        |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 0.0443        |
-------------------------------------------
Output 73: Average over 48 episodes - Reward: 0.7083333333333334
------------------------------------------
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 73           |
|    time_elapsed         | 287          |
|    total_timesteps      | 149504       |
| train/                  |              |
|    approx_kl            | 0.0006143631 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0196      |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0133       |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.000778    |
|    value_loss           | 0.0329       |
------------------------------------------
Output 74: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 74            |
|    time_elapsed         | 290           |
|    total_timesteps      | 151552        |
| train/                  |               |
|    approx_kl            | 0.00058600644 |
|    clip_fraction        | 0.0042        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0157       |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.019         |
|    n_updates            | 730           |
|    policy_gradient_loss | -0.00138      |
|    value_loss           | 0.034         |
-------------------------------------------
Output 75: Average over 52 episodes - Reward: 0.7692307692307693
-------------------------------------------
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 75            |
|    time_elapsed         | 294           |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 0.00094482215 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0191        |
|    n_updates            | 740           |
|    policy_gradient_loss | -0.00132      |
|    value_loss           | 0.0353        |
-------------------------------------------
Output 76: Average over 52 episodes - Reward: 0.7307692307692307
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 76          |
|    time_elapsed         | 299         |
|    total_timesteps      | 155648      |
| train/                  |             |
|    approx_kl            | 0.000200846 |
|    clip_fraction        | 0.00288     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0115     |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0166      |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.000585   |
|    value_loss           | 0.0363      |
-----------------------------------------
Output 77: Average over 44 episodes - Reward: 0.6590909090909091
-------------------------------------------
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 77            |
|    time_elapsed         | 303           |
|    total_timesteps      | 157696        |
| train/                  |               |
|    approx_kl            | 0.00062166963 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0164       |
|    explained_variance   | 0.21          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0293        |
|    n_updates            | 760           |
|    policy_gradient_loss | -0.000409     |
|    value_loss           | 0.0407        |
-------------------------------------------
Output 78: Average over 44 episodes - Reward: 0.7727272727272727
------------------------------------------
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 78           |
|    time_elapsed         | 307          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0003419726 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0123      |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0221       |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00161     |
|    value_loss           | 0.033        |
------------------------------------------
Output 79: Average over 59 episodes - Reward: 0.7457627118644068
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 79           |
|    time_elapsed         | 311          |
|    total_timesteps      | 161792       |
| train/                  |              |
|    approx_kl            | 0.0006669709 |
|    clip_fraction        | 0.00518      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0184      |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0126       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 0.0269       |
------------------------------------------
Output 80: Average over 56 episodes - Reward: 0.6607142857142857
-------------------------------------------
| time/                   |               |
|    fps                  | 518           |
|    iterations           | 80            |
|    time_elapsed         | 316           |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.00013641248 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0241       |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.017         |
|    n_updates            | 790           |
|    policy_gradient_loss | -0.00023      |
|    value_loss           | 0.0402        |
-------------------------------------------
Output 81: Average over 51 episodes - Reward: 0.7450980392156863
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 81           |
|    time_elapsed         | 319          |
|    total_timesteps      | 165888       |
| train/                  |              |
|    approx_kl            | 0.0007337489 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0212      |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0141       |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.000878    |
|    value_loss           | 0.0426       |
------------------------------------------
Output 82: Average over 54 episodes - Reward: 0.7962962962962963
-------------------------------------------
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 82            |
|    time_elapsed         | 323           |
|    total_timesteps      | 167936        |
| train/                  |               |
|    approx_kl            | 0.00048018238 |
|    clip_fraction        | 0.00332       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0174        |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.00105      |
|    value_loss           | 0.0379        |
-------------------------------------------
Output 83: Average over 54 episodes - Reward: 0.7592592592592593
-------------------------------------------
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 83            |
|    time_elapsed         | 326           |
|    total_timesteps      | 169984        |
| train/                  |               |
|    approx_kl            | 0.00050552265 |
|    clip_fraction        | 0.00479       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0129       |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0148        |
|    n_updates            | 820           |
|    policy_gradient_loss | -0.000914     |
|    value_loss           | 0.0363        |
-------------------------------------------
Output 84: Average over 47 episodes - Reward: 0.6382978723404256
------------------------------------------
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 84           |
|    time_elapsed         | 330          |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0011615122 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0195      |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0216       |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.000703    |
|    value_loss           | 0.0355       |
------------------------------------------
Output 85: Average over 47 episodes - Reward: 0.574468085106383
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 85           |
|    time_elapsed         | 333          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0007271331 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.036       |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0157       |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 0.0346       |
------------------------------------------
Output 86: Average over 53 episodes - Reward: 0.7358490566037735
------------------------------------------
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 86           |
|    time_elapsed         | 337          |
|    total_timesteps      | 176128       |
| train/                  |              |
|    approx_kl            | 0.0018951688 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0378      |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0223       |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 0.0373       |
------------------------------------------
Output 87: Average over 52 episodes - Reward: 0.5961538461538461
-------------------------------------------
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 87            |
|    time_elapsed         | 340           |
|    total_timesteps      | 178176        |
| train/                  |               |
|    approx_kl            | 0.00074766786 |
|    clip_fraction        | 0.011         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0359       |
|    explained_variance   | 0.205         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0316        |
|    n_updates            | 860           |
|    policy_gradient_loss | -0.00186      |
|    value_loss           | 0.0394        |
-------------------------------------------
Output 88: Average over 41 episodes - Reward: 0.6341463414634146
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 88           |
|    time_elapsed         | 344          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0010426112 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0283      |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0142       |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00152     |
|    value_loss           | 0.0405       |
------------------------------------------
Output 89: Average over 55 episodes - Reward: 0.6545454545454545
-------------------------------------------
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 89            |
|    time_elapsed         | 348           |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 0.00027150224 |
|    clip_fraction        | 0.00684       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0212       |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0192        |
|    n_updates            | 880           |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 0.0368        |
-------------------------------------------
Output 90: Average over 54 episodes - Reward: 0.6851851851851852
-------------------------------------------
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 90            |
|    time_elapsed         | 352           |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 0.00037707883 |
|    clip_fraction        | 0.00361       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0219       |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0195        |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.000687     |
|    value_loss           | 0.0472        |
-------------------------------------------
Output 91: Average over 56 episodes - Reward: 0.7857142857142857
-------------------------------------------
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 91            |
|    time_elapsed         | 356           |
|    total_timesteps      | 186368        |
| train/                  |               |
|    approx_kl            | 0.00044050053 |
|    clip_fraction        | 0.00376       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0184       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0248        |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.000422     |
|    value_loss           | 0.0429        |
-------------------------------------------
Output 92: Average over 48 episodes - Reward: 0.75
------------------------------------------
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 92           |
|    time_elapsed         | 361          |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0004714257 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0136      |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0131       |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000834    |
|    value_loss           | 0.0379       |
------------------------------------------
Output 93: Average over 53 episodes - Reward: 0.8301886792452831
-------------------------------------------
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 93            |
|    time_elapsed         | 366           |
|    total_timesteps      | 190464        |
| train/                  |               |
|    approx_kl            | 0.00016294586 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0244        |
|    n_updates            | 920           |
|    policy_gradient_loss | -0.000628     |
|    value_loss           | 0.0348        |
-------------------------------------------
Output 94: Average over 48 episodes - Reward: 0.7708333333333334
------------------------------------------
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 94           |
|    time_elapsed         | 370          |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 0.0014486826 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0117      |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0134       |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.000683    |
|    value_loss           | 0.0324       |
------------------------------------------
Output 95: Average over 46 episodes - Reward: 0.6086956521739131
-------------------------------------------
| time/                   |               |
|    fps                  | 518           |
|    iterations           | 95            |
|    time_elapsed         | 375           |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 0.00082427054 |
|    clip_fraction        | 0.00723       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.016        |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0103        |
|    n_updates            | 940           |
|    policy_gradient_loss | -0.00164      |
|    value_loss           | 0.0273        |
-------------------------------------------
Output 96: Average over 50 episodes - Reward: 0.72
-------------------------------------------
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 96            |
|    time_elapsed         | 379           |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00036438246 |
|    clip_fraction        | 0.00508       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0203       |
|    explained_variance   | 0.222         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0116        |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 0.0317        |
-------------------------------------------
Output 97: Average over 44 episodes - Reward: 0.6818181818181818
-----------------------------------------
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 97          |
|    time_elapsed         | 384         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.001066285 |
|    clip_fraction        | 0.00435     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0315     |
|    explained_variance   | 0.126       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0276      |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.000478   |
|    value_loss           | 0.0383      |
-----------------------------------------
Output 98: Average over 53 episodes - Reward: 0.6792452830188679
------------------------------------------
| time/                   |              |
|    fps                  | 516          |
|    iterations           | 98           |
|    time_elapsed         | 388          |
|    total_timesteps      | 200704       |
| train/                  |              |
|    approx_kl            | 0.0035966039 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0341       |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.000656    |
|    value_loss           | 0.036        |
------------------------------------------
Output 99: Average over 51 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 99           |
|    time_elapsed         | 392          |
|    total_timesteps      | 202752       |
| train/                  |              |
|    approx_kl            | 0.0015414003 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0136       |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 0.0409       |
------------------------------------------
Output 100: Average over 60 episodes - Reward: 0.6166666666666667
------------------------------------------
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 100          |
|    time_elapsed         | 395          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0017694521 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0247       |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00168     |
|    value_loss           | 0.0391       |
------------------------------------------
Output 101: Average over 55 episodes - Reward: 0.5818181818181818
------------------------------------------
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 101          |
|    time_elapsed         | 399          |
|    total_timesteps      | 206848       |
| train/                  |              |
|    approx_kl            | 0.0015224796 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.132       |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0235       |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 0.0484       |
------------------------------------------
Output 102: Average over 43 episodes - Reward: 0.7209302325581395
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 102          |
|    time_elapsed         | 402          |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 0.0011223515 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0.142        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0281       |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000351    |
|    value_loss           | 0.0512       |
------------------------------------------
Output 103: Average over 45 episodes - Reward: 0.5777777777777777
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 103          |
|    time_elapsed         | 406          |
|    total_timesteps      | 210944       |
| train/                  |              |
|    approx_kl            | 0.0012929495 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.267        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0184       |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 0.0327       |
------------------------------------------
Output 104: Average over 56 episodes - Reward: 0.5892857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 104          |
|    time_elapsed         | 410          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0010659052 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0997      |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0138       |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.0405       |
------------------------------------------
Output 105: Average over 47 episodes - Reward: 0.723404255319149
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 105         |
|    time_elapsed         | 414         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.001032447 |
|    clip_fraction        | 0.0111      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0737     |
|    explained_variance   | 0.196       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.024       |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.000829   |
|    value_loss           | 0.0474      |
-----------------------------------------
Output 106: Average over 49 episodes - Reward: 0.8367346938775511
-------------------------------------------
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 106           |
|    time_elapsed         | 417           |
|    total_timesteps      | 217088        |
| train/                  |               |
|    approx_kl            | 0.00080464885 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0786       |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0197        |
|    n_updates            | 1050          |
|    policy_gradient_loss | -0.000739     |
|    value_loss           | 0.0424        |
-------------------------------------------
Output 107: Average over 47 episodes - Reward: 0.7446808510638298
------------------------------------------
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 107          |
|    time_elapsed         | 421          |
|    total_timesteps      | 219136       |
| train/                  |              |
|    approx_kl            | 0.0005384034 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0822      |
|    explained_variance   | 0.224        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0138       |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.000231    |
|    value_loss           | 0.0344       |
------------------------------------------
Output 108: Average over 53 episodes - Reward: 0.7169811320754716
-------------------------------------------
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 108           |
|    time_elapsed         | 424           |
|    total_timesteps      | 221184        |
| train/                  |               |
|    approx_kl            | 0.00064939505 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0768       |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0201        |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.000441     |
|    value_loss           | 0.0339        |
-------------------------------------------
Output 109: Average over 54 episodes - Reward: 0.7222222222222222
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 109          |
|    time_elapsed         | 427          |
|    total_timesteps      | 223232       |
| train/                  |              |
|    approx_kl            | 0.0011657047 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0736      |
|    explained_variance   | 0.182        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0215       |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.000555    |
|    value_loss           | 0.0389       |
------------------------------------------
Output 110: Average over 47 episodes - Reward: 0.6808510638297872
------------------------------------------
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 110          |
|    time_elapsed         | 430          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.0007767617 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.077       |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0177       |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00028     |
|    value_loss           | 0.0384       |
------------------------------------------
Output 111: Average over 44 episodes - Reward: 0.7727272727272727
-------------------------------------------
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 111           |
|    time_elapsed         | 434           |
|    total_timesteps      | 227328        |
| train/                  |               |
|    approx_kl            | 0.00073735346 |
|    clip_fraction        | 0.012         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.072        |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0194        |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 0.0352        |
-------------------------------------------
Output 112: Average over 41 episodes - Reward: 0.7073170731707317
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 112          |
|    time_elapsed         | 437          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0012590206 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0613      |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.014        |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 0.035        |
------------------------------------------
Output 113: Average over 49 episodes - Reward: 0.7142857142857143
-------------------------------------------
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 113           |
|    time_elapsed         | 441           |
|    total_timesteps      | 231424        |
| train/                  |               |
|    approx_kl            | 0.00025560035 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0524       |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.016         |
|    n_updates            | 1120          |
|    policy_gradient_loss | -0.000508     |
|    value_loss           | 0.029         |
-------------------------------------------
Output 114: Average over 44 episodes - Reward: 0.7045454545454546
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 114          |
|    time_elapsed         | 445          |
|    total_timesteps      | 233472       |
| train/                  |              |
|    approx_kl            | 0.0008532315 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0728      |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0222       |
|    n_updates            | 1130         |
|    policy_gradient_loss | -8.19e-06    |
|    value_loss           | 0.0384       |
------------------------------------------
Output 115: Average over 52 episodes - Reward: 0.7692307692307693
-------------------------------------------
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 115           |
|    time_elapsed         | 449           |
|    total_timesteps      | 235520        |
| train/                  |               |
|    approx_kl            | 0.00076471304 |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0533       |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0102        |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.0021       |
|    value_loss           | 0.0302        |
-------------------------------------------
Output 116: Average over 50 episodes - Reward: 0.72
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 116         |
|    time_elapsed         | 452         |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.000412018 |
|    clip_fraction        | 0.00864     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0546     |
|    explained_variance   | 0.229       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0167      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.000353   |
|    value_loss           | 0.0367      |
-----------------------------------------
Output 117: Average over 57 episodes - Reward: 0.7368421052631579
------------------------------------------
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 117          |
|    time_elapsed         | 456          |
|    total_timesteps      | 239616       |
| train/                  |              |
|    approx_kl            | 0.0018078504 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0567      |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.017        |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 0.0348       |
------------------------------------------
Output 118: Average over 47 episodes - Reward: 0.6595744680851063
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 118           |
|    time_elapsed         | 460           |
|    total_timesteps      | 241664        |
| train/                  |               |
|    approx_kl            | 0.00080604455 |
|    clip_fraction        | 0.0141        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0526       |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00945       |
|    n_updates            | 1170          |
|    policy_gradient_loss | -0.00204      |
|    value_loss           | 0.0391        |
-------------------------------------------
Output 119: Average over 48 episodes - Reward: 0.6041666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 119           |
|    time_elapsed         | 463           |
|    total_timesteps      | 243712        |
| train/                  |               |
|    approx_kl            | 0.00055907236 |
|    clip_fraction        | 0.0114        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0481       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0303        |
|    n_updates            | 1180          |
|    policy_gradient_loss | -0.000787     |
|    value_loss           | 0.0376        |
-------------------------------------------
Output 120: Average over 45 episodes - Reward: 0.6222222222222222
-------------------------------------------
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 120           |
|    time_elapsed         | 467           |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.00075352733 |
|    clip_fraction        | 0.00396       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.043        |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0245        |
|    n_updates            | 1190          |
|    policy_gradient_loss | -0.000286     |
|    value_loss           | 0.0324        |
-------------------------------------------
Output 121: Average over 49 episodes - Reward: 0.7346938775510204
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 121         |
|    time_elapsed         | 470         |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.001028092 |
|    clip_fraction        | 0.00591     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0269     |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0166      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.000957   |
|    value_loss           | 0.0386      |
-----------------------------------------
Output 122: Average over 51 episodes - Reward: 0.7058823529411765
------------------------------------------
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 122          |
|    time_elapsed         | 474          |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 0.0010415434 |
|    clip_fraction        | 0.00518      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0249      |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.017        |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.000675    |
|    value_loss           | 0.0369       |
------------------------------------------
Output 123: Average over 50 episodes - Reward: 0.78
-------------------------------------------
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 123           |
|    time_elapsed         | 477           |
|    total_timesteps      | 251904        |
| train/                  |               |
|    approx_kl            | 0.00021762634 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0146        |
|    n_updates            | 1220          |
|    policy_gradient_loss | -0.000592     |
|    value_loss           | 0.035         |
-------------------------------------------
Output 124: Average over 48 episodes - Reward: 0.75
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 124          |
|    time_elapsed         | 480          |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.0004869751 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0328      |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0169       |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.000867    |
|    value_loss           | 0.0355       |
------------------------------------------
Output 125: Average over 46 episodes - Reward: 0.6739130434782609
-------------------------------------------
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 125           |
|    time_elapsed         | 484           |
|    total_timesteps      | 256000        |
| train/                  |               |
|    approx_kl            | 0.00050307385 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0303       |
|    explained_variance   | 0.306         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0156        |
|    n_updates            | 1240          |
|    policy_gradient_loss | -0.000294     |
|    value_loss           | 0.0291        |
-------------------------------------------
Output 126: Average over 49 episodes - Reward: 0.7959183673469388
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 126         |
|    time_elapsed         | 487         |
|    total_timesteps      | 258048      |
| train/                  |             |
|    approx_kl            | 0.000199075 |
|    clip_fraction        | 0.00518     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0268     |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0172      |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.000356   |
|    value_loss           | 0.0306      |
-----------------------------------------
Output 127: Average over 44 episodes - Reward: 0.6818181818181818
-------------------------------------------
| time/                   |               |
|    fps                  | 530           |
|    iterations           | 127           |
|    time_elapsed         | 490           |
|    total_timesteps      | 260096        |
| train/                  |               |
|    approx_kl            | 0.00043362164 |
|    clip_fraction        | 0.00435       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0143        |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.000473     |
|    value_loss           | 0.0313        |
-------------------------------------------
Output 128: Average over 40 episodes - Reward: 0.725
------------------------------------------
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 128          |
|    time_elapsed         | 494          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0003113775 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0145      |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0132       |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000458    |
|    value_loss           | 0.0287       |
------------------------------------------
Output 129: Average over 50 episodes - Reward: 0.8
------------------------------------------
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 129          |
|    time_elapsed         | 497          |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 1.649381e-05 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0104      |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0144       |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.000214    |
|    value_loss           | 0.0259       |
------------------------------------------
Output 130: Average over 44 episodes - Reward: 0.5909090909090909
-------------------------------------------
| time/                   |               |
|    fps                  | 531           |
|    iterations           | 130           |
|    time_elapsed         | 501           |
|    total_timesteps      | 266240        |
| train/                  |               |
|    approx_kl            | 0.00010183646 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.018         |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.00024      |
|    value_loss           | 0.0335        |
-------------------------------------------
Output 131: Average over 52 episodes - Reward: 0.5576923076923077
------------------------------------------
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 131          |
|    time_elapsed         | 504          |
|    total_timesteps      | 268288       |
| train/                  |              |
|    approx_kl            | 0.0013630812 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0183      |
|    explained_variance   | 0.0546       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0162       |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 0.0359       |
------------------------------------------
Output 132: Average over 47 episodes - Reward: 0.6808510638297872
------------------------------------------
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 132          |
|    time_elapsed         | 508          |
|    total_timesteps      | 270336       |
| train/                  |              |
|    approx_kl            | 0.0012748379 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0385      |
|    explained_variance   | 0.0643       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.027        |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.000176    |
|    value_loss           | 0.0474       |
------------------------------------------
Output 133: Average over 47 episodes - Reward: 0.7446808510638298
------------------------------------------
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 133          |
|    time_elapsed         | 512          |
|    total_timesteps      | 272384       |
| train/                  |              |
|    approx_kl            | 0.0012594792 |
|    clip_fraction        | 0.00771      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0384      |
|    explained_variance   | 0.0853       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0182       |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.000782    |
|    value_loss           | 0.0369       |
------------------------------------------
Output 134: Average over 48 episodes - Reward: 0.6041666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 134           |
|    time_elapsed         | 515           |
|    total_timesteps      | 274432        |
| train/                  |               |
|    approx_kl            | 0.00020157848 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0249       |
|    explained_variance   | 0.134         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0217        |
|    n_updates            | 1330          |
|    policy_gradient_loss | -0.000986     |
|    value_loss           | 0.036         |
-------------------------------------------
Output 135: Average over 47 episodes - Reward: 0.6808510638297872
-------------------------------------------
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 135           |
|    time_elapsed         | 519           |
|    total_timesteps      | 276480        |
| train/                  |               |
|    approx_kl            | 0.00045650871 |
|    clip_fraction        | 0.00508       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0209       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.014         |
|    n_updates            | 1340          |
|    policy_gradient_loss | -0.00163      |
|    value_loss           | 0.0371        |
-------------------------------------------
Output 136: Average over 53 episodes - Reward: 0.7735849056603774
------------------------------------------
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 136          |
|    time_elapsed         | 522          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0002786898 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0208      |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0221       |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.000814    |
|    value_loss           | 0.0414       |
------------------------------------------
Output 137: Average over 46 episodes - Reward: 0.6956521739130435
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 137         |
|    time_elapsed         | 525         |
|    total_timesteps      | 280576      |
| train/                  |             |
|    approx_kl            | 0.001043569 |
|    clip_fraction        | 0.00322     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0171     |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0196      |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.000834   |
|    value_loss           | 0.0383      |
-----------------------------------------
Output 138: Average over 50 episodes - Reward: 0.78
------------------------------------------
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 138          |
|    time_elapsed         | 529          |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0027076157 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0259      |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0252       |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.000817    |
|    value_loss           | 0.0352       |
------------------------------------------
Output 139: Average over 48 episodes - Reward: 0.5833333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 139           |
|    time_elapsed         | 532           |
|    total_timesteps      | 284672        |
| train/                  |               |
|    approx_kl            | 0.00033137255 |
|    clip_fraction        | 0.00654       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0348       |
|    explained_variance   | 0.0825        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0239        |
|    n_updates            | 1380          |
|    policy_gradient_loss | -0.00162      |
|    value_loss           | 0.036         |
-------------------------------------------
Output 140: Average over 45 episodes - Reward: 0.6444444444444445
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 140          |
|    time_elapsed         | 536          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.0008266675 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0285      |
|    explained_variance   | 0.104        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0155       |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 0.0373       |
------------------------------------------
Output 141: Average over 46 episodes - Reward: 0.6304347826086957
-------------------------------------------
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 141           |
|    time_elapsed         | 540           |
|    total_timesteps      | 288768        |
| train/                  |               |
|    approx_kl            | 0.00026158654 |
|    clip_fraction        | 0.00459       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0237       |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0122        |
|    n_updates            | 1400          |
|    policy_gradient_loss | -0.000628     |
|    value_loss           | 0.0305        |
-------------------------------------------
Output 142: Average over 45 episodes - Reward: 0.7333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 142          |
|    time_elapsed         | 544          |
|    total_timesteps      | 290816       |
| train/                  |              |
|    approx_kl            | 0.0005640375 |
|    clip_fraction        | 0.00698      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0282      |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0101       |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 0.0382       |
------------------------------------------
Output 143: Average over 47 episodes - Reward: 0.7446808510638298
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 143          |
|    time_elapsed         | 547          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0006376859 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.028       |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0293       |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.000876    |
|    value_loss           | 0.036        |
------------------------------------------
Output 144: Average over 57 episodes - Reward: 0.7017543859649122
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 144          |
|    time_elapsed         | 551          |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0041336566 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0787      |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0181       |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.000906    |
|    value_loss           | 0.0331       |
------------------------------------------
Output 145: Average over 51 episodes - Reward: 0.6274509803921569
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 145          |
|    time_elapsed         | 555          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0019773329 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0789      |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.018        |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00262     |
|    value_loss           | 0.0447       |
------------------------------------------
Output 146: Average over 55 episodes - Reward: 0.6
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 146         |
|    time_elapsed         | 559         |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.012421638 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0746     |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0289      |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00227    |
|    value_loss           | 0.0426      |
-----------------------------------------
Output 147: Average over 55 episodes - Reward: 0.6363636363636364
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 147         |
|    time_elapsed         | 563         |
|    total_timesteps      | 301056      |
| train/                  |             |
|    approx_kl            | 0.001832657 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0942     |
|    explained_variance   | 0.108       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0318      |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 0.046       |
-----------------------------------------
Output 148: Average over 53 episodes - Reward: 0.6037735849056604
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 148          |
|    time_elapsed         | 567          |
|    total_timesteps      | 303104       |
| train/                  |              |
|    approx_kl            | 0.0019422355 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0801      |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0315       |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 0.0448       |
------------------------------------------
Output 149: Average over 50 episodes - Reward: 0.76
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 149          |
|    time_elapsed         | 571          |
|    total_timesteps      | 305152       |
| train/                  |              |
|    approx_kl            | 0.0012006414 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0704      |
|    explained_variance   | 0.137        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0138       |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 0.0457       |
------------------------------------------
Output 150: Average over 49 episodes - Reward: 0.7142857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 150          |
|    time_elapsed         | 574          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0023245295 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0684      |
|    explained_variance   | 0.153        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0149       |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 0.0367       |
------------------------------------------
Output 151: Average over 52 episodes - Reward: 0.7115384615384616
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 151         |
|    time_elapsed         | 577         |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.002283651 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0629     |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0152      |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 0.0409      |
-----------------------------------------
Output 152: Average over 40 episodes - Reward: 0.5
------------------------------------------
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 152          |
|    time_elapsed         | 581          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0011112196 |
|    clip_fraction        | 0.00825      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0702      |
|    explained_variance   | 0.143        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0142       |
|    n_updates            | 1510         |
|    policy_gradient_loss | 6.5e-05      |
|    value_loss           | 0.0391       |
------------------------------------------
Output 153: Average over 50 episodes - Reward: 0.7
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 153         |
|    time_elapsed         | 585         |
|    total_timesteps      | 313344      |
| train/                  |             |
|    approx_kl            | 0.003165348 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0555     |
|    explained_variance   | 0.108       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0192      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 0.035       |
-----------------------------------------
Output 154: Average over 48 episodes - Reward: 0.5833333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 535           |
|    iterations           | 154           |
|    time_elapsed         | 588           |
|    total_timesteps      | 315392        |
| train/                  |               |
|    approx_kl            | 0.00041085985 |
|    clip_fraction        | 0.00518       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0531       |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0154        |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.000321     |
|    value_loss           | 0.0401        |
-------------------------------------------
Output 155: Average over 49 episodes - Reward: 0.8163265306122449
-------------------------------------------
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 155           |
|    time_elapsed         | 591           |
|    total_timesteps      | 317440        |
| train/                  |               |
|    approx_kl            | 0.00016049398 |
|    clip_fraction        | 0.00522       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0458       |
|    explained_variance   | 0.102         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0228        |
|    n_updates            | 1540          |
|    policy_gradient_loss | 0.000102      |
|    value_loss           | 0.0436        |
-------------------------------------------
Output 156: Average over 51 episodes - Reward: 0.7843137254901961
-----------------------------------------
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 156         |
|    time_elapsed         | 595         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.008505339 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0337     |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0109      |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 0.0381      |
-----------------------------------------
Output 157: Average over 43 episodes - Reward: 0.6511627906976745
-------------------------------------------
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 157           |
|    time_elapsed         | 599           |
|    total_timesteps      | 321536        |
| train/                  |               |
|    approx_kl            | 0.00035108117 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0296       |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00911       |
|    n_updates            | 1560          |
|    policy_gradient_loss | -0.00134      |
|    value_loss           | 0.0339        |
-------------------------------------------
Output 158: Average over 43 episodes - Reward: 0.7441860465116279
-------------------------------------------
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 158           |
|    time_elapsed         | 602           |
|    total_timesteps      | 323584        |
| train/                  |               |
|    approx_kl            | 0.00082811154 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0296       |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0141        |
|    n_updates            | 1570          |
|    policy_gradient_loss | -0.000573     |
|    value_loss           | 0.0354        |
-------------------------------------------
Output 159: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 159           |
|    time_elapsed         | 606           |
|    total_timesteps      | 325632        |
| train/                  |               |
|    approx_kl            | 0.00019311346 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0354       |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0118        |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.000345     |
|    value_loss           | 0.0277        |
-------------------------------------------
Output 160: Average over 52 episodes - Reward: 0.6346153846153846
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 160          |
|    time_elapsed         | 610          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0030940636 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0387      |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0146       |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.000659    |
|    value_loss           | 0.0382       |
------------------------------------------
Output 161: Average over 49 episodes - Reward: 0.6326530612244898
------------------------------------------
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 161          |
|    time_elapsed         | 613          |
|    total_timesteps      | 329728       |
| train/                  |              |
|    approx_kl            | 0.0016645489 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0555      |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0254       |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.0387       |
------------------------------------------
Output 162: Average over 42 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 162          |
|    time_elapsed         | 617          |
|    total_timesteps      | 331776       |
| train/                  |              |
|    approx_kl            | 0.0018693088 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0427      |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0293       |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.0379       |
------------------------------------------
Output 163: Average over 42 episodes - Reward: 0.7380952380952381
-------------------------------------------
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 163           |
|    time_elapsed         | 621           |
|    total_timesteps      | 333824        |
| train/                  |               |
|    approx_kl            | 0.00045518205 |
|    clip_fraction        | 0.00479       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0355       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0179        |
|    n_updates            | 1620          |
|    policy_gradient_loss | -0.000422     |
|    value_loss           | 0.0329        |
-------------------------------------------
Output 164: Average over 49 episodes - Reward: 0.7346938775510204
-----------------------------------------
| time/                   |             |
|    fps                  | 537         |
|    iterations           | 164         |
|    time_elapsed         | 625         |
|    total_timesteps      | 335872      |
| train/                  |             |
|    approx_kl            | 0.009864368 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0557     |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00235    |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 0.0273      |
-----------------------------------------
Output 165: Average over 52 episodes - Reward: 0.7307692307692307
------------------------------------------
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 165          |
|    time_elapsed         | 629          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0004526423 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.066       |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0248       |
|    n_updates            | 1640         |
|    policy_gradient_loss | 0.0001       |
|    value_loss           | 0.0405       |
------------------------------------------
Output 166: Average over 45 episodes - Reward: 0.6
------------------------------------------
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 166          |
|    time_elapsed         | 632          |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.0010809341 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0449      |
|    explained_variance   | 0.164        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0155       |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.0371       |
------------------------------------------
Output 167: Average over 43 episodes - Reward: 0.627906976744186
------------------------------------------
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 167          |
|    time_elapsed         | 635          |
|    total_timesteps      | 342016       |
| train/                  |              |
|    approx_kl            | 0.0009800488 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0496      |
|    explained_variance   | 0.1          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0241       |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 0.0423       |
------------------------------------------
Output 168: Average over 44 episodes - Reward: 0.6590909090909091
-------------------------------------------
| time/                   |               |
|    fps                  | 538           |
|    iterations           | 168           |
|    time_elapsed         | 639           |
|    total_timesteps      | 344064        |
| train/                  |               |
|    approx_kl            | 0.00049301994 |
|    clip_fraction        | 0.00566       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0414       |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.016         |
|    n_updates            | 1670          |
|    policy_gradient_loss | -0.000364     |
|    value_loss           | 0.0339        |
-------------------------------------------
Output 169: Average over 42 episodes - Reward: 0.7142857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 169          |
|    time_elapsed         | 642          |
|    total_timesteps      | 346112       |
| train/                  |              |
|    approx_kl            | 0.0022868146 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0423      |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.023        |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 0.0381       |
------------------------------------------
Output 170: Average over 41 episodes - Reward: 0.6097560975609756
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 170          |
|    time_elapsed         | 646          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0006575639 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0359      |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0151       |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 0.0318       |
------------------------------------------
Output 171: Average over 49 episodes - Reward: 0.6530612244897959
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 171          |
|    time_elapsed         | 649          |
|    total_timesteps      | 350208       |
| train/                  |              |
|    approx_kl            | 0.0007663324 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.044       |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0226       |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00328     |
|    value_loss           | 0.0315       |
------------------------------------------
Output 172: Average over 50 episodes - Reward: 0.6
-----------------------------------------
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 172         |
|    time_elapsed         | 653         |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.001176596 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0395     |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0191      |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 0.0389      |
-----------------------------------------
Output 173: Average over 51 episodes - Reward: 0.6470588235294118
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 173          |
|    time_elapsed         | 656          |
|    total_timesteps      | 354304       |
| train/                  |              |
|    approx_kl            | 0.0007049114 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0347      |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0223       |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 0.0434       |
------------------------------------------
Output 174: Average over 48 episodes - Reward: 0.625
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 174          |
|    time_elapsed         | 660          |
|    total_timesteps      | 356352       |
| train/                  |              |
|    approx_kl            | 0.0005088544 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0324      |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.023        |
|    n_updates            | 1730         |
|    policy_gradient_loss | 6.47e-05     |
|    value_loss           | 0.0436       |
------------------------------------------
Output 175: Average over 49 episodes - Reward: 0.8163265306122449
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 175          |
|    time_elapsed         | 664          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.0007937774 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0267      |
|    explained_variance   | 0.134        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0254       |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.000868    |
|    value_loss           | 0.0428       |
------------------------------------------
Output 176: Average over 49 episodes - Reward: 0.7142857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 176          |
|    time_elapsed         | 667          |
|    total_timesteps      | 360448       |
| train/                  |              |
|    approx_kl            | 0.0020685343 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0244      |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0127       |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.0333       |
------------------------------------------
Output 177: Average over 54 episodes - Reward: 0.7037037037037037
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 177           |
|    time_elapsed         | 671           |
|    total_timesteps      | 362496        |
| train/                  |               |
|    approx_kl            | 0.00020461192 |
|    clip_fraction        | 0.00352       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.219         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.018         |
|    n_updates            | 1760          |
|    policy_gradient_loss | -0.000284     |
|    value_loss           | 0.0366        |
-------------------------------------------
Output 178: Average over 51 episodes - Reward: 0.6862745098039216
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 178           |
|    time_elapsed         | 675           |
|    total_timesteps      | 364544        |
| train/                  |               |
|    approx_kl            | 0.00056349835 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0145       |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.023         |
|    n_updates            | 1770          |
|    policy_gradient_loss | -0.000997     |
|    value_loss           | 0.0408        |
-------------------------------------------
Output 179: Average over 55 episodes - Reward: 0.5636363636363636
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 179          |
|    time_elapsed         | 679          |
|    total_timesteps      | 366592       |
| train/                  |              |
|    approx_kl            | 0.0012514407 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0204      |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0185       |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.000337    |
|    value_loss           | 0.0398       |
------------------------------------------
Output 180: Average over 49 episodes - Reward: 0.6530612244897959
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 180           |
|    time_elapsed         | 682           |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.00018622808 |
|    clip_fraction        | 0.00444       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0222        |
|    n_updates            | 1790          |
|    policy_gradient_loss | 5.83e-05      |
|    value_loss           | 0.0495        |
-------------------------------------------
Output 181: Average over 42 episodes - Reward: 0.6428571428571429
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 181          |
|    time_elapsed         | 686          |
|    total_timesteps      | 370688       |
| train/                  |              |
|    approx_kl            | 0.0009974949 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0205      |
|    explained_variance   | 0.1          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0246       |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.000789    |
|    value_loss           | 0.0402       |
------------------------------------------
Output 182: Average over 48 episodes - Reward: 0.7916666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 182          |
|    time_elapsed         | 690          |
|    total_timesteps      | 372736       |
| train/                  |              |
|    approx_kl            | 0.0004589688 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0197      |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0136       |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.000275    |
|    value_loss           | 0.031        |
------------------------------------------
Output 183: Average over 51 episodes - Reward: 0.7647058823529411
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 183           |
|    time_elapsed         | 693           |
|    total_timesteps      | 374784        |
| train/                  |               |
|    approx_kl            | 0.00010569437 |
|    clip_fraction        | 0.00342       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0162        |
|    n_updates            | 1820          |
|    policy_gradient_loss | 0.000138      |
|    value_loss           | 0.029         |
-------------------------------------------
Output 184: Average over 52 episodes - Reward: 0.75
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 184           |
|    time_elapsed         | 697           |
|    total_timesteps      | 376832        |
| train/                  |               |
|    approx_kl            | 0.00071870896 |
|    clip_fraction        | 0.00679       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0239       |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0228        |
|    n_updates            | 1830          |
|    policy_gradient_loss | -0.000709     |
|    value_loss           | 0.0398        |
-------------------------------------------
Output 185: Average over 50 episodes - Reward: 0.7
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 185           |
|    time_elapsed         | 701           |
|    total_timesteps      | 378880        |
| train/                  |               |
|    approx_kl            | 0.00018639045 |
|    clip_fraction        | 0.00303       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 0.167         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0159        |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.000303     |
|    value_loss           | 0.0377        |
-------------------------------------------
Output 186: Average over 44 episodes - Reward: 0.75
--------------------------------------------
| time/                   |                |
|    fps                  | 540            |
|    iterations           | 186            |
|    time_elapsed         | 704            |
|    total_timesteps      | 380928         |
| train/                  |                |
|    approx_kl            | 0.000115752424 |
|    clip_fraction        | 0.00283        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0134        |
|    explained_variance   | 0.174          |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0133         |
|    n_updates            | 1850           |
|    policy_gradient_loss | -0.000168      |
|    value_loss           | 0.0346         |
--------------------------------------------
Output 187: Average over 44 episodes - Reward: 0.7272727272727273
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 187           |
|    time_elapsed         | 708           |
|    total_timesteps      | 382976        |
| train/                  |               |
|    approx_kl            | 3.2436714e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0.242         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00778       |
|    n_updates            | 1860          |
|    policy_gradient_loss | -7.79e-05     |
|    value_loss           | 0.029         |
-------------------------------------------
Output 188: Average over 54 episodes - Reward: 0.8333333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 188           |
|    time_elapsed         | 711           |
|    total_timesteps      | 385024        |
| train/                  |               |
|    approx_kl            | 0.00079470116 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00969      |
|    explained_variance   | 0.232         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0139        |
|    n_updates            | 1870          |
|    policy_gradient_loss | -0.000665     |
|    value_loss           | 0.032         |
-------------------------------------------
Output 189: Average over 49 episodes - Reward: 0.7346938775510204
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 189           |
|    time_elapsed         | 716           |
|    total_timesteps      | 387072        |
| train/                  |               |
|    approx_kl            | 9.3299284e-05 |
|    clip_fraction        | 0.00107       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00974      |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0126        |
|    n_updates            | 1880          |
|    policy_gradient_loss | -9.87e-05     |
|    value_loss           | 0.0313        |
-------------------------------------------
Output 190: Average over 55 episodes - Reward: 0.7090909090909091
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 190          |
|    time_elapsed         | 720          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 7.729343e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00888     |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0151       |
|    n_updates            | 1890         |
|    policy_gradient_loss | -5.63e-05    |
|    value_loss           | 0.0321       |
------------------------------------------
Output 191: Average over 47 episodes - Reward: 0.8085106382978723
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 191           |
|    time_elapsed         | 724           |
|    total_timesteps      | 391168        |
| train/                  |               |
|    approx_kl            | 0.00049102306 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0.175         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0177        |
|    n_updates            | 1900          |
|    policy_gradient_loss | -0.000222     |
|    value_loss           | 0.039         |
-------------------------------------------
Output 192: Average over 48 episodes - Reward: 0.7708333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 192           |
|    time_elapsed         | 729           |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 0.00035804065 |
|    clip_fraction        | 0.00288       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0174       |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0111        |
|    n_updates            | 1910          |
|    policy_gradient_loss | -0.000198     |
|    value_loss           | 0.0268        |
-------------------------------------------
Output 193: Average over 56 episodes - Reward: 0.7678571428571429
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 193          |
|    time_elapsed         | 732          |
|    total_timesteps      | 395264       |
| train/                  |              |
|    approx_kl            | 5.997662e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0149      |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0146       |
|    n_updates            | 1920         |
|    policy_gradient_loss | -7.9e-05     |
|    value_loss           | 0.028        |
------------------------------------------
Output 194: Average over 43 episodes - Reward: 0.7209302325581395
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 194           |
|    time_elapsed         | 736           |
|    total_timesteps      | 397312        |
| train/                  |               |
|    approx_kl            | 0.00019601177 |
|    clip_fraction        | 0.00259       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.018        |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0184        |
|    n_updates            | 1930          |
|    policy_gradient_loss | -2.78e-05     |
|    value_loss           | 0.0403        |
-------------------------------------------
Output 195: Average over 45 episodes - Reward: 0.6
-----------------------------------------
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 195         |
|    time_elapsed         | 740         |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.006916265 |
|    clip_fraction        | 0.00381     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0234     |
|    explained_variance   | 0.187       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0137      |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.000332    |
|    value_loss           | 0.0311      |
-----------------------------------------
Output 196: Average over 52 episodes - Reward: 0.8076923076923077
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 196          |
|    time_elapsed         | 743          |
|    total_timesteps      | 401408       |
| train/                  |              |
|    approx_kl            | 0.0021719323 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0267      |
|    explained_variance   | 0.128        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0162       |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 0.0408       |
------------------------------------------
Output 197: Average over 53 episodes - Reward: 0.7924528301886793
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 197          |
|    time_elapsed         | 747          |
|    total_timesteps      | 403456       |
| train/                  |              |
|    approx_kl            | 0.0014951914 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0258      |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0219       |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00054     |
|    value_loss           | 0.0329       |
------------------------------------------
Output 198: Average over 45 episodes - Reward: 0.6222222222222222
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 198           |
|    time_elapsed         | 750           |
|    total_timesteps      | 405504        |
| train/                  |               |
|    approx_kl            | 0.00043018803 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0202       |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0137        |
|    n_updates            | 1970          |
|    policy_gradient_loss | -0.000872     |
|    value_loss           | 0.0353        |
-------------------------------------------
Output 199: Average over 50 episodes - Reward: 0.76
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 199          |
|    time_elapsed         | 754          |
|    total_timesteps      | 407552       |
| train/                  |              |
|    approx_kl            | 0.0028496226 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00987     |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.016        |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 0.0342       |
------------------------------------------
Output 200: Average over 54 episodes - Reward: 0.7222222222222222
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 200           |
|    time_elapsed         | 758           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 5.6840392e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00794      |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0211        |
|    n_updates            | 1990          |
|    policy_gradient_loss | -8.02e-05     |
|    value_loss           | 0.0318        |
-------------------------------------------
Output 201: Average over 54 episodes - Reward: 0.8518518518518519
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 201           |
|    time_elapsed         | 762           |
|    total_timesteps      | 411648        |
| train/                  |               |
|    approx_kl            | 0.00016894209 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00897      |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0208        |
|    n_updates            | 2000          |
|    policy_gradient_loss | -0.000165     |
|    value_loss           | 0.0416        |
-------------------------------------------
Output 202: Average over 51 episodes - Reward: 0.6470588235294118
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 202          |
|    time_elapsed         | 765          |
|    total_timesteps      | 413696       |
| train/                  |              |
|    approx_kl            | 0.0022970259 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0236      |
|    explained_variance   | 0.222        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0229       |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 0.0333       |
------------------------------------------
Output 203: Average over 47 episodes - Reward: 0.7021276595744681
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 203         |
|    time_elapsed         | 769         |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.001647685 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0228     |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0188      |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 0.0414      |
-----------------------------------------
Output 204: Average over 46 episodes - Reward: 0.7608695652173914
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 204           |
|    time_elapsed         | 773           |
|    total_timesteps      | 417792        |
| train/                  |               |
|    approx_kl            | 0.00024558365 |
|    clip_fraction        | 0.00425       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.0539        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.022         |
|    n_updates            | 2030          |
|    policy_gradient_loss | -0.000357     |
|    value_loss           | 0.0364        |
-------------------------------------------
Output 205: Average over 53 episodes - Reward: 0.7547169811320755
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 205          |
|    time_elapsed         | 777          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.0006319848 |
|    clip_fraction        | 0.00728      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0284      |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.017        |
|    n_updates            | 2040         |
|    policy_gradient_loss | 6.97e-05     |
|    value_loss           | 0.0343       |
------------------------------------------
Output 206: Average over 58 episodes - Reward: 0.7413793103448276
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 206           |
|    time_elapsed         | 780           |
|    total_timesteps      | 421888        |
| train/                  |               |
|    approx_kl            | 0.00037454264 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0244       |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.015         |
|    n_updates            | 2050          |
|    policy_gradient_loss | -0.000175     |
|    value_loss           | 0.0366        |
-------------------------------------------
Output 207: Average over 39 episodes - Reward: 0.5641025641025641
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 207          |
|    time_elapsed         | 784          |
|    total_timesteps      | 423936       |
| train/                  |              |
|    approx_kl            | 0.0053954464 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0502      |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0193       |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.0441       |
------------------------------------------
Output 208: Average over 44 episodes - Reward: 0.7045454545454546
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 208         |
|    time_elapsed         | 788         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.003601056 |
|    clip_fraction        | 0.0249      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0308     |
|    explained_variance   | 0.0725      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0163      |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 0.0359      |
-----------------------------------------
Output 209: Average over 53 episodes - Reward: 0.7358490566037735
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 209          |
|    time_elapsed         | 792          |
|    total_timesteps      | 428032       |
| train/                  |              |
|    approx_kl            | 0.0014371789 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0394      |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0204       |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.0359       |
------------------------------------------
Output 210: Average over 57 episodes - Reward: 0.6842105263157895
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 210          |
|    time_elapsed         | 796          |
|    total_timesteps      | 430080       |
| train/                  |              |
|    approx_kl            | 0.0018229876 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0372      |
|    explained_variance   | 0.191        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00797      |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 0.0419       |
------------------------------------------
Output 211: Average over 48 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 211          |
|    time_elapsed         | 800          |
|    total_timesteps      | 432128       |
| train/                  |              |
|    approx_kl            | 0.0027286268 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0381      |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0182       |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.0437       |
------------------------------------------
Output 212: Average over 51 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 212          |
|    time_elapsed         | 804          |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 0.0036124752 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0407      |
|    explained_variance   | 0.144        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0199       |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 0.0406       |
------------------------------------------
Output 213: Average over 53 episodes - Reward: 0.7547169811320755
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 213          |
|    time_elapsed         | 809          |
|    total_timesteps      | 436224       |
| train/                  |              |
|    approx_kl            | 0.0011044056 |
|    clip_fraction        | 0.00786      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0321      |
|    explained_variance   | 0.182        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0123       |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.000777    |
|    value_loss           | 0.04         |
------------------------------------------
Output 214: Average over 55 episodes - Reward: 0.8363636363636363
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 214          |
|    time_elapsed         | 813          |
|    total_timesteps      | 438272       |
| train/                  |              |
|    approx_kl            | 0.0002523387 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0361      |
|    explained_variance   | 0.133        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0229       |
|    n_updates            | 2130         |
|    policy_gradient_loss | 0.000346     |
|    value_loss           | 0.0405       |
------------------------------------------
Output 215: Average over 48 episodes - Reward: 0.625
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 215          |
|    time_elapsed         | 817          |
|    total_timesteps      | 440320       |
| train/                  |              |
|    approx_kl            | 0.0005221976 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0406      |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0135       |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.000912    |
|    value_loss           | 0.0355       |
------------------------------------------
Output 216: Average over 49 episodes - Reward: 0.673469387755102
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 216          |
|    time_elapsed         | 820          |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0009460959 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0316      |
|    explained_variance   | 0.0872       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0151       |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.000937    |
|    value_loss           | 0.038        |
------------------------------------------
Output 217: Average over 43 episodes - Reward: 0.7441860465116279
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 217           |
|    time_elapsed         | 824           |
|    total_timesteps      | 444416        |
| train/                  |               |
|    approx_kl            | 0.00080148026 |
|    clip_fraction        | 0.00947       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0271       |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0254        |
|    n_updates            | 2160          |
|    policy_gradient_loss | -0.00111      |
|    value_loss           | 0.0367        |
-------------------------------------------
Output 218: Average over 43 episodes - Reward: 0.6046511627906976
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 218          |
|    time_elapsed         | 828          |
|    total_timesteps      | 446464       |
| train/                  |              |
|    approx_kl            | 0.0009514501 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0328      |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0132       |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.000462    |
|    value_loss           | 0.0318       |
------------------------------------------
Output 219: Average over 50 episodes - Reward: 0.68
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 219           |
|    time_elapsed         | 831           |
|    total_timesteps      | 448512        |
| train/                  |               |
|    approx_kl            | 0.00079814205 |
|    clip_fraction        | 0.0132        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0289       |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0146        |
|    n_updates            | 2180          |
|    policy_gradient_loss | -0.00135      |
|    value_loss           | 0.0368        |
-------------------------------------------
Output 220: Average over 48 episodes - Reward: 0.6875
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 220           |
|    time_elapsed         | 835           |
|    total_timesteps      | 450560        |
| train/                  |               |
|    approx_kl            | 0.00061035913 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0369       |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00256      |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000848     |
|    value_loss           | 0.0362        |
-------------------------------------------
Output 221: Average over 51 episodes - Reward: 0.6470588235294118
-------------------------------------------
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 221           |
|    time_elapsed         | 838           |
|    total_timesteps      | 452608        |
| train/                  |               |
|    approx_kl            | 0.00042535705 |
|    clip_fraction        | 0.00635       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0281       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0133        |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.000702     |
|    value_loss           | 0.0343        |
-------------------------------------------
Output 222: Average over 45 episodes - Reward: 0.7555555555555555
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 222          |
|    time_elapsed         | 842          |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 0.0009860324 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0413      |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0184       |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 0.0395       |
------------------------------------------
Output 223: Average over 45 episodes - Reward: 0.5555555555555556
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 223          |
|    time_elapsed         | 846          |
|    total_timesteps      | 456704       |
| train/                  |              |
|    approx_kl            | 0.0010126103 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0349      |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0167       |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00186     |
|    value_loss           | 0.0303       |
------------------------------------------
Output 224: Average over 44 episodes - Reward: 0.6818181818181818
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 224          |
|    time_elapsed         | 849          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0012086111 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0326      |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0278       |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.000585    |
|    value_loss           | 0.0367       |
------------------------------------------
Output 225: Average over 51 episodes - Reward: 0.7450980392156863
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 225          |
|    time_elapsed         | 853          |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 0.0011606333 |
|    clip_fraction        | 0.00903      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0277      |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0184       |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00236     |
|    value_loss           | 0.0338       |
------------------------------------------
Output 226: Average over 50 episodes - Reward: 0.7
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 226           |
|    time_elapsed         | 856           |
|    total_timesteps      | 462848        |
| train/                  |               |
|    approx_kl            | 0.00028438223 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.025        |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0154        |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.000233     |
|    value_loss           | 0.036         |
-------------------------------------------
Output 227: Average over 43 episodes - Reward: 0.5813953488372093
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 227          |
|    time_elapsed         | 860          |
|    total_timesteps      | 464896       |
| train/                  |              |
|    approx_kl            | 0.0003875804 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0259      |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0193       |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00012     |
|    value_loss           | 0.0402       |
------------------------------------------
Output 228: Average over 44 episodes - Reward: 0.6590909090909091
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 228         |
|    time_elapsed         | 863         |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.001020036 |
|    clip_fraction        | 0.00278     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0198     |
|    explained_variance   | 0.165       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0167      |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.000824   |
|    value_loss           | 0.0312      |
-----------------------------------------
Output 229: Average over 52 episodes - Reward: 0.7307692307692307
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 229           |
|    time_elapsed         | 867           |
|    total_timesteps      | 468992        |
| train/                  |               |
|    approx_kl            | 0.00071953365 |
|    clip_fraction        | 0.00562       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0314       |
|    explained_variance   | 0.222         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0225        |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.000377     |
|    value_loss           | 0.0334        |
-------------------------------------------
Output 230: Average over 47 episodes - Reward: 0.7659574468085106
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 230         |
|    time_elapsed         | 871         |
|    total_timesteps      | 471040      |
| train/                  |             |
|    approx_kl            | 0.002662906 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0326     |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0274      |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 0.0417      |
-----------------------------------------
Output 231: Average over 46 episodes - Reward: 0.7391304347826086
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 231           |
|    time_elapsed         | 875           |
|    total_timesteps      | 473088        |
| train/                  |               |
|    approx_kl            | 0.00027930085 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0161       |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0163        |
|    n_updates            | 2300          |
|    policy_gradient_loss | -0.000661     |
|    value_loss           | 0.0317        |
-------------------------------------------
Output 232: Average over 55 episodes - Reward: 0.7454545454545455
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 232         |
|    time_elapsed         | 879         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.000832267 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0204     |
|    explained_variance   | 0.19        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0157      |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 0.0361      |
-----------------------------------------
Output 233: Average over 40 episodes - Reward: 0.6
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 233          |
|    time_elapsed         | 883          |
|    total_timesteps      | 477184       |
| train/                  |              |
|    approx_kl            | 0.0002390336 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0161      |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0191       |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.000527    |
|    value_loss           | 0.0393       |
------------------------------------------
Output 234: Average over 47 episodes - Reward: 0.7872340425531915
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 234           |
|    time_elapsed         | 886           |
|    total_timesteps      | 479232        |
| train/                  |               |
|    approx_kl            | 0.00037290648 |
|    clip_fraction        | 0.00532       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.022        |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0148        |
|    n_updates            | 2330          |
|    policy_gradient_loss | -0.000585     |
|    value_loss           | 0.0297        |
-------------------------------------------
Output 235: Average over 44 episodes - Reward: 0.6590909090909091
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 235          |
|    time_elapsed         | 890          |
|    total_timesteps      | 481280       |
| train/                  |              |
|    approx_kl            | 0.0017024092 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.025       |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0113       |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.000551    |
|    value_loss           | 0.0337       |
------------------------------------------
Output 236: Average over 34 episodes - Reward: 0.5294117647058824
-------------------------------------------
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 236           |
|    time_elapsed         | 894           |
|    total_timesteps      | 483328        |
| train/                  |               |
|    approx_kl            | 0.00021850673 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0319       |
|    explained_variance   | 0.21          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0147        |
|    n_updates            | 2350          |
|    policy_gradient_loss | 9.52e-05      |
|    value_loss           | 0.0321        |
-------------------------------------------
Output 237: Average over 45 episodes - Reward: 0.7111111111111111
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 237         |
|    time_elapsed         | 897         |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.007536929 |
|    clip_fraction        | 0.00947     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0268     |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00279    |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 0.0214      |
-----------------------------------------
Output 238: Average over 39 episodes - Reward: 0.6410256410256411
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 238         |
|    time_elapsed         | 901         |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.004398587 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0228     |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0299      |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 0.0348      |
-----------------------------------------
Output 239: Average over 47 episodes - Reward: 0.6595744680851063
-------------------------------------------
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 239           |
|    time_elapsed         | 904           |
|    total_timesteps      | 489472        |
| train/                  |               |
|    approx_kl            | 0.00017323371 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0241       |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0113        |
|    n_updates            | 2380          |
|    policy_gradient_loss | 8.94e-05      |
|    value_loss           | 0.0278        |
-------------------------------------------
Output 240: Average over 54 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 240          |
|    time_elapsed         | 907          |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0006243846 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0244      |
|    explained_variance   | 0.217        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00653      |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.000504    |
|    value_loss           | 0.0337       |
------------------------------------------
Output 241: Average over 44 episodes - Reward: 0.7272727272727273
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 241          |
|    time_elapsed         | 911          |
|    total_timesteps      | 493568       |
| train/                  |              |
|    approx_kl            | 0.0019304489 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0222      |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0195       |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.000836    |
|    value_loss           | 0.0426       |
------------------------------------------
Output 242: Average over 47 episodes - Reward: 0.723404255319149
-------------------------------------------
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 242           |
|    time_elapsed         | 915           |
|    total_timesteps      | 495616        |
| train/                  |               |
|    approx_kl            | 0.00019371975 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00797      |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0236        |
|    n_updates            | 2410          |
|    policy_gradient_loss | -0.000361     |
|    value_loss           | 0.0395        |
-------------------------------------------
Output 243: Average over 46 episodes - Reward: 0.8260869565217391
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 243          |
|    time_elapsed         | 919          |
|    total_timesteps      | 497664       |
| train/                  |              |
|    approx_kl            | 0.0025241943 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0275      |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0065       |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 0.035        |
------------------------------------------
Output 244: Average over 49 episodes - Reward: 0.7346938775510204
-------------------------------------------
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 244           |
|    time_elapsed         | 922           |
|    total_timesteps      | 499712        |
| train/                  |               |
|    approx_kl            | 0.00081850926 |
|    clip_fraction        | 0.00732       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00715       |
|    n_updates            | 2430          |
|    policy_gradient_loss | -0.000469     |
|    value_loss           | 0.0257        |
-------------------------------------------
Output 245: Average over 45 episodes - Reward: 0.7111111111111111
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 245          |
|    time_elapsed         | 926          |
|    total_timesteps      | 501760       |
| train/                  |              |
|    approx_kl            | 0.0006769445 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0185      |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0163       |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.000464    |
|    value_loss           | 0.0314       |
------------------------------------------
Overall: Average Reward: 0.589343582786947
Using cpu device
Logging to ./PPOtensorboard/PPO_3
Output 1: Average over 66 episodes - Reward: 0.0
-----------------------------
| time/              |      |
|    fps             | 1061 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 2048 |
-----------------------------
Output 2: Average over 60 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.00955623 |
|    clip_fraction        | 0.039      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -42.3      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0104    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00942   |
|    value_loss           | 0.00396    |
----------------------------------------
Output 3: Average over 81 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 640        |
|    iterations           | 3          |
|    time_elapsed         | 9          |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.01566854 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -3.58      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0388    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.000322   |
----------------------------------------
Output 4: Average over 96 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 602         |
|    iterations           | 4           |
|    time_elapsed         | 13          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.018199803 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | -2.45       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0249     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 2.69e-05    |
-----------------------------------------
Output 5: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 596         |
|    iterations           | 5           |
|    time_elapsed         | 17          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.014350644 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | -8.98       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 1.51e-05    |
-----------------------------------------
Output 6: Average over 81 episodes - Reward: 0.0
---------------------------------------
| time/                   |           |
|    fps                  | 599       |
|    iterations           | 6         |
|    time_elapsed         | 20        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.0227642 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.17     |
|    explained_variance   | -5.19     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0307   |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0235   |
|    value_loss           | 4.13e-07  |
---------------------------------------
Output 7: Average over 43 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 591         |
|    iterations           | 7           |
|    time_elapsed         | 24          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.018991759 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | -4.36       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 5.97e-08    |
-----------------------------------------
Output 8: Average over 38 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 593         |
|    iterations           | 8           |
|    time_elapsed         | 27          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.026966976 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | -0.942      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0382     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0283     |
|    value_loss           | 5.09e-09    |
-----------------------------------------
Output 9: Average over 28 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 592         |
|    iterations           | 9           |
|    time_elapsed         | 31          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.013402104 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | -0.176      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0131     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 1.44e-09    |
-----------------------------------------
Output 10: Average over 33 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 590         |
|    iterations           | 10          |
|    time_elapsed         | 34          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.016367413 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | -1.98       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0328      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 4.63e-10    |
-----------------------------------------
Output 11: Average over 23 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 591         |
|    iterations           | 11          |
|    time_elapsed         | 38          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.008981561 |
|    clip_fraction        | 0.0982      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.938      |
|    explained_variance   | -0.129      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000868    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 2.04e-10    |
-----------------------------------------
Output 12: Average over 26 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 590         |
|    iterations           | 12          |
|    time_elapsed         | 41          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.007639297 |
|    clip_fraction        | 0.0567      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.928      |
|    explained_variance   | -0.584      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00501     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00755    |
|    value_loss           | 7.07e-11    |
-----------------------------------------
Output 13: Average over 23 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 593         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.007142221 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.915      |
|    explained_variance   | -0.233      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0321     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 6e-11       |
-----------------------------------------
Output 14: Average over 22 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 593         |
|    iterations           | 14          |
|    time_elapsed         | 48          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.014465157 |
|    clip_fraction        | 0.0639      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.961      |
|    explained_variance   | -0.107      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00993    |
|    value_loss           | 4.76e-11    |
-----------------------------------------
Output 15: Average over 21 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 594         |
|    iterations           | 15          |
|    time_elapsed         | 51          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.008315325 |
|    clip_fraction        | 0.0524      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.986      |
|    explained_variance   | -0.306      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00725    |
|    value_loss           | 1.34e-10    |
-----------------------------------------
Output 16: Average over 22 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 594         |
|    iterations           | 16          |
|    time_elapsed         | 55          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.030353658 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | -8.91       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0423     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 1.69e-08    |
-----------------------------------------
Output 17: Average over 20 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 592          |
|    iterations           | 17           |
|    time_elapsed         | 58           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0075773764 |
|    clip_fraction        | 0.0686       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.919       |
|    explained_variance   | -4.13        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00834     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00809     |
|    value_loss           | 1.11e-08     |
------------------------------------------
Output 18: Average over 21 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 591        |
|    iterations           | 18         |
|    time_elapsed         | 62         |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.00994532 |
|    clip_fraction        | 0.0387     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.89      |
|    explained_variance   | -1.29      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.012     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.00679   |
|    value_loss           | 2.63e-07   |
----------------------------------------
Output 19: Average over 20 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 591          |
|    iterations           | 19           |
|    time_elapsed         | 65           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0070009637 |
|    clip_fraction        | 0.0705       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.856       |
|    explained_variance   | -1.09        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0126       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00653     |
|    value_loss           | 1.24e-10     |
------------------------------------------
Output 20: Average over 21 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 586         |
|    iterations           | 20          |
|    time_elapsed         | 69          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.003147388 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.9        |
|    explained_variance   | -14.1       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00165    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 3.97e-07    |
-----------------------------------------
Output 21: Average over 28 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 585        |
|    iterations           | 21         |
|    time_elapsed         | 73         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.04021986 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.937     |
|    explained_variance   | -7.35      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0531    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0277    |
|    value_loss           | 1.46e-11   |
----------------------------------------
Output 22: Average over 28 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 582         |
|    iterations           | 22          |
|    time_elapsed         | 77          |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.004611636 |
|    clip_fraction        | 0.0419      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.913      |
|    explained_variance   | -45.8       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0309     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.4e-10     |
-----------------------------------------
Output 23: Average over 22 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 580         |
|    iterations           | 23          |
|    time_elapsed         | 81          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.009748638 |
|    clip_fraction        | 0.0687      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | -8.33       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 2.78e-11    |
-----------------------------------------
Output 24: Average over 24 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 579         |
|    iterations           | 24          |
|    time_elapsed         | 84          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009819889 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.75       |
|    explained_variance   | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0269     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 5.27e-07    |
-----------------------------------------
Output 25: Average over 25 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 578         |
|    iterations           | 25          |
|    time_elapsed         | 88          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.005283555 |
|    clip_fraction        | 0.0595      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | -117        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0329     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 3.69e-11    |
-----------------------------------------
Output 26: Average over 23 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 577         |
|    iterations           | 26          |
|    time_elapsed         | 92          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.008509366 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.695      |
|    explained_variance   | -1.55       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0429     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 2.06e-09    |
-----------------------------------------
Output 27: Average over 23 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 576         |
|    iterations           | 27          |
|    time_elapsed         | 95          |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.005622804 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | -0.671      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00442    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 7.83e-08    |
-----------------------------------------
Output 28: Average over 21 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 573          |
|    iterations           | 28           |
|    time_elapsed         | 100          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0069093453 |
|    clip_fraction        | 0.0689       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.626       |
|    explained_variance   | -2.44        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0356      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.0101      |
|    value_loss           | 2.54e-07     |
------------------------------------------
Output 29: Average over 21 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 567         |
|    iterations           | 29          |
|    time_elapsed         | 104         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.005159198 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | -1.97       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0184     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 4.01e-07    |
-----------------------------------------
Output 30: Average over 23 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 562         |
|    iterations           | 30          |
|    time_elapsed         | 109         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.004414201 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.68       |
|    explained_variance   | -6.13       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0146     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 1.8e-08     |
-----------------------------------------
Output 31: Average over 21 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 560         |
|    iterations           | 31          |
|    time_elapsed         | 113         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.012246929 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.621      |
|    explained_variance   | -1.93       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0205      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 7.98e-08    |
-----------------------------------------
Output 32: Average over 21 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 560          |
|    iterations           | 32           |
|    time_elapsed         | 116          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0054439204 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.604       |
|    explained_variance   | -4.31        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00515      |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0088      |
|    value_loss           | 6.81e-07     |
------------------------------------------
Output 33: Average over 20 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 560          |
|    iterations           | 33           |
|    time_elapsed         | 120          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0038668327 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.544       |
|    explained_variance   | -1.79        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00313      |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00726     |
|    value_loss           | 8.97e-10     |
------------------------------------------
Output 34: Average over 21 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 560          |
|    iterations           | 34           |
|    time_elapsed         | 124          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0059441226 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.502       |
|    explained_variance   | -0.25        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00443     |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00354     |
|    value_loss           | 3.36e-08     |
------------------------------------------
Output 35: Average over 21 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 559          |
|    iterations           | 35           |
|    time_elapsed         | 128          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0031801343 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | -0.21        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00695      |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 3.05e-07     |
------------------------------------------
Output 36: Average over 21 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 557         |
|    iterations           | 36          |
|    time_elapsed         | 132         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.006066682 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | -4.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00161     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 1.32e-06    |
-----------------------------------------
Output 37: Average over 20 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 555         |
|    iterations           | 37          |
|    time_elapsed         | 136         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.011733443 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.55       |
|    explained_variance   | -7.46       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0511     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 2.85e-09    |
-----------------------------------------
Output 38: Average over 21 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 38           |
|    time_elapsed         | 140          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0035952302 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.579       |
|    explained_variance   | -12.6        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00899     |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00587     |
|    value_loss           | 1.29e-09     |
------------------------------------------
Output 39: Average over 24 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 39           |
|    time_elapsed         | 144          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0053893966 |
|    clip_fraction        | 0.0595       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.577       |
|    explained_variance   | -0.633       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0467      |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00952     |
|    value_loss           | 4.47e-10     |
------------------------------------------
Output 40: Average over 23 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 40          |
|    time_elapsed         | 147         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.005165132 |
|    clip_fraction        | 0.0498      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.607      |
|    explained_variance   | -1.33       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.017      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0093     |
|    value_loss           | 3.38e-09    |
-----------------------------------------
Output 41: Average over 29 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 41           |
|    time_elapsed         | 151          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0058569466 |
|    clip_fraction        | 0.0746       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.634       |
|    explained_variance   | -19.3        |
|    learning_rate        | 0.0003       |
|    loss                 | -8.77e-05    |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 2.01e-09     |
------------------------------------------
Output 42: Average over 34 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 42          |
|    time_elapsed         | 155         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.012088793 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.678      |
|    explained_variance   | -2.64       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00813    |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 1.52e-08    |
-----------------------------------------
Output 43: Average over 39 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 43          |
|    time_elapsed         | 159         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.005060465 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | -0.461      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0149     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00886    |
|    value_loss           | 1.36e-07    |
-----------------------------------------
Output 44: Average over 36 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 44          |
|    time_elapsed         | 163         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.006975901 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.612      |
|    explained_variance   | -6.58       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0164     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 8.42e-08    |
-----------------------------------------
Output 45: Average over 43 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 45          |
|    time_elapsed         | 168         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.008818854 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.649      |
|    explained_variance   | -1.05       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 7.49e-07    |
-----------------------------------------
Output 46: Average over 45 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 46          |
|    time_elapsed         | 172         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.009849883 |
|    clip_fraction        | 0.0842      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.661      |
|    explained_variance   | -0.729      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0395     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 2.01e-07    |
-----------------------------------------
Output 47: Average over 38 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 47          |
|    time_elapsed         | 176         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.004301423 |
|    clip_fraction        | 0.0428      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.595      |
|    explained_variance   | -3.26       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00832    |
|    value_loss           | 3.98e-07    |
-----------------------------------------
Output 48: Average over 31 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 48          |
|    time_elapsed         | 180         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.006988625 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.539      |
|    explained_variance   | -4.09       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000367    |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00922    |
|    value_loss           | 4.15e-08    |
-----------------------------------------
Output 49: Average over 28 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 49           |
|    time_elapsed         | 183          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0064564827 |
|    clip_fraction        | 0.0758       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.474       |
|    explained_variance   | -0.605       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0167      |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 3.57e-08     |
------------------------------------------
Output 50: Average over 31 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 50          |
|    time_elapsed         | 187         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.009557342 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | -1.05       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0326      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.05e-06    |
-----------------------------------------
Output 51: Average over 39 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 51          |
|    time_elapsed         | 191         |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.009118833 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.11        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00869    |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 2.92e-08    |
-----------------------------------------
Output 52: Average over 58 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 52          |
|    time_elapsed         | 195         |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.012226559 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.736      |
|    explained_variance   | -0.315      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0414     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 3.89e-08    |
-----------------------------------------
Output 53: Average over 69 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 53          |
|    time_elapsed         | 199         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.012715671 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.821      |
|    explained_variance   | -2.82       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 2.58e-08    |
-----------------------------------------
Output 54: Average over 100 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 544        |
|    iterations           | 54         |
|    time_elapsed         | 203        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.01670549 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.859     |
|    explained_variance   | -4.27      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0253    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 3.43e-08   |
----------------------------------------
Output 55: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 55          |
|    time_elapsed         | 206         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.012626646 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.904      |
|    explained_variance   | -15.8       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 1.16e-06    |
-----------------------------------------
Output 56: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 56          |
|    time_elapsed         | 210         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.005975879 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.885      |
|    explained_variance   | -33.5       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00682     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 1.29e-05    |
-----------------------------------------
Output 57: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 57          |
|    time_elapsed         | 214         |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.012422494 |
|    clip_fraction        | 0.0821      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.801      |
|    explained_variance   | -50.4       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0387     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 8.55e-08    |
-----------------------------------------
Output 58: Average over 100 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 58           |
|    time_elapsed         | 217          |
|    total_timesteps      | 118784       |
| train/                  |              |
|    approx_kl            | 0.0069034295 |
|    clip_fraction        | 0.082        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.808       |
|    explained_variance   | -19.5        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00482     |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 2.35e-06     |
------------------------------------------
Output 59: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 59          |
|    time_elapsed         | 221         |
|    total_timesteps      | 120832      |
| train/                  |             |
|    approx_kl            | 0.013671358 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | -3.68       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0465     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 1.85e-07    |
-----------------------------------------
Output 60: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 60          |
|    time_elapsed         | 224         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.012306709 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.866      |
|    explained_variance   | -3.76       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 7.74e-07    |
-----------------------------------------
Output 61: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 61          |
|    time_elapsed         | 228         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.008398833 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.903      |
|    explained_variance   | -1.62       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00182    |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 3.14e-07    |
-----------------------------------------
Output 62: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 62          |
|    time_elapsed         | 231         |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.015439628 |
|    clip_fraction        | 0.0951      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.929      |
|    explained_variance   | -8.42       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00164    |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 1.16e-07    |
-----------------------------------------
Output 63: Average over 99 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 548         |
|    iterations           | 63          |
|    time_elapsed         | 235         |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.020197857 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.934      |
|    explained_variance   | -11         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0517     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 6.92e-08    |
-----------------------------------------
Output 64: Average over 42 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 64          |
|    time_elapsed         | 238         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.019806769 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | -2.62       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0211     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 1.15e-08    |
-----------------------------------------
Output 65: Average over 34 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 65          |
|    time_elapsed         | 242         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.024046762 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | -1.8        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0159      |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 2.58e-07    |
-----------------------------------------
Output 66: Average over 27 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 66          |
|    time_elapsed         | 245         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.013581952 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.806      |
|    explained_variance   | -1.26       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 2.91e-07    |
-----------------------------------------
Output 67: Average over 29 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 67          |
|    time_elapsed         | 249         |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.040228367 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.84       |
|    explained_variance   | -2.55       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 3.58e-09    |
-----------------------------------------
Output 68: Average over 24 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 68          |
|    time_elapsed         | 254         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.020994067 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | -6.41       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0203     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 3.19e-07    |
-----------------------------------------
Output 69: Average over 32 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 547        |
|    iterations           | 69         |
|    time_elapsed         | 258        |
|    total_timesteps      | 141312     |
| train/                  |            |
|    approx_kl            | 0.02153644 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.89      |
|    explained_variance   | -2.73      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000839   |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.00949   |
|    value_loss           | 8.66e-08   |
----------------------------------------
Output 70: Average over 43 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 70          |
|    time_elapsed         | 262         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.013302613 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.879      |
|    explained_variance   | -8.01       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0254      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00803    |
|    value_loss           | 1.36e-08    |
-----------------------------------------
Output 71: Average over 30 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 71         |
|    time_elapsed         | 266        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.02035419 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.823     |
|    explained_variance   | -13.2      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0151    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0122    |
|    value_loss           | 8.2e-10    |
----------------------------------------
Output 72: Average over 29 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 72          |
|    time_elapsed         | 276         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.008115258 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.812      |
|    explained_variance   | -5.1        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0147     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00889    |
|    value_loss           | 3.74e-07    |
-----------------------------------------
Output 73: Average over 25 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 532        |
|    iterations           | 73         |
|    time_elapsed         | 280        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.03493721 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.706     |
|    explained_variance   | -2.62      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0276    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0229    |
|    value_loss           | 8.99e-10   |
----------------------------------------
Output 74: Average over 25 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 532         |
|    iterations           | 74          |
|    time_elapsed         | 284         |
|    total_timesteps      | 151552      |
| train/                  |             |
|    approx_kl            | 0.028224444 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.769      |
|    explained_variance   | -7.42       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.107      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 6.23e-06    |
-----------------------------------------
Output 75: Average over 24 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 532         |
|    iterations           | 75          |
|    time_elapsed         | 288         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.010485093 |
|    clip_fraction        | 0.0696      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.745      |
|    explained_variance   | -6.15       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0268     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 8.55e-09    |
-----------------------------------------
Output 76: Average over 24 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 76           |
|    time_elapsed         | 291          |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 0.0060139643 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.713       |
|    explained_variance   | -4.79        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0206      |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.0065      |
|    value_loss           | 1.75e-10     |
------------------------------------------
Output 77: Average over 21 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 77          |
|    time_elapsed         | 295         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.007754194 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.712      |
|    explained_variance   | -17.4       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0004     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 4.52e-11    |
-----------------------------------------
Output 78: Average over 21 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 78          |
|    time_elapsed         | 299         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.004456833 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.702      |
|    explained_variance   | -133        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0228     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 5.24e-10    |
-----------------------------------------
Output 79: Average over 23 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 79           |
|    time_elapsed         | 302          |
|    total_timesteps      | 161792       |
| train/                  |              |
|    approx_kl            | 0.0098529775 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.681       |
|    explained_variance   | -13          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0132      |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00566     |
|    value_loss           | 6.88e-12     |
------------------------------------------
Output 80: Average over 29 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 80          |
|    time_elapsed         | 306         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.015515402 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.617      |
|    explained_variance   | 0.00726     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 1.21e-10    |
-----------------------------------------
Output 81: Average over 25 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 81           |
|    time_elapsed         | 310          |
|    total_timesteps      | 165888       |
| train/                  |              |
|    approx_kl            | 0.0052757827 |
|    clip_fraction        | 0.0577       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.558       |
|    explained_variance   | -34.6        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0101       |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 3.01e-11     |
------------------------------------------
Output 82: Average over 25 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 82          |
|    time_elapsed         | 314         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.005551745 |
|    clip_fraction        | 0.0632      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.605      |
|    explained_variance   | -5.24       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0119     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 4.35e-11    |
-----------------------------------------
Output 83: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 83          |
|    time_elapsed         | 318         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.063710526 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.708      |
|    explained_variance   | -1.73       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0677     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 1.17e-06    |
-----------------------------------------
Output 84: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 84          |
|    time_elapsed         | 323         |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.014132043 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.85       |
|    explained_variance   | -22.1       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0287     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 5.75e-10    |
-----------------------------------------
Output 85: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 85          |
|    time_elapsed         | 327         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.011629807 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | -71.8       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0349     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 8.33e-09    |
-----------------------------------------
Output 86: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 86          |
|    time_elapsed         | 331         |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.012347361 |
|    clip_fraction        | 0.0836      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.857      |
|    explained_variance   | -16.9       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0106     |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00903    |
|    value_loss           | 5.73e-08    |
-----------------------------------------
Output 87: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 87          |
|    time_elapsed         | 336         |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.025538405 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.851      |
|    explained_variance   | -13.5       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0659      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 3.07e-08    |
-----------------------------------------
Output 88: Average over 100 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 88           |
|    time_elapsed         | 339          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0062472364 |
|    clip_fraction        | 0.039        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.82        |
|    explained_variance   | -0.0114      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0239      |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00895     |
|    value_loss           | 0.000818     |
------------------------------------------
Output 89: Average over 100 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 530        |
|    iterations           | 89         |
|    time_elapsed         | 343        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.01113071 |
|    clip_fraction        | 0.0902     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.809     |
|    explained_variance   | -0.602     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00152   |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 5.5e-05    |
----------------------------------------
Output 90: Average over 100 episodes - Reward: 0.01
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 90          |
|    time_elapsed         | 348         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.011364978 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0189     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.000345    |
-----------------------------------------
Output 91: Average over 100 episodes - Reward: 0.08
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 91          |
|    time_elapsed         | 352         |
|    total_timesteps      | 186368      |
| train/                  |             |
|    approx_kl            | 0.017656239 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.804      |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0294     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.00873     |
-----------------------------------------
Output 92: Average over 100 episodes - Reward: 0.21
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 92          |
|    time_elapsed         | 357         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.020156778 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.772      |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00215    |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.0316      |
-----------------------------------------
Output 93: Average over 100 episodes - Reward: 0.39
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 93          |
|    time_elapsed         | 360         |
|    total_timesteps      | 190464      |
| train/                  |             |
|    approx_kl            | 0.021701064 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.0601      |
-----------------------------------------
Output 94: Average over 100 episodes - Reward: 0.59
----------------------------------------
| time/                   |            |
|    fps                  | 527        |
|    iterations           | 94         |
|    time_elapsed         | 364        |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.01280774 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0163     |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0284    |
|    value_loss           | 0.0712     |
----------------------------------------
Output 95: Average over 100 episodes - Reward: 0.76
----------------------------------------
| time/                   |            |
|    fps                  | 528        |
|    iterations           | 95         |
|    time_elapsed         | 368        |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.01367118 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.559     |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00784    |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.0725     |
----------------------------------------
Output 96: Average over 100 episodes - Reward: 0.86
----------------------------------------
| time/                   |            |
|    fps                  | 528        |
|    iterations           | 96         |
|    time_elapsed         | 372        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.01650661 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.535     |
|    explained_variance   | 0.273      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00575   |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0215    |
|    value_loss           | 0.0594     |
----------------------------------------
Output 97: Average over 100 episodes - Reward: 0.88
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 97          |
|    time_elapsed         | 375         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.008901229 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0244      |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0417      |
-----------------------------------------
Output 98: Average over 100 episodes - Reward: 0.9
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 98          |
|    time_elapsed         | 379         |
|    total_timesteps      | 200704      |
| train/                  |             |
|    approx_kl            | 0.005698063 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00462     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.029       |
-----------------------------------------
Output 99: Average over 100 episodes - Reward: 0.92
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 99          |
|    time_elapsed         | 383         |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.005716242 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00289     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.03        |
-----------------------------------------
Output 100: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 100         |
|    time_elapsed         | 387         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.008601718 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.335      |
|    explained_variance   | 0.198       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000534   |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.0218      |
-----------------------------------------
Output 101: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 101         |
|    time_elapsed         | 391         |
|    total_timesteps      | 206848      |
| train/                  |             |
|    approx_kl            | 0.010685238 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.299      |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0442     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.00493     |
-----------------------------------------
Output 102: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 102          |
|    time_elapsed         | 396          |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 0.0069757365 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.251       |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0141      |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00894     |
|    value_loss           | 0.00916      |
------------------------------------------
Output 103: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 103         |
|    time_elapsed         | 400         |
|    total_timesteps      | 210944      |
| train/                  |             |
|    approx_kl            | 0.013260649 |
|    clip_fraction        | 0.0548      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.236       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0234     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.00374     |
-----------------------------------------
Output 104: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 104         |
|    time_elapsed         | 405         |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.009351274 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 3.4e-05     |
-----------------------------------------
Output 105: Average over 100 episodes - Reward: 0.97
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 105         |
|    time_elapsed         | 408         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.007741304 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0102     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.000958   |
|    value_loss           | 0.00299     |
-----------------------------------------
Output 106: Average over 100 episodes - Reward: 0.98
----------------------------------------
| time/                   |            |
|    fps                  | 524        |
|    iterations           | 106        |
|    time_elapsed         | 413        |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.01135004 |
|    clip_fraction        | 0.0772     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.179      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00639   |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.00729   |
|    value_loss           | 0.00913    |
----------------------------------------
Output 107: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 522        |
|    iterations           | 107        |
|    time_elapsed         | 419        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.00827909 |
|    clip_fraction        | 0.0416     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0911    |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00452    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.00714   |
|    value_loss           | 0.0102     |
----------------------------------------
Output 108: Average over 100 episodes - Reward: 0.98
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 108          |
|    time_elapsed         | 422          |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 0.0028982228 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0757      |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00402      |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00497     |
|    value_loss           | 0.00289      |
------------------------------------------
Output 109: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 109         |
|    time_elapsed         | 426         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.005481352 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.062      |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0148     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 0.00389     |
-----------------------------------------
Output 110: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 110          |
|    time_elapsed         | 430          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.0025658521 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0527      |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0028      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 0.00302      |
------------------------------------------
Output 111: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 111          |
|    time_elapsed         | 434          |
|    total_timesteps      | 227328       |
| train/                  |              |
|    approx_kl            | 0.0014019976 |
|    clip_fraction        | 0.00674      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0404      |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000515    |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00233     |
|    value_loss           | 8.9e-06      |
------------------------------------------
Output 112: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 112          |
|    time_elapsed         | 438          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0059091668 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0467      |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00764     |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00433     |
|    value_loss           | 0.00303      |
------------------------------------------
Output 113: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 113          |
|    time_elapsed         | 441          |
|    total_timesteps      | 231424       |
| train/                  |              |
|    approx_kl            | 0.0017481251 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0415      |
|    explained_variance   | 0.448        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000808    |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00418     |
|    value_loss           | 0.000374     |
------------------------------------------
Output 114: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 114         |
|    time_elapsed         | 445         |
|    total_timesteps      | 233472      |
| train/                  |             |
|    approx_kl            | 0.010577077 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0238     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00676    |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 2.5e-06     |
-----------------------------------------
Output 115: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 115          |
|    time_elapsed         | 448          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0062539554 |
|    clip_fraction        | 0.00322      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.033       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00559      |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00259     |
|    value_loss           | 0.00409      |
------------------------------------------
Output 116: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 116          |
|    time_elapsed         | 452          |
|    total_timesteps      | 237568       |
| train/                  |              |
|    approx_kl            | 0.0074264808 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0724      |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0222      |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00247     |
|    value_loss           | 1.02e-05     |
------------------------------------------
Output 117: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 117         |
|    time_elapsed         | 456         |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.015120713 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0819     |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00461    |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 2.49e-05    |
-----------------------------------------
Output 118: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 118         |
|    time_elapsed         | 459         |
|    total_timesteps      | 241664      |
| train/                  |             |
|    approx_kl            | 0.009952472 |
|    clip_fraction        | 0.00884     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0523     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00998    |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 5.08e-06    |
-----------------------------------------
Output 119: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 119         |
|    time_elapsed         | 463         |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.017145326 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0652     |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0207     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00636    |
|    value_loss           | 9.34e-07    |
-----------------------------------------
Output 120: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 120         |
|    time_elapsed         | 467         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.008581959 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0015     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.00273     |
-----------------------------------------
Output 121: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 121         |
|    time_elapsed         | 470         |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.004646722 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00112     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 3.38e-05    |
-----------------------------------------
Output 122: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 122         |
|    time_elapsed         | 474         |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.014251228 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0732     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.019      |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 2.19e-05    |
-----------------------------------------
Output 123: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 123         |
|    time_elapsed         | 477         |
|    total_timesteps      | 251904      |
| train/                  |             |
|    approx_kl            | 0.005732785 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0525     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00159    |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 4.9e-06     |
-----------------------------------------
Output 124: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 124         |
|    time_elapsed         | 481         |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.013754174 |
|    clip_fraction        | 0.0817      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0421     |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 1.55e-06    |
-----------------------------------------
Output 125: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 125         |
|    time_elapsed         | 484         |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.026324498 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 7.3e-05     |
-----------------------------------------
Output 126: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 126         |
|    time_elapsed         | 488         |
|    total_timesteps      | 258048      |
| train/                  |             |
|    approx_kl            | 0.015886866 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00853    |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 4.6e-05     |
-----------------------------------------
Output 127: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 127         |
|    time_elapsed         | 491         |
|    total_timesteps      | 260096      |
| train/                  |             |
|    approx_kl            | 0.011791678 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0739     |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.029      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 1.14e-05    |
-----------------------------------------
Output 128: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 529        |
|    iterations           | 128        |
|    time_elapsed         | 495        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.00629388 |
|    clip_fraction        | 0.0325     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0565    |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0169    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.00935   |
|    value_loss           | 2.65e-06   |
----------------------------------------
Output 129: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 129         |
|    time_elapsed         | 499         |
|    total_timesteps      | 264192      |
| train/                  |             |
|    approx_kl            | 0.011957061 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0623     |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.025      |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 3.76e-08    |
-----------------------------------------
Output 130: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 130         |
|    time_elapsed         | 502         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.058459826 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0989     |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0212     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.000106    |
-----------------------------------------
Output 131: Average over 100 episodes - Reward: 0.92
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 131         |
|    time_elapsed         | 506         |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.032891635 |
|    clip_fraction        | 0.0228      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0545     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.017      |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.35e-05    |
-----------------------------------------
Output 132: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 132         |
|    time_elapsed         | 511         |
|    total_timesteps      | 270336      |
| train/                  |             |
|    approx_kl            | 0.013935834 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0974     |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.018       |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.0304      |
-----------------------------------------
Output 133: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 528        |
|    iterations           | 133        |
|    time_elapsed         | 515        |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.07456036 |
|    clip_fraction        | 0.0839     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0929    |
|    explained_variance   | 0.305      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0539    |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 0.00332    |
----------------------------------------
Output 134: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 134         |
|    time_elapsed         | 518         |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.022734342 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0368     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.000256    |
-----------------------------------------
Output 135: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 135         |
|    time_elapsed         | 522         |
|    total_timesteps      | 276480      |
| train/                  |             |
|    approx_kl            | 0.012208366 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0021     |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.00407     |
-----------------------------------------
Output 136: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 136          |
|    time_elapsed         | 526          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0036415965 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.305        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0146      |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00785     |
|    value_loss           | 0.00354      |
------------------------------------------
Output 137: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 137         |
|    time_elapsed         | 530         |
|    total_timesteps      | 280576      |
| train/                  |             |
|    approx_kl            | 0.019062625 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0541     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 8.27e-05    |
-----------------------------------------
Output 138: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 529          |
|    iterations           | 138          |
|    time_elapsed         | 534          |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0016352041 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00752      |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.01        |
|    value_loss           | 2.09e-05     |
------------------------------------------
Output 139: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 529        |
|    iterations           | 139        |
|    time_elapsed         | 538        |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.02305676 |
|    clip_fraction        | 0.0548     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0834    |
|    explained_variance   | 0.421      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000275  |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.00379    |
----------------------------------------
Output 140: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 140           |
|    time_elapsed         | 541           |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 0.00052235235 |
|    clip_fraction        | 0.0042        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0709       |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00108      |
|    n_updates            | 1390          |
|    policy_gradient_loss | -0.00177      |
|    value_loss           | 8.42e-06      |
-------------------------------------------
Output 141: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 141          |
|    time_elapsed         | 546          |
|    total_timesteps      | 288768       |
| train/                  |              |
|    approx_kl            | 0.0014757712 |
|    clip_fraction        | 0.00884      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0651      |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0127      |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00485     |
|    value_loss           | 9.28e-06     |
------------------------------------------
Output 142: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 142          |
|    time_elapsed         | 550          |
|    total_timesteps      | 290816       |
| train/                  |              |
|    approx_kl            | 0.0007760164 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0595      |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000211    |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00291     |
|    value_loss           | 1.47e-06     |
------------------------------------------
Output 143: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 143          |
|    time_elapsed         | 554          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0010011215 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0589      |
|    explained_variance   | 0.522        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0209      |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00203     |
|    value_loss           | 0.00196      |
------------------------------------------
Output 144: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 144          |
|    time_elapsed         | 557          |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0010266319 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0598      |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000274    |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.000328    |
|    value_loss           | 3.46e-06     |
------------------------------------------
Output 145: Average over 100 episodes - Reward: 0.28
-----------------------------------------
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 145         |
|    time_elapsed         | 561         |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.020533033 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00723    |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 1.86e-06    |
-----------------------------------------
Output 146: Average over 100 episodes - Reward: 0.69
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 146         |
|    time_elapsed         | 565         |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.044081524 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.13        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0433      |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.105       |
-----------------------------------------
Output 147: Average over 100 episodes - Reward: 0.97
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 147         |
|    time_elapsed         | 568         |
|    total_timesteps      | 301056      |
| train/                  |             |
|    approx_kl            | 0.050612304 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00351     |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.0657      |
-----------------------------------------
Output 148: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 148         |
|    time_elapsed         | 572         |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.013171597 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | -0.883      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 0.00572     |
-----------------------------------------
Output 149: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 149         |
|    time_elapsed         | 575         |
|    total_timesteps      | 305152      |
| train/                  |             |
|    approx_kl            | 0.011850243 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.000127    |
-----------------------------------------
Output 150: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 530        |
|    iterations           | 150        |
|    time_elapsed         | 578        |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.03802294 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0922    |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0235    |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0286    |
|    value_loss           | 0.000102   |
----------------------------------------
Output 151: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 151         |
|    time_elapsed         | 582         |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.021127945 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0713     |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0229     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 1.57e-05    |
-----------------------------------------
Output 152: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 530        |
|    iterations           | 152        |
|    time_elapsed         | 586        |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.06137252 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0913    |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0165    |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0274    |
|    value_loss           | 8.68e-05   |
----------------------------------------
Output 153: Average over 100 episodes - Reward: 0.98
----------------------------------------
| time/                   |            |
|    fps                  | 530        |
|    iterations           | 153        |
|    time_elapsed         | 590        |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.05996625 |
|    clip_fraction        | 0.0317     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0398    |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0214    |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.00163    |
----------------------------------------
Output 154: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 154         |
|    time_elapsed         | 593         |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.009630589 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.029      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 0.00515     |
-----------------------------------------
Output 155: Average over 99 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 155         |
|    time_elapsed         | 597         |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.017681766 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0324     |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00306    |
|    value_loss           | 5.87e-06    |
-----------------------------------------
Output 156: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 156         |
|    time_elapsed         | 601         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.014563652 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.263      |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0462     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0532     |
|    value_loss           | 0.000973    |
-----------------------------------------
Output 157: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 157         |
|    time_elapsed         | 605         |
|    total_timesteps      | 321536      |
| train/                  |             |
|    approx_kl            | 0.006305323 |
|    clip_fraction        | 0.0923      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0305     |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.000407    |
-----------------------------------------
Output 158: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 158          |
|    time_elapsed         | 608          |
|    total_timesteps      | 323584       |
| train/                  |              |
|    approx_kl            | 0.0055059665 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.137       |
|    explained_variance   | 0.4          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0192      |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.0213      |
|    value_loss           | 0.00303      |
------------------------------------------
Output 159: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 159         |
|    time_elapsed         | 612         |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.021674046 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00264     |
-----------------------------------------
Output 160: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 531           |
|    iterations           | 160           |
|    time_elapsed         | 616           |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.00069968007 |
|    clip_fraction        | 0.015         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0824       |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00678      |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.00378      |
|    value_loss           | 2.83e-05      |
-------------------------------------------
Output 161: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 161         |
|    time_elapsed         | 620         |
|    total_timesteps      | 329728      |
| train/                  |             |
|    approx_kl            | 0.010805925 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0663     |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.019      |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00768    |
|    value_loss           | 1.25e-05    |
-----------------------------------------
Output 162: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 162         |
|    time_elapsed         | 624         |
|    total_timesteps      | 331776      |
| train/                  |             |
|    approx_kl            | 0.008632803 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.068      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00358    |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 5.94e-06    |
-----------------------------------------
Output 163: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 163         |
|    time_elapsed         | 628         |
|    total_timesteps      | 333824      |
| train/                  |             |
|    approx_kl            | 0.015478287 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0811     |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.00221     |
-----------------------------------------
Output 164: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 164         |
|    time_elapsed         | 632         |
|    total_timesteps      | 335872      |
| train/                  |             |
|    approx_kl            | 0.048117593 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0158     |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 1.39e-05    |
-----------------------------------------
Output 165: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 165         |
|    time_elapsed         | 636         |
|    total_timesteps      | 337920      |
| train/                  |             |
|    approx_kl            | 0.010727936 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.049      |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.000273    |
-----------------------------------------
Output 166: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 166         |
|    time_elapsed         | 639         |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.009780035 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0587     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.000128    |
-----------------------------------------
Output 167: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 167         |
|    time_elapsed         | 643         |
|    total_timesteps      | 342016      |
| train/                  |             |
|    approx_kl            | 0.019363374 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 5.82e-05    |
-----------------------------------------
Output 168: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 168         |
|    time_elapsed         | 647         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.047899656 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0531     |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0197     |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 3.72e-05    |
-----------------------------------------
Output 169: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 169         |
|    time_elapsed         | 650         |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.002908793 |
|    clip_fraction        | 0.00869     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0347     |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00442    |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 6.55e-06    |
-----------------------------------------
Output 170: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 170          |
|    time_elapsed         | 654          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0026292368 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0223      |
|    explained_variance   | 0.417        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00327     |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.000831    |
|    value_loss           | 0.00385      |
------------------------------------------
Output 171: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 532          |
|    iterations           | 171          |
|    time_elapsed         | 658          |
|    total_timesteps      | 350208       |
| train/                  |              |
|    approx_kl            | 0.0010073154 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0149      |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00373      |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.000985    |
|    value_loss           | 0.00359      |
------------------------------------------
Output 172: Average over 100 episodes - Reward: 0.02
---------------------------------------
| time/                   |           |
|    fps                  | 532       |
|    iterations           | 172       |
|    time_elapsed         | 661       |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.2901681 |
|    clip_fraction        | 0.131     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0757   |
|    explained_variance   | 0.928     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0524   |
|    n_updates            | 1710      |
|    policy_gradient_loss | -0.0247   |
|    value_loss           | 6.01e-06  |
---------------------------------------
Output 173: Average over 100 episodes - Reward: 0.14
-----------------------------------------
| time/                   |             |
|    fps                  | 532         |
|    iterations           | 173         |
|    time_elapsed         | 665         |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.042466976 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | -0.552      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0616     |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.03        |
-----------------------------------------
Output 174: Average over 100 episodes - Reward: 0.4
-----------------------------------------
| time/                   |             |
|    fps                  | 532         |
|    iterations           | 174         |
|    time_elapsed         | 669         |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.030728351 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.269      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00044    |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.0341      |
-----------------------------------------
Output 175: Average over 100 episodes - Reward: 0.58
----------------------------------------
| time/                   |            |
|    fps                  | 533        |
|    iterations           | 175        |
|    time_elapsed         | 672        |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.02894476 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.261     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00303   |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.0605     |
----------------------------------------
Output 176: Average over 100 episodes - Reward: 0.77
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 176         |
|    time_elapsed         | 675         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.008892574 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.242      |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00866     |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.0747      |
-----------------------------------------
Output 177: Average over 100 episodes - Reward: 0.88
----------------------------------------
| time/                   |            |
|    fps                  | 533        |
|    iterations           | 177        |
|    time_elapsed         | 679        |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.00976898 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.206     |
|    explained_variance   | 0.265      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00612   |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.0492     |
----------------------------------------
Output 178: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 178         |
|    time_elapsed         | 683         |
|    total_timesteps      | 364544      |
| train/                  |             |
|    approx_kl            | 0.030583246 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.027      |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0266      |
-----------------------------------------
Output 179: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 532        |
|    iterations           | 179        |
|    time_elapsed         | 687        |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.03395506 |
|    clip_fraction        | 0.0743     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.107      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0357    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 0.000405   |
----------------------------------------
Output 180: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 533        |
|    iterations           | 180        |
|    time_elapsed         | 691        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.05929849 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.24      |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0567    |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0289    |
|    value_loss           | 0.000579   |
----------------------------------------
Output 181: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 533        |
|    iterations           | 181        |
|    time_elapsed         | 695        |
|    total_timesteps      | 370688     |
| train/                  |            |
|    approx_kl            | 0.09613324 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.168     |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0522    |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.000214   |
----------------------------------------
Output 182: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 182         |
|    time_elapsed         | 698         |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.017891478 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.029      |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.00344     |
-----------------------------------------
Output 183: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 183         |
|    time_elapsed         | 701         |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.011788532 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.024      |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0283     |
|    value_loss           | 0.000135    |
-----------------------------------------
Output 184: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 184         |
|    time_elapsed         | 705         |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.008612316 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0106     |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 7.75e-05    |
-----------------------------------------
Output 185: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 185         |
|    time_elapsed         | 709         |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.004112335 |
|    clip_fraction        | 0.0343      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00236    |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00965    |
|    value_loss           | 2.19e-05    |
-----------------------------------------
Output 186: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 186          |
|    time_elapsed         | 712          |
|    total_timesteps      | 380928       |
| train/                  |              |
|    approx_kl            | 0.0022830898 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0736      |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.02        |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00732     |
|    value_loss           | 0.00338      |
------------------------------------------
Output 187: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 187         |
|    time_elapsed         | 716         |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.008066225 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0681     |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0156     |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.000455   |
|    value_loss           | 1.31e-05    |
-----------------------------------------
Output 188: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 188          |
|    time_elapsed         | 720          |
|    total_timesteps      | 385024       |
| train/                  |              |
|    approx_kl            | 0.0084254965 |
|    clip_fraction        | 0.0552       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0681      |
|    explained_variance   | 0.292        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00263     |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00953     |
|    value_loss           | 0.00355      |
------------------------------------------
Output 189: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 189         |
|    time_elapsed         | 724         |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.073140174 |
|    clip_fraction        | 0.0636      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0553     |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0444     |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.00339     |
-----------------------------------------
Output 190: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 190         |
|    time_elapsed         | 727         |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.073582634 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0897     |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 1.42e-05    |
-----------------------------------------
Output 191: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 191         |
|    time_elapsed         | 731         |
|    total_timesteps      | 391168      |
| train/                  |             |
|    approx_kl            | 0.021584101 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0316     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.000242    |
-----------------------------------------
Output 192: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 192         |
|    time_elapsed         | 735         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.016901875 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.000136    |
-----------------------------------------
Output 193: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 193         |
|    time_elapsed         | 738         |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.008358977 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0349     |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 8.67e-05    |
-----------------------------------------
Output 194: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 194         |
|    time_elapsed         | 742         |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.015848577 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0229     |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 2.79e-05    |
-----------------------------------------
Output 195: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 195         |
|    time_elapsed         | 746         |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.004383558 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0115      |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00751    |
|    value_loss           | 8.85e-06    |
-----------------------------------------
Output 196: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 196          |
|    time_elapsed         | 750          |
|    total_timesteps      | 401408       |
| train/                  |              |
|    approx_kl            | 0.0048411153 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.145       |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00945     |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00668     |
|    value_loss           | 7.18e-06     |
------------------------------------------
Output 197: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 197         |
|    time_elapsed         | 753         |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.010263557 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0103      |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00914    |
|    value_loss           | 6e-06       |
-----------------------------------------
Output 198: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 198         |
|    time_elapsed         | 757         |
|    total_timesteps      | 405504      |
| train/                  |             |
|    approx_kl            | 0.013884893 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0192     |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 6.87e-06    |
-----------------------------------------
Output 199: Average over 100 episodes - Reward: 0.95
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 199         |
|    time_elapsed         | 761         |
|    total_timesteps      | 407552      |
| train/                  |             |
|    approx_kl            | 0.038860723 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.166      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 2.81e-08    |
-----------------------------------------
Output 200: Average over 100 episodes - Reward: 0.96
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 200         |
|    time_elapsed         | 765         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.014950564 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.199       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.0106      |
-----------------------------------------
Output 201: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 201          |
|    time_elapsed         | 769          |
|    total_timesteps      | 411648       |
| train/                  |              |
|    approx_kl            | 0.0066884584 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00062      |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.0114       |
------------------------------------------
Output 202: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 202          |
|    time_elapsed         | 773          |
|    total_timesteps      | 413696       |
| train/                  |              |
|    approx_kl            | 0.0139368465 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.814        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0227      |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 9.35e-05     |
------------------------------------------
Output 203: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 534        |
|    iterations           | 203        |
|    time_elapsed         | 777        |
|    total_timesteps      | 415744     |
| train/                  |            |
|    approx_kl            | 0.03576683 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0444    |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0249    |
|    value_loss           | 5.04e-05   |
----------------------------------------
Output 204: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 534        |
|    iterations           | 204        |
|    time_elapsed         | 781        |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.03578677 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.217     |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.012     |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.0352    |
|    value_loss           | 0.000194   |
----------------------------------------
Output 205: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 534        |
|    iterations           | 205        |
|    time_elapsed         | 784        |
|    total_timesteps      | 419840     |
| train/                  |            |
|    approx_kl            | 0.01840649 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.166     |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0543    |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0279    |
|    value_loss           | 0.000102   |
----------------------------------------
Output 206: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 535        |
|    iterations           | 206        |
|    time_elapsed         | 788        |
|    total_timesteps      | 421888     |
| train/                  |            |
|    approx_kl            | 0.05715007 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0422    |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0313    |
|    value_loss           | 3.13e-05   |
----------------------------------------
Output 207: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 207         |
|    time_elapsed         | 791         |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.029173188 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.237      |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0528     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.000107    |
-----------------------------------------
Output 208: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 208         |
|    time_elapsed         | 795         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.024264868 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0359     |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 4.99e-05    |
-----------------------------------------
Output 209: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 209         |
|    time_elapsed         | 798         |
|    total_timesteps      | 428032      |
| train/                  |             |
|    approx_kl            | 0.029636899 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0886     |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.013      |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 1.48e-05    |
-----------------------------------------
Output 210: Average over 100 episodes - Reward: 0.94
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 210         |
|    time_elapsed         | 802         |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.025580196 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0544     |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 1.52e-06    |
-----------------------------------------
Output 211: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 211         |
|    time_elapsed         | 806         |
|    total_timesteps      | 432128      |
| train/                  |             |
|    approx_kl            | 0.021380141 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00586     |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.0194      |
-----------------------------------------
Output 212: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 212         |
|    time_elapsed         | 809         |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 0.028818797 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.00433     |
-----------------------------------------
Output 213: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 213         |
|    time_elapsed         | 812         |
|    total_timesteps      | 436224      |
| train/                  |             |
|    approx_kl            | 0.018327728 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0117     |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.000145    |
-----------------------------------------
Output 214: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 214         |
|    time_elapsed         | 816         |
|    total_timesteps      | 438272      |
| train/                  |             |
|    approx_kl            | 0.013472239 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0247     |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 5.97e-05    |
-----------------------------------------
Output 215: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 536        |
|    iterations           | 215        |
|    time_elapsed         | 820        |
|    total_timesteps      | 440320     |
| train/                  |            |
|    approx_kl            | 0.01153482 |
|    clip_fraction        | 0.0393     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00701   |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 3.3e-05    |
----------------------------------------
Output 216: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 216          |
|    time_elapsed         | 823          |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0028144568 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.13        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00476     |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.0101      |
|    value_loss           | 1.31e-05     |
------------------------------------------
Output 217: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 537         |
|    iterations           | 217         |
|    time_elapsed         | 827         |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.003683574 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0206     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 1.01e-05    |
-----------------------------------------
Output 218: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 218          |
|    time_elapsed         | 831          |
|    total_timesteps      | 446464       |
| train/                  |              |
|    approx_kl            | 0.0022306882 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00146      |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00431     |
|    value_loss           | 5.79e-06     |
------------------------------------------
Output 219: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 537         |
|    iterations           | 219         |
|    time_elapsed         | 835         |
|    total_timesteps      | 448512      |
| train/                  |             |
|    approx_kl            | 0.005030704 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0945     |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00336    |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 4.16e-06    |
-----------------------------------------
Output 220: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 220          |
|    time_elapsed         | 839          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.0029843808 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00348     |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00358     |
|    value_loss           | 1.08e-06     |
------------------------------------------
Output 221: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 221         |
|    time_elapsed         | 842         |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.007443277 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00148    |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 2.18e-06    |
-----------------------------------------
Output 222: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 222         |
|    time_elapsed         | 846         |
|    total_timesteps      | 454656      |
| train/                  |             |
|    approx_kl            | 0.012329534 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00334    |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 1e-05       |
-----------------------------------------
Output 223: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 223          |
|    time_elapsed         | 850          |
|    total_timesteps      | 456704       |
| train/                  |              |
|    approx_kl            | 0.0057616746 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0988      |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00251      |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00819     |
|    value_loss           | 1.73e-06     |
------------------------------------------
Output 224: Average over 100 episodes - Reward: 0.6
------------------------------------------
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 224          |
|    time_elapsed         | 854          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0147109665 |
|    clip_fraction        | 0.155        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0258      |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.013       |
|    value_loss           | 1.03e-06     |
------------------------------------------
Output 225: Average over 100 episodes - Reward: 0.84
----------------------------------------
| time/                   |            |
|    fps                  | 536        |
|    iterations           | 225        |
|    time_elapsed         | 858        |
|    total_timesteps      | 460800     |
| train/                  |            |
|    approx_kl            | 0.04571975 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.221     |
|    explained_variance   | 0.0814     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0359     |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0224    |
|    value_loss           | 0.0896     |
----------------------------------------
Output 226: Average over 100 episodes - Reward: 0.99
---------------------------------------
| time/                   |           |
|    fps                  | 536       |
|    iterations           | 226       |
|    time_elapsed         | 862       |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.0283792 |
|    clip_fraction        | 0.0997    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.184    |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0148    |
|    n_updates            | 2250      |
|    policy_gradient_loss | -0.0207   |
|    value_loss           | 0.0395    |
---------------------------------------
Output 227: Average over 100 episodes - Reward: 0.92
----------------------------------------
| time/                   |            |
|    fps                  | 536        |
|    iterations           | 227        |
|    time_elapsed         | 866        |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.06335106 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.218     |
|    explained_variance   | -0.23      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0266    |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.0223    |
|    value_loss           | 0.00267    |
----------------------------------------
Output 228: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 535        |
|    iterations           | 228        |
|    time_elapsed         | 871        |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.01186456 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.0934     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0054     |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0091    |
|    value_loss           | 0.0289     |
----------------------------------------
Output 229: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 229         |
|    time_elapsed         | 876         |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.021752259 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.329      |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.00386     |
-----------------------------------------
Output 230: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 230         |
|    time_elapsed         | 881         |
|    total_timesteps      | 471040      |
| train/                  |             |
|    approx_kl            | 0.011371849 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0429     |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.00017     |
-----------------------------------------
Output 231: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 231         |
|    time_elapsed         | 885         |
|    total_timesteps      | 473088      |
| train/                  |             |
|    approx_kl            | 0.027837154 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0284     |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.000107    |
-----------------------------------------
Output 232: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 534        |
|    iterations           | 232        |
|    time_elapsed         | 889        |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.01623333 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.222     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0317    |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.0202    |
|    value_loss           | 0.00325    |
----------------------------------------
Output 233: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 534        |
|    iterations           | 233        |
|    time_elapsed         | 892        |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.01855064 |
|    clip_fraction        | 0.075      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.187     |
|    explained_variance   | 0.382      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0541    |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.0029     |
----------------------------------------
Output 234: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 234         |
|    time_elapsed         | 896         |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.008807367 |
|    clip_fraction        | 0.0454      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 2.5e-05     |
-----------------------------------------
Output 235: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 235         |
|    time_elapsed         | 899         |
|    total_timesteps      | 481280      |
| train/                  |             |
|    approx_kl            | 0.039568953 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0176     |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 1.6e-05     |
-----------------------------------------
Output 236: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 236          |
|    time_elapsed         | 903          |
|    total_timesteps      | 483328       |
| train/                  |              |
|    approx_kl            | 0.0034239753 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000602    |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00511     |
|    value_loss           | 2.78e-06     |
------------------------------------------
Output 237: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 237         |
|    time_elapsed         | 907         |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.007769142 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00595     |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 1.41e-06    |
-----------------------------------------
Output 238: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 238         |
|    time_elapsed         | 911         |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.001800803 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 3.47e-06    |
-----------------------------------------
Output 239: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 239         |
|    time_elapsed         | 914         |
|    total_timesteps      | 489472      |
| train/                  |             |
|    approx_kl            | 0.010986578 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00123    |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 7.36e-07    |
-----------------------------------------
Output 240: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 240          |
|    time_elapsed         | 918          |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0047849934 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00307     |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00147     |
|    value_loss           | 3.57e-07     |
------------------------------------------
Output 241: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 241         |
|    time_elapsed         | 922         |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.006692307 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 3.62e-05    |
-----------------------------------------
Output 242: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 242          |
|    time_elapsed         | 926          |
|    total_timesteps      | 495616       |
| train/                  |              |
|    approx_kl            | 0.0134951165 |
|    clip_fraction        | 0.0823       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.142       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.021       |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 7.53e-06     |
------------------------------------------
Output 243: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 243         |
|    time_elapsed         | 929         |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.007411478 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0107     |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 1.19e-06    |
-----------------------------------------
Output 244: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 535        |
|    iterations           | 244        |
|    time_elapsed         | 933        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.02498835 |
|    clip_fraction        | 0.0579     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00539   |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.00473   |
|    value_loss           | 4.07e-09   |
----------------------------------------
Output 245: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 535        |
|    iterations           | 245        |
|    time_elapsed         | 937        |
|    total_timesteps      | 501760     |
| train/                  |            |
|    approx_kl            | 0.04565585 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.206     |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0318    |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0354    |
|    value_loss           | 0.000147   |
----------------------------------------
Overall: Average Reward: 0.7437646340221928
Using cpu device
Logging to ./PPOtensorboard/PPO_4
Output 1: Average over 68 episodes - Reward: 0.0
-----------------------------
| time/              |      |
|    fps             | 1022 |
|    iterations      | 1    |
|    time_elapsed    | 2    |
|    total_timesteps | 2048 |
-----------------------------
Output 2: Average over 66 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 2           |
|    time_elapsed         | 5           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008898015 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -8.33       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0442     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0088     |
|    value_loss           | 0.00526     |
-----------------------------------------
Output 3: Average over 74 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 652        |
|    iterations           | 3          |
|    time_elapsed         | 9          |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.01007098 |
|    clip_fraction        | 0.0718     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -1.26      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0438    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.000433   |
----------------------------------------
Output 4: Average over 86 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 636         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010500294 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | -1.88       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0327     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.000111    |
-----------------------------------------
Output 5: Average over 84 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 615         |
|    iterations           | 5           |
|    time_elapsed         | 16          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.013072573 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | -6.91       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0484     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 4.47e-05    |
-----------------------------------------
Output 6: Average over 96 episodes - Reward: 0.010416666666666666
-----------------------------------------
| time/                   |             |
|    fps                  | 607         |
|    iterations           | 6           |
|    time_elapsed         | 20          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.020190157 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | -1.4        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.026      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 1.67e-06    |
-----------------------------------------
Output 7: Average over 100 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 604          |
|    iterations           | 7            |
|    time_elapsed         | 23           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0067512495 |
|    clip_fraction        | 0.0523       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | -0.0313      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00734     |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.00272      |
------------------------------------------
Output 8: Average over 100 episodes - Reward: 0.02
----------------------------------------
| time/                   |            |
|    fps                  | 600        |
|    iterations           | 8          |
|    time_elapsed         | 27         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.00571044 |
|    clip_fraction        | 0.0241     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | -1.87      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0301    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.000288   |
----------------------------------------
Output 9: Average over 100 episodes - Reward: 0.01
------------------------------------------
| time/                   |              |
|    fps                  | 591          |
|    iterations           | 9            |
|    time_elapsed         | 31           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0051703984 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0159       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00993     |
|    value_loss           | 0.0062       |
------------------------------------------
Output 10: Average over 98 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 580         |
|    iterations           | 10          |
|    time_elapsed         | 35          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.011358933 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.00379     |
-----------------------------------------
Output 11: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 569         |
|    iterations           | 11          |
|    time_elapsed         | 39          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.014981709 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -6.39       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0462     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.000556    |
-----------------------------------------
Output 12: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 564         |
|    iterations           | 12          |
|    time_elapsed         | 43          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.005882006 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.181       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00224    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.00569     |
-----------------------------------------
Output 13: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 563         |
|    iterations           | 13          |
|    time_elapsed         | 47          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.006418375 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | -3.26       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.000442    |
-----------------------------------------
Output 14: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 563         |
|    iterations           | 14          |
|    time_elapsed         | 50          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.016858118 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.955      |
|    explained_variance   | -0.528      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0203     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 6.31e-05    |
-----------------------------------------
Output 15: Average over 100 episodes - Reward: 0.01
-----------------------------------------
| time/                   |             |
|    fps                  | 565         |
|    iterations           | 15          |
|    time_elapsed         | 54          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.009897208 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.0102      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00614    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.00791     |
-----------------------------------------
Output 16: Average over 88 episodes - Reward: 0.0
------------------------------------------
| time/                   |              |
|    fps                  | 563          |
|    iterations           | 16           |
|    time_elapsed         | 58           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0066365954 |
|    clip_fraction        | 0.0582       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.818       |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00127      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 0.00758      |
------------------------------------------
Output 17: Average over 93 episodes - Reward: 0.021505376344086023
-----------------------------------------
| time/                   |             |
|    fps                  | 563         |
|    iterations           | 17          |
|    time_elapsed         | 61          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.008268384 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.787      |
|    explained_variance   | -3.14       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0218     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.000279    |
-----------------------------------------
Output 18: Average over 89 episodes - Reward: 0.011235955056179775
------------------------------------------
| time/                   |              |
|    fps                  | 561          |
|    iterations           | 18           |
|    time_elapsed         | 65           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0066724997 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.723       |
|    explained_variance   | 0.0564       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0296       |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00659     |
|    value_loss           | 0.00763      |
------------------------------------------
Output 19: Average over 86 episodes - Reward: 0.046511627906976744
-----------------------------------------
| time/                   |             |
|    fps                  | 561         |
|    iterations           | 19          |
|    time_elapsed         | 69          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.006780224 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.0631      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 0.0043      |
-----------------------------------------
Output 20: Average over 60 episodes - Reward: 0.1
------------------------------------------
| time/                   |              |
|    fps                  | 560          |
|    iterations           | 20           |
|    time_elapsed         | 73           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0041488344 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.643       |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00686      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00735     |
|    value_loss           | 0.0134       |
------------------------------------------
Output 21: Average over 82 episodes - Reward: 0.07317073170731707
------------------------------------------
| time/                   |              |
|    fps                  | 561          |
|    iterations           | 21           |
|    time_elapsed         | 76           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0056623872 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.639       |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0321       |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00597     |
|    value_loss           | 0.0198       |
------------------------------------------
Output 22: Average over 88 episodes - Reward: 0.03409090909090909
------------------------------------------
| time/                   |              |
|    fps                  | 560          |
|    iterations           | 22           |
|    time_elapsed         | 80           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0058072843 |
|    clip_fraction        | 0.0646       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0151      |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00906     |
|    value_loss           | 0.0178       |
------------------------------------------
Output 23: Average over 64 episodes - Reward: 0.015625
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 23           |
|    time_elapsed         | 85           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0065208976 |
|    clip_fraction        | 0.07         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.563       |
|    explained_variance   | 0.0571       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00681     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.0089      |
|    value_loss           | 0.0138       |
------------------------------------------
Output 24: Average over 77 episodes - Reward: 0.05194805194805195
----------------------------------------
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 24         |
|    time_elapsed         | 89         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.00817571 |
|    clip_fraction        | 0.0717     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.534     |
|    explained_variance   | 0.00387    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0309    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.00947   |
|    value_loss           | 0.00519    |
----------------------------------------
Output 25: Average over 69 episodes - Reward: 0.07246376811594203
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 25          |
|    time_elapsed         | 93          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.004805957 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.487      |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0106     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 0.0127      |
-----------------------------------------
Output 26: Average over 69 episodes - Reward: 0.07246376811594203
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 26           |
|    time_elapsed         | 97           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0055887895 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.493       |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00687      |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00763     |
|    value_loss           | 0.0159       |
------------------------------------------
Output 27: Average over 82 episodes - Reward: 0.012195121951219513
-----------------------------------------
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 27          |
|    time_elapsed         | 101         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.005248784 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.107       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00142     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.0175      |
-----------------------------------------
Output 28: Average over 85 episodes - Reward: 0.023529411764705882
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 28          |
|    time_elapsed         | 105         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.002892485 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.00572     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0206     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 0.00513     |
-----------------------------------------
Output 29: Average over 63 episodes - Reward: 0.07936507936507936
-----------------------------------------
| time/                   |             |
|    fps                  | 542         |
|    iterations           | 29          |
|    time_elapsed         | 109         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.004149422 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.193       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.02        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00801    |
|    value_loss           | 0.00625     |
-----------------------------------------
Output 30: Average over 66 episodes - Reward: 0.07575757575757576
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 30          |
|    time_elapsed         | 113         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.004954491 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.446      |
|    explained_variance   | 0.186       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00364    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 0.0161      |
-----------------------------------------
Output 31: Average over 69 episodes - Reward: 0.043478260869565216
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 31           |
|    time_elapsed         | 117          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0034413687 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.422       |
|    explained_variance   | 0.151        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000293     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0054      |
|    value_loss           | 0.0178       |
------------------------------------------
Output 32: Average over 80 episodes - Reward: 0.05
------------------------------------------
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 32           |
|    time_elapsed         | 120          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0049412446 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.372       |
|    explained_variance   | 0.131        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00282      |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00651     |
|    value_loss           | 0.0104       |
------------------------------------------
Output 33: Average over 59 episodes - Reward: 0.0847457627118644
-----------------------------------------
| time/                   |             |
|    fps                  | 542         |
|    iterations           | 33          |
|    time_elapsed         | 124         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.007080162 |
|    clip_fraction        | 0.0665      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.199       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000734   |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 0.0132      |
-----------------------------------------
Output 34: Average over 68 episodes - Reward: 0.16176470588235295
-----------------------------------------
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 34          |
|    time_elapsed         | 128         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.003843524 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.018       |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 0.017       |
-----------------------------------------
Output 35: Average over 81 episodes - Reward: 0.024691358024691357
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 35           |
|    time_elapsed         | 131          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0038814684 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00167     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00566     |
|    value_loss           | 0.0299       |
------------------------------------------
Output 36: Average over 52 episodes - Reward: 0.057692307692307696
------------------------------------------
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 36           |
|    time_elapsed         | 135          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0030271302 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | -0.199       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0252      |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00738     |
|    value_loss           | 0.0103       |
------------------------------------------
Output 37: Average over 65 episodes - Reward: 0.1076923076923077
------------------------------------------
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 37           |
|    time_elapsed         | 138          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0028881398 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.259       |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00278     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00374     |
|    value_loss           | 0.0109       |
------------------------------------------
Output 38: Average over 63 episodes - Reward: 0.06349206349206349
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 38           |
|    time_elapsed         | 142          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0024312136 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.247       |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0112       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 0.0208       |
------------------------------------------
Output 39: Average over 60 episodes - Reward: 0.18333333333333332
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 39          |
|    time_elapsed         | 146         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.005036618 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | 0.188       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00582    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 0.0138      |
-----------------------------------------
Output 40: Average over 71 episodes - Reward: 0.028169014084507043
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 40           |
|    time_elapsed         | 149          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0022008894 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0133       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.0277       |
------------------------------------------
Output 41: Average over 63 episodes - Reward: 0.07936507936507936
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 41           |
|    time_elapsed         | 153          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0033623744 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.246       |
|    explained_variance   | 0.073        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0096       |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 0.00963      |
------------------------------------------
Output 42: Average over 67 episodes - Reward: 0.08955223880597014
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 42           |
|    time_elapsed         | 157          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0030035775 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0075       |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00346     |
|    value_loss           | 0.0166       |
------------------------------------------
Output 43: Average over 59 episodes - Reward: 0.1016949152542373
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 43           |
|    time_elapsed         | 161          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0023191408 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.216       |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0112       |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 0.0199       |
------------------------------------------
Output 44: Average over 60 episodes - Reward: 0.2
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 44          |
|    time_elapsed         | 164         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.008484285 |
|    clip_fraction        | 0.0548      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00216    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 0.0186      |
-----------------------------------------
Output 45: Average over 48 episodes - Reward: 0.125
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 45           |
|    time_elapsed         | 168          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0020309486 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.191        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00729      |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 0.0331       |
------------------------------------------
Output 46: Average over 75 episodes - Reward: 0.14666666666666667
----------------------------------------
| time/                   |            |
|    fps                  | 546        |
|    iterations           | 46         |
|    time_elapsed         | 172        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.00580104 |
|    clip_fraction        | 0.0272     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.191     |
|    explained_variance   | 0.0496     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0204    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.00561   |
|    value_loss           | 0.0205     |
----------------------------------------
Output 47: Average over 70 episodes - Reward: 0.14285714285714285
------------------------------------------
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 47           |
|    time_elapsed         | 176          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0030604284 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0.245        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0238       |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00228     |
|    value_loss           | 0.0297       |
------------------------------------------
Output 48: Average over 76 episodes - Reward: 0.06578947368421052
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 48           |
|    time_elapsed         | 180          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0035268534 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.19        |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0175       |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00694     |
|    value_loss           | 0.0276       |
------------------------------------------
Output 49: Average over 60 episodes - Reward: 0.08333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 49           |
|    time_elapsed         | 183          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0008912211 |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.183       |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.0003       |
|    loss                 | 3.18e-05     |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00168     |
|    value_loss           | 0.0181       |
------------------------------------------
Output 50: Average over 61 episodes - Reward: 0.16393442622950818
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 50           |
|    time_elapsed         | 186          |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0019989272 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.157       |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0104       |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 0.0176       |
------------------------------------------
Output 51: Average over 80 episodes - Reward: 0.1
------------------------------------------
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 51           |
|    time_elapsed         | 190          |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.0015536316 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.019        |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.0033      |
|    value_loss           | 0.0273       |
------------------------------------------
Output 52: Average over 60 episodes - Reward: 0.15
-----------------------------------------
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 52          |
|    time_elapsed         | 193         |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.001238175 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00956     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 0.0244      |
-----------------------------------------
Output 53: Average over 60 episodes - Reward: 0.05
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 53           |
|    time_elapsed         | 197          |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 0.0016387783 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0176       |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 0.0253       |
------------------------------------------
Output 54: Average over 54 episodes - Reward: 0.18518518518518517
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 54           |
|    time_elapsed         | 200          |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 0.0031194673 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00262      |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00411     |
|    value_loss           | 0.0127       |
------------------------------------------
Output 55: Average over 69 episodes - Reward: 0.13043478260869565
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 55           |
|    time_elapsed         | 204          |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.0011706457 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.13        |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00288     |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00221     |
|    value_loss           | 0.0233       |
------------------------------------------
Output 56: Average over 79 episodes - Reward: 0.0759493670886076
-----------------------------------------
| time/                   |             |
|    fps                  | 551         |
|    iterations           | 56          |
|    time_elapsed         | 207         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.002492757 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.229       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0026     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 0.0273      |
-----------------------------------------
Output 57: Average over 67 episodes - Reward: 0.08955223880597014
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 57           |
|    time_elapsed         | 211          |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.0021176485 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.428        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00381     |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00434     |
|    value_loss           | 0.0177       |
------------------------------------------
Output 58: Average over 55 episodes - Reward: 0.23636363636363636
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 58           |
|    time_elapsed         | 215          |
|    total_timesteps      | 118784       |
| train/                  |              |
|    approx_kl            | 0.0013064235 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0957      |
|    explained_variance   | 0.409        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000309    |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 0.0154       |
------------------------------------------
Output 59: Average over 71 episodes - Reward: 0.14084507042253522
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 59           |
|    time_elapsed         | 218          |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0010763863 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0957      |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0145       |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 0.0301       |
------------------------------------------
Output 60: Average over 61 episodes - Reward: 0.2459016393442623
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 60          |
|    time_elapsed         | 221         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.003223936 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0966     |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0149      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 0.0236      |
-----------------------------------------
Output 61: Average over 67 episodes - Reward: 0.14925373134328357
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 61           |
|    time_elapsed         | 225          |
|    total_timesteps      | 124928       |
| train/                  |              |
|    approx_kl            | 0.0016921919 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.488        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00262      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00233     |
|    value_loss           | 0.0274       |
------------------------------------------
Output 62: Average over 61 episodes - Reward: 0.18032786885245902
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 62          |
|    time_elapsed         | 228         |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.004435104 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0898     |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00492    |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 0.022       |
-----------------------------------------
Output 63: Average over 53 episodes - Reward: 0.3584905660377358
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 63           |
|    time_elapsed         | 232          |
|    total_timesteps      | 129024       |
| train/                  |              |
|    approx_kl            | 0.0054028886 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0883      |
|    explained_variance   | 0.506        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0165       |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00397     |
|    value_loss           | 0.022        |
------------------------------------------
Output 64: Average over 57 episodes - Reward: 0.12280701754385964
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 64          |
|    time_elapsed         | 236         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.014286152 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00168    |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 0.0215      |
-----------------------------------------
Output 65: Average over 63 episodes - Reward: 0.23809523809523808
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 65          |
|    time_elapsed         | 239         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.002128406 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00647     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 0.0188      |
-----------------------------------------
Output 66: Average over 66 episodes - Reward: 0.2727272727272727
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 66          |
|    time_elapsed         | 243         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.003290498 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000896    |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 0.0182      |
-----------------------------------------
Output 67: Average over 58 episodes - Reward: 0.29310344827586204
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 67           |
|    time_elapsed         | 247          |
|    total_timesteps      | 137216       |
| train/                  |              |
|    approx_kl            | 0.0025280996 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.556        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0212       |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00313     |
|    value_loss           | 0.0255       |
------------------------------------------
Output 68: Average over 56 episodes - Reward: 0.30357142857142855
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 68          |
|    time_elapsed         | 251         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.001942806 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 0.0221      |
-----------------------------------------
Output 69: Average over 62 episodes - Reward: 0.24193548387096775
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 69           |
|    time_elapsed         | 256          |
|    total_timesteps      | 141312       |
| train/                  |              |
|    approx_kl            | 0.0020002418 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.653        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00559      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00289     |
|    value_loss           | 0.0191       |
------------------------------------------
Output 70: Average over 62 episodes - Reward: 0.20967741935483872
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 70           |
|    time_elapsed         | 260          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0018734356 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.741        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00521      |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.0168       |
------------------------------------------
Output 71: Average over 58 episodes - Reward: 0.20689655172413793
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 71           |
|    time_elapsed         | 264          |
|    total_timesteps      | 145408       |
| train/                  |              |
|    approx_kl            | 0.0018180183 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.661        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00174      |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 0.0213       |
------------------------------------------
Output 72: Average over 49 episodes - Reward: 0.14285714285714285
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 72          |
|    time_elapsed         | 267         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.002845965 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0186      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 0.0202      |
-----------------------------------------
Output 73: Average over 59 episodes - Reward: 0.3220338983050847
-----------------------------------------
| time/                   |             |
|    fps                  | 551         |
|    iterations           | 73          |
|    time_elapsed         | 271         |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.004315342 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00622    |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 0.0161      |
-----------------------------------------
Output 74: Average over 59 episodes - Reward: 0.288135593220339
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 74           |
|    time_elapsed         | 274          |
|    total_timesteps      | 151552       |
| train/                  |              |
|    approx_kl            | 0.0019752593 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.139       |
|    explained_variance   | 0.62         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0036       |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.021        |
------------------------------------------
Output 75: Average over 45 episodes - Reward: 0.26666666666666666
-----------------------------------------
| time/                   |             |
|    fps                  | 551         |
|    iterations           | 75          |
|    time_elapsed         | 278         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.002249803 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0125      |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 0.019       |
-----------------------------------------
Output 76: Average over 55 episodes - Reward: 0.2
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 76           |
|    time_elapsed         | 281          |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 0.0011695339 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.567        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00709      |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 0.0191       |
------------------------------------------
Output 77: Average over 58 episodes - Reward: 0.1896551724137931
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 77           |
|    time_elapsed         | 285          |
|    total_timesteps      | 157696       |
| train/                  |              |
|    approx_kl            | 0.0043832986 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.66         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00397      |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 0.0178       |
------------------------------------------
Output 78: Average over 58 episodes - Reward: 0.29310344827586204
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 78           |
|    time_elapsed         | 289          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0019128697 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.655        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00295      |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 0.0185       |
------------------------------------------
Output 79: Average over 52 episodes - Reward: 0.36538461538461536
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 79           |
|    time_elapsed         | 292          |
|    total_timesteps      | 161792       |
| train/                  |              |
|    approx_kl            | 0.0011655592 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.113       |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0108       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 0.0193       |
------------------------------------------
Output 80: Average over 56 episodes - Reward: 0.25
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 80           |
|    time_elapsed         | 296          |
|    total_timesteps      | 163840       |
| train/                  |              |
|    approx_kl            | 0.0022890908 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.111       |
|    explained_variance   | 0.652        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00756      |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 0.0207       |
------------------------------------------
Output 81: Average over 53 episodes - Reward: 0.3584905660377358
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 81           |
|    time_elapsed         | 300          |
|    total_timesteps      | 165888       |
| train/                  |              |
|    approx_kl            | 0.0017410492 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.631        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00988     |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.0205       |
------------------------------------------
Output 82: Average over 45 episodes - Reward: 0.3333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 82           |
|    time_elapsed         | 304          |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 0.0016870095 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.108       |
|    explained_variance   | 0.635        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.013        |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 0.0201       |
------------------------------------------
Output 83: Average over 54 episodes - Reward: 0.35185185185185186
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 83           |
|    time_elapsed         | 307          |
|    total_timesteps      | 169984       |
| train/                  |              |
|    approx_kl            | 0.0027516915 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0956      |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0262       |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00425     |
|    value_loss           | 0.0189       |
------------------------------------------
Output 84: Average over 53 episodes - Reward: 0.3018867924528302
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 84           |
|    time_elapsed         | 310          |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0023104139 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.59         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00459     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 0.0232       |
------------------------------------------
Output 85: Average over 60 episodes - Reward: 0.21666666666666667
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 85           |
|    time_elapsed         | 314          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0024687015 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.681        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00859      |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 0.018        |
------------------------------------------
Output 86: Average over 60 episodes - Reward: 0.2
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 86          |
|    time_elapsed         | 318         |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.002843871 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00791     |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 0.0233      |
-----------------------------------------
Output 87: Average over 55 episodes - Reward: 0.32727272727272727
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 87          |
|    time_elapsed         | 322         |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.001858244 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00203    |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 0.0197      |
-----------------------------------------
Output 88: Average over 47 episodes - Reward: 0.3617021276595745
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 88           |
|    time_elapsed         | 327          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0018862087 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0978      |
|    explained_variance   | 0.573        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0151       |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00269     |
|    value_loss           | 0.0238       |
------------------------------------------
Output 89: Average over 68 episodes - Reward: 0.22058823529411764
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 89           |
|    time_elapsed         | 333          |
|    total_timesteps      | 182272       |
| train/                  |              |
|    approx_kl            | 0.0015236303 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0826      |
|    explained_variance   | 0.603        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00183      |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 0.0191       |
------------------------------------------
Output 90: Average over 59 episodes - Reward: 0.1864406779661017
------------------------------------------
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 90           |
|    time_elapsed         | 337          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0026391528 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.668        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0116       |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00285     |
|    value_loss           | 0.0207       |
------------------------------------------
Output 91: Average over 61 episodes - Reward: 0.2786885245901639
-----------------------------------------
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 91          |
|    time_elapsed         | 342         |
|    total_timesteps      | 186368      |
| train/                  |             |
|    approx_kl            | 0.004267507 |
|    clip_fraction        | 0.023       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00365     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 0.0191      |
-----------------------------------------
Output 92: Average over 51 episodes - Reward: 0.2549019607843137
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 92           |
|    time_elapsed         | 348          |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0027175848 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.64         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00586      |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00412     |
|    value_loss           | 0.0229       |
------------------------------------------
Output 93: Average over 56 episodes - Reward: 0.23214285714285715
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 93           |
|    time_elapsed         | 352          |
|    total_timesteps      | 190464       |
| train/                  |              |
|    approx_kl            | 0.0014199016 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.665        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0102       |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000644    |
|    value_loss           | 0.018        |
------------------------------------------
Output 94: Average over 57 episodes - Reward: 0.15789473684210525
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 94           |
|    time_elapsed         | 356          |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 0.0021206269 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.702        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.026        |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 0.0185       |
------------------------------------------
Output 95: Average over 74 episodes - Reward: 0.16216216216216217
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 95           |
|    time_elapsed         | 359          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.0017088895 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0.705        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00451      |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 0.0173       |
------------------------------------------
Output 96: Average over 49 episodes - Reward: 0.2857142857142857
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 96           |
|    time_elapsed         | 363          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0025343578 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0.689        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0105       |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 0.0194       |
------------------------------------------
Output 97: Average over 43 episodes - Reward: 0.32558139534883723
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 97           |
|    time_elapsed         | 367          |
|    total_timesteps      | 198656       |
| train/                  |              |
|    approx_kl            | 0.0008351053 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0902      |
|    explained_variance   | 0.661        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00928      |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.000864    |
|    value_loss           | 0.0171       |
------------------------------------------
Output 98: Average over 57 episodes - Reward: 0.2807017543859649
-------------------------------------------
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 98            |
|    time_elapsed         | 370           |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 0.00085784413 |
|    clip_fraction        | 0.01          |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0706       |
|    explained_variance   | 0.565         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00974       |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.00127      |
|    value_loss           | 0.017         |
-------------------------------------------
Output 99: Average over 56 episodes - Reward: 0.32142857142857145
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 99           |
|    time_elapsed         | 374          |
|    total_timesteps      | 202752       |
| train/                  |              |
|    approx_kl            | 0.0015142257 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0988      |
|    explained_variance   | 0.622        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0065      |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00364     |
|    value_loss           | 0.0202       |
------------------------------------------
Output 100: Average over 43 episodes - Reward: 0.4186046511627907
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 100         |
|    time_elapsed         | 378         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.004625505 |
|    clip_fraction        | 0.0225      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00712     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 0.0191      |
-----------------------------------------
Output 101: Average over 64 episodes - Reward: 0.25
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 101          |
|    time_elapsed         | 382          |
|    total_timesteps      | 206848       |
| train/                  |              |
|    approx_kl            | 0.0014451555 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0739      |
|    explained_variance   | 0.58         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000815    |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 0.0192       |
------------------------------------------
Output 102: Average over 50 episodes - Reward: 0.34
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 102          |
|    time_elapsed         | 386          |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 0.0020350073 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.655        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00975      |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 0.0224       |
------------------------------------------
Output 103: Average over 70 episodes - Reward: 0.32857142857142857
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 103          |
|    time_elapsed         | 389          |
|    total_timesteps      | 210944       |
| train/                  |              |
|    approx_kl            | 0.0018742643 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0864      |
|    explained_variance   | 0.619        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0153       |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00276     |
|    value_loss           | 0.0181       |
------------------------------------------
Output 104: Average over 49 episodes - Reward: 0.2653061224489796
----------------------------------------
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 104        |
|    time_elapsed         | 393        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.00232518 |
|    clip_fraction        | 0.0124     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0167     |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.00241   |
|    value_loss           | 0.0257     |
----------------------------------------
Output 105: Average over 43 episodes - Reward: 0.2558139534883721
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 105          |
|    time_elapsed         | 397          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.0017191644 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00332      |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00366     |
|    value_loss           | 0.0182       |
------------------------------------------
Output 106: Average over 51 episodes - Reward: 0.27450980392156865
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 106          |
|    time_elapsed         | 400          |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 0.0023526035 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00125     |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 0.0147       |
------------------------------------------
Output 107: Average over 49 episodes - Reward: 0.1836734693877551
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 107          |
|    time_elapsed         | 405          |
|    total_timesteps      | 219136       |
| train/                  |              |
|    approx_kl            | 0.0029355483 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.622        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00408     |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00519     |
|    value_loss           | 0.0195       |
------------------------------------------
Output 108: Average over 55 episodes - Reward: 0.3090909090909091
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 108          |
|    time_elapsed         | 409          |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 0.0015810061 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0.68         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00269     |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.0034      |
|    value_loss           | 0.0156       |
------------------------------------------
Output 109: Average over 50 episodes - Reward: 0.34
-----------------------------------------
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 109         |
|    time_elapsed         | 413         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.003690204 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00897     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 0.0207      |
-----------------------------------------
Output 110: Average over 45 episodes - Reward: 0.26666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 110          |
|    time_elapsed         | 418          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.0040276446 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0.678        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00286      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 0.0186       |
------------------------------------------
Output 111: Average over 44 episodes - Reward: 0.29545454545454547
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 111          |
|    time_elapsed         | 421          |
|    total_timesteps      | 227328       |
| train/                  |              |
|    approx_kl            | 0.0023415494 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.573        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00703      |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.0196       |
------------------------------------------
Output 112: Average over 48 episodes - Reward: 0.2708333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 112          |
|    time_elapsed         | 425          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0029417502 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.106       |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00312      |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 0.0177       |
------------------------------------------
Output 113: Average over 47 episodes - Reward: 0.19148936170212766
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 113          |
|    time_elapsed         | 429          |
|    total_timesteps      | 231424       |
| train/                  |              |
|    approx_kl            | 0.0015825466 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 0.612        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0115       |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 0.0176       |
------------------------------------------
Output 114: Average over 37 episodes - Reward: 0.32432432432432434
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 114          |
|    time_elapsed         | 432          |
|    total_timesteps      | 233472       |
| train/                  |              |
|    approx_kl            | 0.0026420373 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0.601        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0067       |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00483     |
|    value_loss           | 0.0172       |
------------------------------------------
Output 115: Average over 36 episodes - Reward: 0.3888888888888889
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 115          |
|    time_elapsed         | 436          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0065466296 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.591        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00337     |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00508     |
|    value_loss           | 0.0163       |
------------------------------------------
Output 116: Average over 40 episodes - Reward: 0.375
-----------------------------------------
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 116         |
|    time_elapsed         | 440         |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.002711289 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00596    |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 0.0169      |
-----------------------------------------
Output 117: Average over 37 episodes - Reward: 0.32432432432432434
-----------------------------------------
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 117         |
|    time_elapsed         | 444         |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.008554438 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00673     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 0.0188      |
-----------------------------------------
Output 118: Average over 47 episodes - Reward: 0.1702127659574468
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 118          |
|    time_elapsed         | 447          |
|    total_timesteps      | 241664       |
| train/                  |              |
|    approx_kl            | 0.0031907246 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.59         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0097       |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 0.0172       |
------------------------------------------
Output 119: Average over 39 episodes - Reward: 0.358974358974359
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 119          |
|    time_elapsed         | 450          |
|    total_timesteps      | 243712       |
| train/                  |              |
|    approx_kl            | 0.0037607835 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0.601        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00677      |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00227     |
|    value_loss           | 0.0168       |
------------------------------------------
Output 120: Average over 41 episodes - Reward: 0.4146341463414634
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 120          |
|    time_elapsed         | 454          |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0016379543 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | 0.629        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00759      |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00205     |
|    value_loss           | 0.0178       |
------------------------------------------
Output 121: Average over 42 episodes - Reward: 0.3333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 121          |
|    time_elapsed         | 458          |
|    total_timesteps      | 247808       |
| train/                  |              |
|    approx_kl            | 0.0044109197 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.674        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0107      |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00396     |
|    value_loss           | 0.0164       |
------------------------------------------
Output 122: Average over 48 episodes - Reward: 0.2708333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 122          |
|    time_elapsed         | 461          |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 0.0018889781 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.604        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0163       |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.00255     |
|    value_loss           | 0.0188       |
------------------------------------------
Output 123: Average over 44 episodes - Reward: 0.3181818181818182
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 123          |
|    time_elapsed         | 465          |
|    total_timesteps      | 251904       |
| train/                  |              |
|    approx_kl            | 0.0044003157 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.673        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00681      |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00557     |
|    value_loss           | 0.0164       |
------------------------------------------
Output 124: Average over 48 episodes - Reward: 0.375
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 124          |
|    time_elapsed         | 469          |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.0017781503 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.11        |
|    explained_variance   | 0.567        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00731      |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00344     |
|    value_loss           | 0.018        |
------------------------------------------
Output 125: Average over 48 episodes - Reward: 0.2916666666666667
----------------------------------------
| time/                   |            |
|    fps                  | 540        |
|    iterations           | 125        |
|    time_elapsed         | 473        |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.00411377 |
|    clip_fraction        | 0.0237     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0133     |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.00341   |
|    value_loss           | 0.0215     |
----------------------------------------
Output 126: Average over 35 episodes - Reward: 0.3142857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 126          |
|    time_elapsed         | 476          |
|    total_timesteps      | 258048       |
| train/                  |              |
|    approx_kl            | 0.0070301453 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000618    |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 0.019        |
------------------------------------------
Output 127: Average over 37 episodes - Reward: 0.2972972972972973
-----------------------------------------
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 127         |
|    time_elapsed         | 480         |
|    total_timesteps      | 260096      |
| train/                  |             |
|    approx_kl            | 0.005782257 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0179      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 0.0156      |
-----------------------------------------
Output 128: Average over 43 episodes - Reward: 0.27906976744186046
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 128          |
|    time_elapsed         | 484          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0015235269 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.429        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00104      |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 0.0211       |
------------------------------------------
Output 129: Average over 46 episodes - Reward: 0.10869565217391304
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 129          |
|    time_elapsed         | 488          |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 0.0030658597 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.144       |
|    explained_variance   | 0.503        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.016        |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.0219       |
------------------------------------------
Output 130: Average over 43 episodes - Reward: 0.23255813953488372
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 130         |
|    time_elapsed         | 492         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.014360162 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00338    |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.021       |
-----------------------------------------
Output 131: Average over 41 episodes - Reward: 0.24390243902439024
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 131          |
|    time_elapsed         | 495          |
|    total_timesteps      | 268288       |
| train/                  |              |
|    approx_kl            | 0.0030222288 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.159       |
|    explained_variance   | 0.391        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0231       |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00383     |
|    value_loss           | 0.0216       |
------------------------------------------
Output 132: Average over 45 episodes - Reward: 0.35555555555555557
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 132          |
|    time_elapsed         | 499          |
|    total_timesteps      | 270336       |
| train/                  |              |
|    approx_kl            | 0.0021010707 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0.407        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00412      |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 0.0217       |
------------------------------------------
Output 133: Average over 46 episodes - Reward: 0.2608695652173913
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 133         |
|    time_elapsed         | 503         |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.002948286 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0124      |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 0.0285      |
-----------------------------------------
Output 134: Average over 40 episodes - Reward: 0.45
-----------------------------------------
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 134         |
|    time_elapsed         | 506         |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.002889113 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0158      |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 0.0215      |
-----------------------------------------
Output 135: Average over 41 episodes - Reward: 0.3902439024390244
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 135          |
|    time_elapsed         | 510          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.0025985437 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.474        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00569      |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00255     |
|    value_loss           | 0.0239       |
------------------------------------------
Output 136: Average over 47 episodes - Reward: 0.2553191489361702
------------------------------------------
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 136          |
|    time_elapsed         | 513          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0019814665 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.475        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0174       |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 0.0228       |
------------------------------------------
Output 137: Average over 47 episodes - Reward: 0.19148936170212766
------------------------------------------
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 137          |
|    time_elapsed         | 517          |
|    total_timesteps      | 280576       |
| train/                  |              |
|    approx_kl            | 0.0021739244 |
|    clip_fraction        | 0.0272       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.448        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00544      |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 0.0248       |
------------------------------------------
Output 138: Average over 41 episodes - Reward: 0.1951219512195122
------------------------------------------
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 138          |
|    time_elapsed         | 520          |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0058758734 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.174       |
|    explained_variance   | 0.562        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0127      |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00436     |
|    value_loss           | 0.0189       |
------------------------------------------
Output 139: Average over 46 episodes - Reward: 0.2391304347826087
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 139          |
|    time_elapsed         | 524          |
|    total_timesteps      | 284672       |
| train/                  |              |
|    approx_kl            | 0.0031446596 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.445        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0146       |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00555     |
|    value_loss           | 0.019        |
------------------------------------------
Output 140: Average over 39 episodes - Reward: 0.38461538461538464
-----------------------------------------
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 140         |
|    time_elapsed         | 527         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.003814795 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00456    |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 0.0176      |
-----------------------------------------
Output 141: Average over 53 episodes - Reward: 0.18867924528301888
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 141          |
|    time_elapsed         | 531          |
|    total_timesteps      | 288768       |
| train/                  |              |
|    approx_kl            | 0.0030762176 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.197       |
|    explained_variance   | 0.58         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0111       |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 0.0193       |
------------------------------------------
Output 142: Average over 32 episodes - Reward: 0.375
------------------------------------------
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 142          |
|    time_elapsed         | 534          |
|    total_timesteps      | 290816       |
| train/                  |              |
|    approx_kl            | 0.0045256778 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.545        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0101       |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00516     |
|    value_loss           | 0.0213       |
------------------------------------------
Output 143: Average over 39 episodes - Reward: 0.358974358974359
------------------------------------------
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 143          |
|    time_elapsed         | 538          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0037638564 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0.473        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00896     |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00521     |
|    value_loss           | 0.0172       |
------------------------------------------
Output 144: Average over 36 episodes - Reward: 0.3055555555555556
-----------------------------------------
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 144         |
|    time_elapsed         | 541         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.007446411 |
|    clip_fraction        | 0.0569      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.027       |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 0.0195      |
-----------------------------------------
Output 145: Average over 38 episodes - Reward: 0.13157894736842105
------------------------------------------
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 145          |
|    time_elapsed         | 544          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0028313103 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.554        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00922      |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.0192       |
------------------------------------------
Output 146: Average over 33 episodes - Reward: 0.3333333333333333
----------------------------------------
| time/                   |            |
|    fps                  | 545        |
|    iterations           | 146        |
|    time_elapsed         | 547        |
|    total_timesteps      | 299008     |
| train/                  |            |
|    approx_kl            | 0.00316412 |
|    clip_fraction        | 0.0286     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.173     |
|    explained_variance   | 0.52       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0013     |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.00445   |
|    value_loss           | 0.0153     |
----------------------------------------
Output 147: Average over 35 episodes - Reward: 0.3142857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 147          |
|    time_elapsed         | 551          |
|    total_timesteps      | 301056       |
| train/                  |              |
|    approx_kl            | 0.0022392978 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.484        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00363      |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 0.0178       |
------------------------------------------
Output 148: Average over 37 episodes - Reward: 0.35135135135135137
-----------------------------------------
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 148         |
|    time_elapsed         | 554         |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.002543461 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00732    |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 0.0166      |
-----------------------------------------
Output 149: Average over 40 episodes - Reward: 0.375
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 149          |
|    time_elapsed         | 557          |
|    total_timesteps      | 305152       |
| train/                  |              |
|    approx_kl            | 0.0030795876 |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0.58         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0113      |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00486     |
|    value_loss           | 0.0162       |
------------------------------------------
Output 150: Average over 39 episodes - Reward: 0.4358974358974359
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 150          |
|    time_elapsed         | 561          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0030735405 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.543        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00838      |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 0.0222       |
------------------------------------------
Output 151: Average over 39 episodes - Reward: 0.358974358974359
-----------------------------------------
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 151         |
|    time_elapsed         | 565         |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.003935194 |
|    clip_fraction        | 0.0372      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00327     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 0.0244      |
-----------------------------------------
Output 152: Average over 36 episodes - Reward: 0.4166666666666667
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 152          |
|    time_elapsed         | 568          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0019881278 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.157       |
|    explained_variance   | 0.558        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00616      |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 0.0193       |
------------------------------------------
Output 153: Average over 37 episodes - Reward: 0.2972972972972973
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 153          |
|    time_elapsed         | 572          |
|    total_timesteps      | 313344       |
| train/                  |              |
|    approx_kl            | 0.0014446053 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.166       |
|    explained_variance   | 0.524        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00966      |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 0.0175       |
------------------------------------------
Output 154: Average over 36 episodes - Reward: 0.4166666666666667
------------------------------------------
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 154          |
|    time_elapsed         | 575          |
|    total_timesteps      | 315392       |
| train/                  |              |
|    approx_kl            | 0.0027228976 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.175       |
|    explained_variance   | 0.5          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0117       |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 0.0213       |
------------------------------------------
Output 155: Average over 37 episodes - Reward: 0.10810810810810811
------------------------------------------
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 155          |
|    time_elapsed         | 579          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0028720144 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.544        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00172      |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 0.0222       |
------------------------------------------
Output 156: Average over 30 episodes - Reward: 0.43333333333333335
------------------------------------------
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 156          |
|    time_elapsed         | 582          |
|    total_timesteps      | 319488       |
| train/                  |              |
|    approx_kl            | 0.0034630205 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.145       |
|    explained_variance   | 0.574        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00582      |
|    n_updates            | 1550         |
|    policy_gradient_loss | -0.00379     |
|    value_loss           | 0.0127       |
------------------------------------------
Output 157: Average over 35 episodes - Reward: 0.2571428571428571
------------------------------------------
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 157          |
|    time_elapsed         | 586          |
|    total_timesteps      | 321536       |
| train/                  |              |
|    approx_kl            | 0.0034150933 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.138       |
|    explained_variance   | 0.554        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0185       |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00381     |
|    value_loss           | 0.0148       |
------------------------------------------
Output 158: Average over 42 episodes - Reward: 0.38095238095238093
----------------------------------------
| time/                   |            |
|    fps                  | 548        |
|    iterations           | 158        |
|    time_elapsed         | 589        |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.00271526 |
|    clip_fraction        | 0.0262     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00431    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.00235   |
|    value_loss           | 0.0172     |
----------------------------------------
Output 159: Average over 28 episodes - Reward: 0.4642857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 159          |
|    time_elapsed         | 593          |
|    total_timesteps      | 325632       |
| train/                  |              |
|    approx_kl            | 0.0028494871 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.531        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00698      |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.00373     |
|    value_loss           | 0.0231       |
------------------------------------------
Output 160: Average over 29 episodes - Reward: 0.4482758620689655
-----------------------------------------
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 160         |
|    time_elapsed         | 596         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.002137443 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00275    |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 0.0134      |
-----------------------------------------
Output 161: Average over 34 episodes - Reward: 0.38235294117647056
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 161          |
|    time_elapsed         | 600          |
|    total_timesteps      | 329728       |
| train/                  |              |
|    approx_kl            | 0.0037181424 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.666        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000282     |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00403     |
|    value_loss           | 0.0135       |
------------------------------------------
Output 162: Average over 31 episodes - Reward: 0.3548387096774194
-----------------------------------------
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 162         |
|    time_elapsed         | 603         |
|    total_timesteps      | 331776      |
| train/                  |             |
|    approx_kl            | 0.004616735 |
|    clip_fraction        | 0.0511      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0108      |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 0.0179      |
-----------------------------------------
Output 163: Average over 34 episodes - Reward: 0.3235294117647059
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 163          |
|    time_elapsed         | 607          |
|    total_timesteps      | 333824       |
| train/                  |              |
|    approx_kl            | 0.0028924947 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.146       |
|    explained_variance   | 0.602        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00652      |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 0.0147       |
------------------------------------------
Output 164: Average over 33 episodes - Reward: 0.3939393939393939
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 164          |
|    time_elapsed         | 610          |
|    total_timesteps      | 335872       |
| train/                  |              |
|    approx_kl            | 0.0035168966 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00935      |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00418     |
|    value_loss           | 0.0163       |
------------------------------------------
Output 165: Average over 35 episodes - Reward: 0.4857142857142857
-----------------------------------------
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 165         |
|    time_elapsed         | 614         |
|    total_timesteps      | 337920      |
| train/                  |             |
|    approx_kl            | 0.005079914 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000626    |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00431    |
|    value_loss           | 0.0186      |
-----------------------------------------
Output 166: Average over 32 episodes - Reward: 0.5
------------------------------------------
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 166          |
|    time_elapsed         | 618          |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.0029426059 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.159       |
|    explained_variance   | 0.602        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0162       |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00383     |
|    value_loss           | 0.0171       |
------------------------------------------
Output 167: Average over 36 episodes - Reward: 0.5
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 167          |
|    time_elapsed         | 621          |
|    total_timesteps      | 342016       |
| train/                  |              |
|    approx_kl            | 0.0021858113 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.14        |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000341     |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 0.0156       |
------------------------------------------
Output 168: Average over 32 episodes - Reward: 0.5625
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 168         |
|    time_elapsed         | 625         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.006691753 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00397    |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 0.0165      |
-----------------------------------------
Output 169: Average over 25 episodes - Reward: 0.28
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 169          |
|    time_elapsed         | 628          |
|    total_timesteps      | 346112       |
| train/                  |              |
|    approx_kl            | 0.0021762443 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.586        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00189     |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 0.015        |
------------------------------------------
Output 170: Average over 36 episodes - Reward: 0.25
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 170          |
|    time_elapsed         | 632          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0047812825 |
|    clip_fraction        | 0.0384       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00766      |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00601     |
|    value_loss           | 0.0095       |
------------------------------------------
Output 171: Average over 30 episodes - Reward: 0.3333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 171          |
|    time_elapsed         | 635          |
|    total_timesteps      | 350208       |
| train/                  |              |
|    approx_kl            | 0.0038139129 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00102     |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00718     |
|    value_loss           | 0.0257       |
------------------------------------------
Output 172: Average over 27 episodes - Reward: 0.2962962962962963
----------------------------------------
| time/                   |            |
|    fps                  | 550        |
|    iterations           | 172        |
|    time_elapsed         | 639        |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.01034845 |
|    clip_fraction        | 0.0356     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.26       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00566   |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.00584   |
|    value_loss           | 0.0172     |
----------------------------------------
Output 173: Average over 32 episodes - Reward: 0.59375
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 173         |
|    time_elapsed         | 643         |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.001976512 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00818    |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 0.0118      |
-----------------------------------------
Output 174: Average over 32 episodes - Reward: 0.34375
-----------------------------------------
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 174         |
|    time_elapsed         | 647         |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.008953303 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0152      |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 0.0182      |
-----------------------------------------
Output 175: Average over 27 episodes - Reward: 0.5555555555555556
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 175          |
|    time_elapsed         | 650          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.0034632343 |
|    clip_fraction        | 0.0445       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00353      |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00452     |
|    value_loss           | 0.0202       |
------------------------------------------
Output 176: Average over 26 episodes - Reward: 0.5769230769230769
------------------------------------------
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 176          |
|    time_elapsed         | 654          |
|    total_timesteps      | 360448       |
| train/                  |              |
|    approx_kl            | 0.0033154744 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0.487        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00184      |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.0145       |
------------------------------------------
Output 177: Average over 28 episodes - Reward: 0.35714285714285715
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 177          |
|    time_elapsed         | 657          |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0020547463 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.577        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00316      |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 0.0131       |
------------------------------------------
Output 178: Average over 28 episodes - Reward: 0.5
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 178          |
|    time_elapsed         | 661          |
|    total_timesteps      | 364544       |
| train/                  |              |
|    approx_kl            | 0.0023478563 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.111       |
|    explained_variance   | 0.532        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00749      |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.0017      |
|    value_loss           | 0.0119       |
------------------------------------------
Output 179: Average over 30 episodes - Reward: 0.43333333333333335
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 179          |
|    time_elapsed         | 665          |
|    total_timesteps      | 366592       |
| train/                  |              |
|    approx_kl            | 0.0024139308 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.113       |
|    explained_variance   | 0.585        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0101       |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00336     |
|    value_loss           | 0.0133       |
------------------------------------------
Output 180: Average over 29 episodes - Reward: 0.4482758620689655
-----------------------------------------
| time/                   |             |
|    fps                  | 551         |
|    iterations           | 180         |
|    time_elapsed         | 668         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.002440428 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0263      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 0.0172      |
-----------------------------------------
Output 181: Average over 28 episodes - Reward: 0.35714285714285715
------------------------------------------
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 181          |
|    time_elapsed         | 671          |
|    total_timesteps      | 370688       |
| train/                  |              |
|    approx_kl            | 0.0019350206 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0.49         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000949    |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 0.0162       |
------------------------------------------
Output 182: Average over 30 episodes - Reward: 0.43333333333333335
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 182          |
|    time_elapsed         | 675          |
|    total_timesteps      | 372736       |
| train/                  |              |
|    approx_kl            | 0.0019693202 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0955      |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00517      |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00212     |
|    value_loss           | 0.0151       |
------------------------------------------
Output 183: Average over 27 episodes - Reward: 0.4444444444444444
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 183         |
|    time_elapsed         | 678         |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.004908381 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0108      |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 0.0144      |
-----------------------------------------
Output 184: Average over 28 episodes - Reward: 0.35714285714285715
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 184          |
|    time_elapsed         | 682          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0014717805 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0981      |
|    explained_variance   | 0.586        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0106       |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 0.0123       |
------------------------------------------
Output 185: Average over 34 episodes - Reward: 0.4411764705882353
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 185          |
|    time_elapsed         | 685          |
|    total_timesteps      | 378880       |
| train/                  |              |
|    approx_kl            | 0.0023825238 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0867      |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00916      |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 0.0153       |
------------------------------------------
Output 186: Average over 28 episodes - Reward: 0.39285714285714285
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 186         |
|    time_elapsed         | 689         |
|    total_timesteps      | 380928      |
| train/                  |             |
|    approx_kl            | 0.004055172 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00319     |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 0.015       |
-----------------------------------------
Output 187: Average over 25 episodes - Reward: 0.36
-----------------------------------------
| time/                   |             |
|    fps                  | 552         |
|    iterations           | 187         |
|    time_elapsed         | 692         |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.003005984 |
|    clip_fraction        | 0.0439      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00369    |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 0.0131      |
-----------------------------------------
Output 188: Average over 28 episodes - Reward: 0.4642857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 188          |
|    time_elapsed         | 696          |
|    total_timesteps      | 385024       |
| train/                  |              |
|    approx_kl            | 0.0014705525 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.53         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0108       |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 0.0108       |
------------------------------------------
Output 189: Average over 26 episodes - Reward: 0.5384615384615384
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 189          |
|    time_elapsed         | 699          |
|    total_timesteps      | 387072       |
| train/                  |              |
|    approx_kl            | 0.0034879765 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0.545        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00447      |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00344     |
|    value_loss           | 0.0144       |
------------------------------------------
Output 190: Average over 30 episodes - Reward: 0.36666666666666664
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 190          |
|    time_elapsed         | 703          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.0016098927 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | 0.564        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00896      |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.000853    |
|    value_loss           | 0.0121       |
------------------------------------------
Output 191: Average over 29 episodes - Reward: 0.4827586206896552
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 191         |
|    time_elapsed         | 707         |
|    total_timesteps      | 391168      |
| train/                  |             |
|    approx_kl            | 0.011621024 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 0.0154      |
-----------------------------------------
Output 192: Average over 26 episodes - Reward: 0.38461538461538464
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 192          |
|    time_elapsed         | 711          |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0033470928 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.143       |
|    explained_variance   | 0.659        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00277      |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.0124       |
------------------------------------------
Output 193: Average over 27 episodes - Reward: 0.4074074074074074
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 193          |
|    time_elapsed         | 715          |
|    total_timesteps      | 395264       |
| train/                  |              |
|    approx_kl            | 0.0022633136 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.644        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00547      |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 0.0105       |
------------------------------------------
Output 194: Average over 28 episodes - Reward: 0.35714285714285715
------------------------------------------
| time/                   |              |
|    fps                  | 552          |
|    iterations           | 194          |
|    time_elapsed         | 718          |
|    total_timesteps      | 397312       |
| train/                  |              |
|    approx_kl            | 0.0013024859 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.407        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00426      |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00249     |
|    value_loss           | 0.0117       |
------------------------------------------
Output 195: Average over 27 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 195          |
|    time_elapsed         | 722          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.0019911835 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.138       |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00727      |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 0.018        |
------------------------------------------
Output 196: Average over 25 episodes - Reward: 0.52
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 196         |
|    time_elapsed         | 725         |
|    total_timesteps      | 401408      |
| train/                  |             |
|    approx_kl            | 0.009863447 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00346     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 0.0126      |
-----------------------------------------
Output 197: Average over 27 episodes - Reward: 0.4074074074074074
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 197          |
|    time_elapsed         | 728          |
|    total_timesteps      | 403456       |
| train/                  |              |
|    approx_kl            | 0.0024202117 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.139       |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00917      |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.0123       |
------------------------------------------
Output 198: Average over 28 episodes - Reward: 0.32142857142857145
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 198         |
|    time_elapsed         | 732         |
|    total_timesteps      | 405504      |
| train/                  |             |
|    approx_kl            | 0.002910091 |
|    clip_fraction        | 0.032       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.177      |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0029      |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 0.0116      |
-----------------------------------------
Output 199: Average over 26 episodes - Reward: 0.34615384615384615
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 199          |
|    time_elapsed         | 736          |
|    total_timesteps      | 407552       |
| train/                  |              |
|    approx_kl            | 0.0026286812 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 0.527        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000976    |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 0.0117       |
------------------------------------------
Output 200: Average over 31 episodes - Reward: 0.45161290322580644
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 200          |
|    time_elapsed         | 740          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0036033755 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.5          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0118      |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00324     |
|    value_loss           | 0.0145       |
------------------------------------------
Output 201: Average over 32 episodes - Reward: 0.4375
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 201         |
|    time_elapsed         | 743         |
|    total_timesteps      | 411648      |
| train/                  |             |
|    approx_kl            | 0.012444817 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00203     |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 0.0176      |
-----------------------------------------
Output 202: Average over 34 episodes - Reward: 0.3235294117647059
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 202         |
|    time_elapsed         | 747         |
|    total_timesteps      | 413696      |
| train/                  |             |
|    approx_kl            | 0.008070963 |
|    clip_fraction        | 0.0504      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00421     |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 0.014       |
-----------------------------------------
Output 203: Average over 30 episodes - Reward: 0.5333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 203          |
|    time_elapsed         | 750          |
|    total_timesteps      | 415744       |
| train/                  |              |
|    approx_kl            | 0.0039951694 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.643        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0128       |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.00538     |
|    value_loss           | 0.0146       |
------------------------------------------
Output 204: Average over 28 episodes - Reward: 0.6071428571428571
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 204         |
|    time_elapsed         | 754         |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.004479845 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00907    |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00437    |
|    value_loss           | 0.017       |
-----------------------------------------
Output 205: Average over 27 episodes - Reward: 0.5185185185185185
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 205          |
|    time_elapsed         | 758          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.0024953713 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.509        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0135       |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 0.012        |
------------------------------------------
Output 206: Average over 30 episodes - Reward: 0.36666666666666664
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 206          |
|    time_elapsed         | 761          |
|    total_timesteps      | 421888       |
| train/                  |              |
|    approx_kl            | 0.0042081666 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.55         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0109      |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.00315     |
|    value_loss           | 0.013        |
------------------------------------------
Output 207: Average over 30 episodes - Reward: 0.43333333333333335
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 207          |
|    time_elapsed         | 765          |
|    total_timesteps      | 423936       |
| train/                  |              |
|    approx_kl            | 0.0026228721 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.477        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00621      |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00387     |
|    value_loss           | 0.0145       |
------------------------------------------
Output 208: Average over 30 episodes - Reward: 0.5
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 208          |
|    time_elapsed         | 768          |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0029716592 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.502        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0129       |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 0.0149       |
------------------------------------------
Output 209: Average over 26 episodes - Reward: 0.23076923076923078
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 209          |
|    time_elapsed         | 772          |
|    total_timesteps      | 428032       |
| train/                  |              |
|    approx_kl            | 0.0032772692 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.517        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00728      |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.0153       |
------------------------------------------
Output 210: Average over 26 episodes - Reward: 0.4230769230769231
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 210          |
|    time_elapsed         | 775          |
|    total_timesteps      | 430080       |
| train/                  |              |
|    approx_kl            | 0.0035440356 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.505        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0041       |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00597     |
|    value_loss           | 0.00917      |
------------------------------------------
Output 211: Average over 29 episodes - Reward: 0.41379310344827586
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 211          |
|    time_elapsed         | 778          |
|    total_timesteps      | 432128       |
| train/                  |              |
|    approx_kl            | 0.0025574397 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00172     |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.0023      |
|    value_loss           | 0.013        |
------------------------------------------
Output 212: Average over 28 episodes - Reward: 0.39285714285714285
------------------------------------------
| time/                   |              |
|    fps                  | 555          |
|    iterations           | 212          |
|    time_elapsed         | 782          |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 0.0020702707 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.174       |
|    explained_variance   | 0.488        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0112       |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 0.0142       |
------------------------------------------
Output 213: Average over 27 episodes - Reward: 0.37037037037037035
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 213          |
|    time_elapsed         | 786          |
|    total_timesteps      | 436224       |
| train/                  |              |
|    approx_kl            | 0.0034255853 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000546    |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00474     |
|    value_loss           | 0.0143       |
------------------------------------------
Output 214: Average over 34 episodes - Reward: 0.47058823529411764
-----------------------------------------
| time/                   |             |
|    fps                  | 555         |
|    iterations           | 214         |
|    time_elapsed         | 789         |
|    total_timesteps      | 438272      |
| train/                  |             |
|    approx_kl            | 0.005111766 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00052    |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00272    |
|    value_loss           | 0.0112      |
-----------------------------------------
Output 215: Average over 27 episodes - Reward: 0.4444444444444444
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 215          |
|    time_elapsed         | 793          |
|    total_timesteps      | 440320       |
| train/                  |              |
|    approx_kl            | 0.0045599504 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.575        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00797      |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00435     |
|    value_loss           | 0.0161       |
------------------------------------------
Output 216: Average over 30 episodes - Reward: 0.3333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 216          |
|    time_elapsed         | 797          |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0039812983 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.146       |
|    explained_variance   | 0.555        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000507     |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 0.0146       |
------------------------------------------
Output 217: Average over 27 episodes - Reward: 0.6296296296296297
----------------------------------------
| time/                   |            |
|    fps                  | 555        |
|    iterations           | 217        |
|    time_elapsed         | 800        |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.00424543 |
|    clip_fraction        | 0.0301     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0042     |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.00322   |
|    value_loss           | 0.0118     |
----------------------------------------
Output 218: Average over 26 episodes - Reward: 0.34615384615384615
-----------------------------------------
| time/                   |             |
|    fps                  | 555         |
|    iterations           | 218         |
|    time_elapsed         | 804         |
|    total_timesteps      | 446464      |
| train/                  |             |
|    approx_kl            | 0.004601448 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0256      |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 0.0146      |
-----------------------------------------
Output 219: Average over 28 episodes - Reward: 0.21428571428571427
------------------------------------------
| time/                   |              |
|    fps                  | 555          |
|    iterations           | 219          |
|    time_elapsed         | 807          |
|    total_timesteps      | 448512       |
| train/                  |              |
|    approx_kl            | 0.0025206502 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.014       |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.0129       |
------------------------------------------
Output 220: Average over 29 episodes - Reward: 0.27586206896551724
-----------------------------------------
| time/                   |             |
|    fps                  | 555         |
|    iterations           | 220         |
|    time_elapsed         | 811         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.014903564 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.129       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00345     |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00633    |
|    value_loss           | 0.0156      |
-----------------------------------------
Output 221: Average over 27 episodes - Reward: 0.4074074074074074
-----------------------------------------
| time/                   |             |
|    fps                  | 555         |
|    iterations           | 221         |
|    time_elapsed         | 815         |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.004713732 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.145       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00173    |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 0.0169      |
-----------------------------------------
Output 222: Average over 29 episodes - Reward: 0.3103448275862069
------------------------------------------
| time/                   |              |
|    fps                  | 555          |
|    iterations           | 222          |
|    time_elapsed         | 819          |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 0.0047774054 |
|    clip_fraction        | 0.0473       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.166       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0108      |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00519     |
|    value_loss           | 0.0141       |
------------------------------------------
Output 223: Average over 28 episodes - Reward: 0.6071428571428571
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 223         |
|    time_elapsed         | 823         |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.005171293 |
|    clip_fraction        | 0.0362      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00693     |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 0.0191      |
-----------------------------------------
Output 224: Average over 27 episodes - Reward: 0.48148148148148145
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 224         |
|    time_elapsed         | 826         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.003474744 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00729     |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 0.0173      |
-----------------------------------------
Output 225: Average over 30 episodes - Reward: 0.43333333333333335
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 225          |
|    time_elapsed         | 830          |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 0.0075937063 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.553        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00665      |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00379     |
|    value_loss           | 0.0149       |
------------------------------------------
Output 226: Average over 33 episodes - Reward: 0.48484848484848486
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 226          |
|    time_elapsed         | 835          |
|    total_timesteps      | 462848       |
| train/                  |              |
|    approx_kl            | 0.0063985535 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.483        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0131      |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.00429     |
|    value_loss           | 0.0142       |
------------------------------------------
Output 227: Average over 32 episodes - Reward: 0.46875
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 227         |
|    time_elapsed         | 839         |
|    total_timesteps      | 464896      |
| train/                  |             |
|    approx_kl            | 0.005023271 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0069      |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 0.0184      |
-----------------------------------------
Output 228: Average over 31 episodes - Reward: 0.3548387096774194
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 228         |
|    time_elapsed         | 842         |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.004610081 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.174      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 0.022       |
-----------------------------------------
Output 229: Average over 26 episodes - Reward: 0.46153846153846156
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 229          |
|    time_elapsed         | 847          |
|    total_timesteps      | 468992       |
| train/                  |              |
|    approx_kl            | 0.0032651057 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00113     |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.00434     |
|    value_loss           | 0.0197       |
------------------------------------------
Output 230: Average over 32 episodes - Reward: 0.4375
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 230          |
|    time_elapsed         | 851          |
|    total_timesteps      | 471040       |
| train/                  |              |
|    approx_kl            | 0.0027232417 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.142       |
|    explained_variance   | 0.219        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00248     |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00291     |
|    value_loss           | 0.0162       |
------------------------------------------
Output 231: Average over 29 episodes - Reward: 0.27586206896551724
------------------------------------------
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 231          |
|    time_elapsed         | 854          |
|    total_timesteps      | 473088       |
| train/                  |              |
|    approx_kl            | 0.0038722607 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.475        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00948      |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00492     |
|    value_loss           | 0.0142       |
------------------------------------------
Output 232: Average over 27 episodes - Reward: 0.48148148148148145
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 232         |
|    time_elapsed         | 858         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.009093117 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0115      |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 0.0142      |
-----------------------------------------
Output 233: Average over 29 episodes - Reward: 0.41379310344827586
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 233         |
|    time_elapsed         | 861         |
|    total_timesteps      | 477184      |
| train/                  |             |
|    approx_kl            | 0.005897702 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00441     |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 0.0145      |
-----------------------------------------
Output 234: Average over 29 episodes - Reward: 0.4482758620689655
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 234         |
|    time_elapsed         | 865         |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.002825226 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00807     |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 0.013       |
-----------------------------------------
Output 235: Average over 28 episodes - Reward: 0.5714285714285714
-----------------------------------------
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 235         |
|    time_elapsed         | 868         |
|    total_timesteps      | 481280      |
| train/                  |             |
|    approx_kl            | 0.004020022 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00258     |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 0.0151      |
-----------------------------------------
Output 236: Average over 25 episodes - Reward: 0.56
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 236          |
|    time_elapsed         | 872          |
|    total_timesteps      | 483328       |
| train/                  |              |
|    approx_kl            | 0.0016739517 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.502        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0021       |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 0.015        |
------------------------------------------
Output 237: Average over 29 episodes - Reward: 0.5172413793103449
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 237         |
|    time_elapsed         | 876         |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.003552867 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000223    |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 0.0137      |
-----------------------------------------
Output 238: Average over 33 episodes - Reward: 0.3939393939393939
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 238          |
|    time_elapsed         | 879          |
|    total_timesteps      | 487424       |
| train/                  |              |
|    approx_kl            | 0.0039708978 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00933      |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00249     |
|    value_loss           | 0.0133       |
------------------------------------------
Output 239: Average over 32 episodes - Reward: 0.375
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 239          |
|    time_elapsed         | 882          |
|    total_timesteps      | 489472       |
| train/                  |              |
|    approx_kl            | 0.0090152975 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.143       |
|    explained_variance   | 0.552        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00741     |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.00423     |
|    value_loss           | 0.0146       |
------------------------------------------
Output 240: Average over 27 episodes - Reward: 0.2222222222222222
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 240         |
|    time_elapsed         | 886         |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.004164514 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00282    |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00228    |
|    value_loss           | 0.0144      |
-----------------------------------------
Output 241: Average over 35 episodes - Reward: 0.5714285714285714
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 241          |
|    time_elapsed         | 889          |
|    total_timesteps      | 493568       |
| train/                  |              |
|    approx_kl            | 0.0050030374 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0.435        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00778      |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.0037      |
|    value_loss           | 0.0133       |
------------------------------------------
Output 242: Average over 29 episodes - Reward: 0.41379310344827586
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 242         |
|    time_elapsed         | 893         |
|    total_timesteps      | 495616      |
| train/                  |             |
|    approx_kl            | 0.002582596 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0051      |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 0.0186      |
-----------------------------------------
Output 243: Average over 25 episodes - Reward: 0.4
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 243          |
|    time_elapsed         | 897          |
|    total_timesteps      | 497664       |
| train/                  |              |
|    approx_kl            | 0.0029631574 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0.532        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.033        |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00321     |
|    value_loss           | 0.0142       |
------------------------------------------
Output 244: Average over 28 episodes - Reward: 0.5
-----------------------------------------
| time/                   |             |
|    fps                  | 554         |
|    iterations           | 244         |
|    time_elapsed         | 900         |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.003355233 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000776   |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 0.0108      |
-----------------------------------------
Output 245: Average over 30 episodes - Reward: 0.5
------------------------------------------
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 245          |
|    time_elapsed         | 904          |
|    total_timesteps      | 501760       |
| train/                  |              |
|    approx_kl            | 0.0027383165 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0868      |
|    explained_variance   | 0.487        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00758      |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 0.013        |
------------------------------------------
Overall: Average Reward: 0.22795991166978088
