Output 1: Average over 100 episodes - Reward: 0.03
-----------------------------
| time/              |      |
|    fps             | 774  |
|    iterations      | 1    |
|    time_elapsed    | 2    |
|    total_timesteps | 2048 |
-----------------------------
Output 2: Average over 100 episodes - Reward: 0.04
-----------------------------------------
| time/                   |             |
|    fps                  | 475         |
|    iterations           | 2           |
|    time_elapsed         | 8           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010778175 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -2.79       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00895    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.0213      |
-----------------------------------------
Output 3: Average over 100 episodes - Reward: 0.12
-----------------------------------------
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 3           |
|    time_elapsed         | 13          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.017334111 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0445     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 0.031       |
-----------------------------------------
Output 4: Average over 100 episodes - Reward: 0.25
-----------------------------------------
| time/                   |             |
|    fps                  | 446         |
|    iterations           | 4           |
|    time_elapsed         | 18          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014057713 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0318     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.0499      |
-----------------------------------------
Output 5: Average over 100 episodes - Reward: 0.51
-----------------------------------------
| time/                   |             |
|    fps                  | 443         |
|    iterations           | 5           |
|    time_elapsed         | 23          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.019008068 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0299      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.046      |
|    value_loss           | 0.0922      |
-----------------------------------------
Output 6: Average over 100 episodes - Reward: 0.71
-----------------------------------------
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 6           |
|    time_elapsed         | 27          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.019509174 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0451     |
|    value_loss           | 0.119       |
-----------------------------------------
Output 7: Average over 100 episodes - Reward: 0.85
-----------------------------------------
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 7           |
|    time_elapsed         | 33          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.019528434 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.972      |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0137      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0391     |
|    value_loss           | 0.101       |
-----------------------------------------
Output 8: Average over 100 episodes - Reward: 0.94
-----------------------------------------
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 8           |
|    time_elapsed         | 37          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.013335816 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.832      |
|    explained_variance   | 0.0762      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0175      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0307     |
|    value_loss           | 0.0655      |
-----------------------------------------
Output 9: Average over 100 episodes - Reward: 0.93
----------------------------------------
| time/                   |            |
|    fps                  | 433        |
|    iterations           | 9          |
|    time_elapsed         | 42         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.01804084 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.69      |
|    explained_variance   | 0.0696     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.029     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.0517     |
----------------------------------------
Output 10: Average over 100 episodes - Reward: 0.95
-----------------------------------------
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 10          |
|    time_elapsed         | 47          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.008682115 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.0684      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00177    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.0266      |
-----------------------------------------
Output 11: Average over 100 episodes - Reward: 0.98
------------------------------------------
| time/                   |              |
|    fps                  | 434          |
|    iterations           | 11           |
|    time_elapsed         | 51           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0065875044 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0.0223       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00222      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.0132      |
|    value_loss           | 0.0279       |
------------------------------------------
Output 12: Average over 100 episodes - Reward: 0.98
---------------------------------------
| time/                   |           |
|    fps                  | 437       |
|    iterations           | 12        |
|    time_elapsed         | 56        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0476722 |
|    clip_fraction        | 0.11      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.384    |
|    explained_variance   | 0.0613    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000989  |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0194   |
|    value_loss           | 0.0103    |
---------------------------------------
Output 13: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 437         |
|    iterations           | 13          |
|    time_elapsed         | 60          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.009908178 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.286      |
|    explained_variance   | 0.0401      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00998    |
|    value_loss           | 0.00982     |
-----------------------------------------
Output 14: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 14          |
|    time_elapsed         | 65          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.006857883 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.0521      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0196     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00966    |
|    value_loss           | 0.00571     |
-----------------------------------------
Output 15: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 15          |
|    time_elapsed         | 71          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.013343428 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00531    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.00221     |
-----------------------------------------
Output 16: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 16           |
|    time_elapsed         | 76           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0068672355 |
|    clip_fraction        | 0.0567       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.137       |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00414      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 4.34e-06     |
------------------------------------------
Output 17: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 17          |
|    time_elapsed         | 80          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.009386027 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0159     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 0.00195     |
-----------------------------------------
Output 18: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 18           |
|    time_elapsed         | 85           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0051243696 |
|    clip_fraction        | 0.0757       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000354     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 4.39e-06     |
------------------------------------------
Output 19: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 19           |
|    time_elapsed         | 90           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0026875506 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000728    |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 0.00194      |
------------------------------------------
Output 20: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 20           |
|    time_elapsed         | 94           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0012545829 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.131       |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000678     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 0.00162      |
------------------------------------------
Output 21: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 433          |
|    iterations           | 21           |
|    time_elapsed         | 99           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0034308778 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000276    |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 1.76e-06     |
------------------------------------------
Output 22: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 433          |
|    iterations           | 22           |
|    time_elapsed         | 104          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0048408797 |
|    clip_fraction        | 0.0826       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00125     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00513     |
|    value_loss           | 0.00251      |
------------------------------------------
Output 23: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 434          |
|    iterations           | 23           |
|    time_elapsed         | 108          |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0003912247 |
|    clip_fraction        | 0.00786      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000293     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000301    |
|    value_loss           | 3.79e-06     |
------------------------------------------
Output 24: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 24           |
|    time_elapsed         | 112          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0021096305 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0142      |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 1.68e-06     |
------------------------------------------
Output 25: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 25           |
|    time_elapsed         | 117          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0005653489 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0131      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 2.54e-06     |
------------------------------------------
Output 26: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 26          |
|    time_elapsed         | 121         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.004454055 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.166       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00174    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 0.00165     |
-----------------------------------------
Output 27: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 437          |
|    iterations           | 27           |
|    time_elapsed         | 126          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0033539622 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0231       |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00524     |
|    value_loss           | 2.32e-06     |
------------------------------------------
Output 28: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 438          |
|    iterations           | 28           |
|    time_elapsed         | 130          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0026721803 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000844     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 0.00166      |
------------------------------------------
Output 29: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 438          |
|    iterations           | 29           |
|    time_elapsed         | 135          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0025191007 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.533        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.01        |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00356     |
|    value_loss           | 5.02e-06     |
------------------------------------------
Output 30: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 437           |
|    iterations           | 30            |
|    time_elapsed         | 140           |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00080347434 |
|    clip_fraction        | 0.0171        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.118        |
|    explained_variance   | 0.167         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00105      |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.00231      |
|    value_loss           | 0.00165       |
-------------------------------------------
Output 31: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 436          |
|    iterations           | 31           |
|    time_elapsed         | 145          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0022382576 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00465     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.000673    |
|    value_loss           | 3.23e-06     |
------------------------------------------
Output 32: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 32          |
|    time_elapsed         | 150         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.001769267 |
|    clip_fraction        | 0.00562     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.005       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.000878   |
|    value_loss           | 1.02e-07    |
-----------------------------------------
Output 33: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 436          |
|    iterations           | 33           |
|    time_elapsed         | 154          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0016057394 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00383     |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 1.25e-10     |
------------------------------------------
Output 34: Average over 100 episodes - Reward: 0.99
-------------------------------------------
| time/                   |               |
|    fps                  | 436           |
|    iterations           | 34            |
|    time_elapsed         | 159           |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00040101234 |
|    clip_fraction        | 0.0417        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.112        |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00102       |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.00164      |
|    value_loss           | 1.94e-06      |
-------------------------------------------
Output 35: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 35           |
|    time_elapsed         | 164          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0037963458 |
|    clip_fraction        | 0.0799       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00916     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00356     |
|    value_loss           | 0.00204      |
------------------------------------------
Output 36: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 36           |
|    time_elapsed         | 169          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0030696266 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.566        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000627    |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 3.37e-06     |
------------------------------------------
Output 37: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 434          |
|    iterations           | 37           |
|    time_elapsed         | 174          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0047057867 |
|    clip_fraction        | 0.0582       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00342     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 8.18e-07     |
------------------------------------------
Output 38: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 38          |
|    time_elapsed         | 178         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.014187144 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0127     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 2.76e-06    |
-----------------------------------------
Output 39: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 436          |
|    iterations           | 39           |
|    time_elapsed         | 183          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0012660637 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0151      |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 4.75e-07     |
------------------------------------------
Output 40: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 437        |
|    iterations           | 40         |
|    time_elapsed         | 187        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.01637173 |
|    clip_fraction        | 0.0342     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000283   |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 6.87e-06   |
----------------------------------------
Output 41: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 437          |
|    iterations           | 41           |
|    time_elapsed         | 191          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0018864621 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.106       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00436     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00306     |
|    value_loss           | 1.25e-06     |
------------------------------------------
Output 42: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 42          |
|    time_elapsed         | 196         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.020846644 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0476     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 2.47e-07    |
-----------------------------------------
Output 43: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 439        |
|    iterations           | 43         |
|    time_elapsed         | 200        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.08179483 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0469    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 4.87e-05   |
----------------------------------------
Output 44: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 438        |
|    iterations           | 44         |
|    time_elapsed         | 205        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.03428629 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0224    |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0209    |
|    value_loss           | 5.58e-06   |
----------------------------------------
Output 45: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 438          |
|    iterations           | 45           |
|    time_elapsed         | 210          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0018416911 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00411     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00226     |
|    value_loss           | 4.96e-08     |
------------------------------------------
Output 46: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 439          |
|    iterations           | 46           |
|    time_elapsed         | 214          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0014465116 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00305     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000881    |
|    value_loss           | 0.00203      |
------------------------------------------
Output 47: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 440          |
|    iterations           | 47           |
|    time_elapsed         | 218          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0040981183 |
|    clip_fraction        | 0.063        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.681        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00123     |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 4.49e-06     |
------------------------------------------
Output 48: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 440          |
|    iterations           | 48           |
|    time_elapsed         | 223          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0003974808 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00111     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000495    |
|    value_loss           | 8.18e-07     |
------------------------------------------
Output 49: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 440          |
|    iterations           | 49           |
|    time_elapsed         | 227          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0034518875 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000172     |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 2.93e-09     |
------------------------------------------
Output 50: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 50          |
|    time_elapsed         | 232         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.002436793 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0226     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 2.49e-07    |
-----------------------------------------
Output 51: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 441          |
|    iterations           | 51           |
|    time_elapsed         | 236          |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.0014864656 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0981      |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000693     |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 2.95e-09     |
------------------------------------------
Output 52: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 442          |
|    iterations           | 52           |
|    time_elapsed         | 240          |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 0.0002689993 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0998      |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000255    |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000732    |
|    value_loss           | 5e-08        |
------------------------------------------
Output 53: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 53          |
|    time_elapsed         | 245         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.001100047 |
|    clip_fraction        | 0.00649     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00171    |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.000312   |
|    value_loss           | 4.26e-10    |
-----------------------------------------
Output 54: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 443          |
|    iterations           | 54           |
|    time_elapsed         | 249          |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 8.043804e-05 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000212    |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000696    |
|    value_loss           | 7.59e-07     |
------------------------------------------
Output 55: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 443        |
|    iterations           | 55         |
|    time_elapsed         | 253        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.00180318 |
|    clip_fraction        | 0.0272     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00111   |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.000127  |
|    value_loss           | 3.92e-09   |
----------------------------------------
Output 56: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 444          |
|    iterations           | 56           |
|    time_elapsed         | 257          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0060653943 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00353      |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.000749    |
|    value_loss           | 8.04e-07     |
------------------------------------------
Output 57: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 445         |
|    iterations           | 57          |
|    time_elapsed         | 261         |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.010241828 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0229     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 7.79e-09    |
-----------------------------------------
Output 58: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 446         |
|    iterations           | 58          |
|    time_elapsed         | 266         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.044756055 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0349     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.043      |
|    value_loss           | 5.71e-05    |
-----------------------------------------
Output 59: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 447         |
|    iterations           | 59          |
|    time_elapsed         | 270         |
|    total_timesteps      | 120832      |
| train/                  |             |
|    approx_kl            | 0.002390564 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000191   |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 4.24e-06    |
-----------------------------------------
Output 60: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 447         |
|    iterations           | 60          |
|    time_elapsed         | 274         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.008646701 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0865     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 4.22e-06    |
-----------------------------------------
Output 61: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 449         |
|    iterations           | 61          |
|    time_elapsed         | 278         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.038198315 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0817     |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00312    |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.00094     |
-----------------------------------------
Output 62: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 449          |
|    iterations           | 62           |
|    time_elapsed         | 282          |
|    total_timesteps      | 126976       |
| train/                  |              |
|    approx_kl            | 0.0029622603 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0934      |
|    explained_variance   | 0.237        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0124      |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.000603    |
|    value_loss           | 0.000903     |
------------------------------------------
Output 63: Average over 100 episodes - Reward: 0.68
-----------------------------------------
| time/                   |             |
|    fps                  | 450         |
|    iterations           | 63          |
|    time_elapsed         | 286         |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.021627026 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 2.06e-06    |
-----------------------------------------
Output 64: Average over 100 episodes - Reward: 0.93
-----------------------------------------
| time/                   |             |
|    fps                  | 450         |
|    iterations           | 64          |
|    time_elapsed         | 291         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.050286923 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.00871     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0466     |
|    value_loss           | 0.143       |
-----------------------------------------
Output 65: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 451        |
|    iterations           | 65         |
|    time_elapsed         | 295        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.04688232 |
|    clip_fraction        | 0.0781     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | -0.198     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0211    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.0461     |
----------------------------------------
Output 66: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 452         |
|    iterations           | 66          |
|    time_elapsed         | 299         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.017397286 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | -2.33       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0306     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.000152    |
-----------------------------------------
Output 67: Average over 100 episodes - Reward: 0.7
-----------------------------------------
| time/                   |             |
|    fps                  | 453         |
|    iterations           | 67          |
|    time_elapsed         | 302         |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.019645423 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0656     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0427     |
|    value_loss           | 2.68e-05    |
-----------------------------------------
Output 68: Average over 100 episodes - Reward: 0.91
-----------------------------------------
| time/                   |             |
|    fps                  | 453         |
|    iterations           | 68          |
|    time_elapsed         | 306         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.041057542 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.359      |
|    explained_variance   | 0.023       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0404      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.11        |
-----------------------------------------
Output 69: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 454         |
|    iterations           | 69          |
|    time_elapsed         | 310         |
|    total_timesteps      | 141312      |
| train/                  |             |
|    approx_kl            | 0.029517587 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.307      |
|    explained_variance   | 0.00714     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00944    |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.0434      |
-----------------------------------------
Output 70: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 455          |
|    iterations           | 70           |
|    time_elapsed         | 314          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0033762092 |
|    clip_fraction        | 0.0427       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | -0.0673      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.009       |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00993     |
|    value_loss           | 0.0079       |
------------------------------------------
Output 71: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 456         |
|    iterations           | 71          |
|    time_elapsed         | 318         |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.003991591 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0295     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.00319     |
-----------------------------------------
Output 72: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 457          |
|    iterations           | 72           |
|    time_elapsed         | 322          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0032173689 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.224       |
|    explained_variance   | 0.0838       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0311      |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.0129      |
|    value_loss           | 0.00475      |
------------------------------------------
Output 73: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 458         |
|    iterations           | 73          |
|    time_elapsed         | 325         |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.004225649 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00287    |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.00162     |
-----------------------------------------
Output 74: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 459          |
|    iterations           | 74           |
|    time_elapsed         | 329          |
|    total_timesteps      | 151552       |
| train/                  |              |
|    approx_kl            | 0.0020302755 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.061        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00302      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00778     |
|    value_loss           | 0.0054       |
------------------------------------------
Output 75: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 460          |
|    iterations           | 75           |
|    time_elapsed         | 333          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0026054499 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0107      |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00635     |
|    value_loss           | 0.00194      |
------------------------------------------
Output 76: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 461          |
|    iterations           | 76           |
|    time_elapsed         | 337          |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 0.0059924615 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.14        |
|    explained_variance   | 0.0768       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0168      |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00697     |
|    value_loss           | 0.00383      |
------------------------------------------
Output 77: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 462          |
|    iterations           | 77           |
|    time_elapsed         | 341          |
|    total_timesteps      | 157696       |
| train/                  |              |
|    approx_kl            | 0.0026904205 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00699     |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00299     |
|    value_loss           | 9.22e-06     |
------------------------------------------
Output 78: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 462         |
|    iterations           | 78          |
|    time_elapsed         | 345         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.005020516 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0165     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 5.44e-06    |
-----------------------------------------
Output 79: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 462         |
|    iterations           | 79          |
|    time_elapsed         | 349         |
|    total_timesteps      | 161792      |
| train/                  |             |
|    approx_kl            | 0.013378655 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0253     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00666    |
|    value_loss           | 2.69e-06    |
-----------------------------------------
Output 80: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 463        |
|    iterations           | 80         |
|    time_elapsed         | 353        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.02096063 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.174     |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0181    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 2.47e-05   |
----------------------------------------
Output 81: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 81          |
|    time_elapsed         | 357         |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.031897303 |
|    clip_fraction        | 0.096       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.037      |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 6.31e-06    |
-----------------------------------------
Output 82: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 82           |
|    time_elapsed         | 361          |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 0.0015071658 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.1         |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0003       |
|    loss                 | -5.99e-05    |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00433     |
|    value_loss           | 0.00196      |
------------------------------------------
Output 83: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 465           |
|    iterations           | 83            |
|    time_elapsed         | 365           |
|    total_timesteps      | 169984        |
| train/                  |               |
|    approx_kl            | 0.00067208847 |
|    clip_fraction        | 0.00845       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0915       |
|    explained_variance   | 0.0626        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00632      |
|    n_updates            | 820           |
|    policy_gradient_loss | -0.00144      |
|    value_loss           | 0.0039        |
-------------------------------------------
Output 84: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 466           |
|    iterations           | 84            |
|    time_elapsed         | 368           |
|    total_timesteps      | 172032        |
| train/                  |               |
|    approx_kl            | 0.00037560536 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0908       |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00107       |
|    n_updates            | 830           |
|    policy_gradient_loss | -0.000179     |
|    value_loss           | 3.66e-06      |
-------------------------------------------
Output 85: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 85          |
|    time_elapsed         | 372         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.055453923 |
|    clip_fraction        | 0.0568      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0633     |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0454     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 0.00195     |
-----------------------------------------
Output 86: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 86          |
|    time_elapsed         | 376         |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.005720675 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.057       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0183     |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 0.00165     |
-----------------------------------------
Output 87: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 468        |
|    iterations           | 87         |
|    time_elapsed         | 380        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.08573519 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0619    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0345    |
|    value_loss           | 1.7e-05    |
----------------------------------------
Output 88: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 88           |
|    time_elapsed         | 384          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0118433945 |
|    clip_fraction        | 0.0552       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0188      |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00431     |
|    value_loss           | 4.2e-07      |
------------------------------------------
Output 89: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 469        |
|    iterations           | 89         |
|    time_elapsed         | 388        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.00290873 |
|    clip_fraction        | 0.0128     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000984  |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.00172   |
|    value_loss           | 1.44e-06   |
----------------------------------------
Output 90: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 470         |
|    iterations           | 90          |
|    time_elapsed         | 392         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.011691468 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0756     |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00347    |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 5.13e-07    |
-----------------------------------------
Output 91: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 470          |
|    iterations           | 91           |
|    time_elapsed         | 395          |
|    total_timesteps      | 186368       |
| train/                  |              |
|    approx_kl            | 0.0073301713 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.086       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00702     |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.000876    |
|    value_loss           | 4.69e-07     |
------------------------------------------
Output 92: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 92           |
|    time_elapsed         | 399          |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0008321719 |
|    clip_fraction        | 0.00918      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000997    |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00186     |
|    value_loss           | 0.00165      |
------------------------------------------
Output 93: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 472           |
|    iterations           | 93            |
|    time_elapsed         | 403           |
|    total_timesteps      | 190464        |
| train/                  |               |
|    approx_kl            | 0.00021436106 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.103        |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00133       |
|    n_updates            | 920           |
|    policy_gradient_loss | -0.0006       |
|    value_loss           | 1.26e-06      |
-------------------------------------------
Output 94: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 94           |
|    time_elapsed         | 407          |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 0.0031954055 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000595     |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.000405    |
|    value_loss           | 5.52e-09     |
------------------------------------------
Output 95: Average over 100 episodes - Reward: 0.81
-----------------------------------------
| time/                   |             |
|    fps                  | 473         |
|    iterations           | 95          |
|    time_elapsed         | 410         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.007844867 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.166       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.01        |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.00165     |
-----------------------------------------
Output 96: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 474         |
|    iterations           | 96          |
|    time_elapsed         | 414         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.044909753 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.0474      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0146     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0371     |
|    value_loss           | 0.0563      |
-----------------------------------------
Output 97: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 475         |
|    iterations           | 97          |
|    time_elapsed         | 418         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.017936131 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | -0.606      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00809    |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.00861     |
-----------------------------------------
Output 98: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 475         |
|    iterations           | 98          |
|    time_elapsed         | 421         |
|    total_timesteps      | 200704      |
| train/                  |             |
|    approx_kl            | 0.010565417 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0807     |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00811    |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 0.000865    |
-----------------------------------------
Output 99: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 99           |
|    time_elapsed         | 425          |
|    total_timesteps      | 202752       |
| train/                  |              |
|    approx_kl            | 0.0023714239 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0784      |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00636     |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.000252    |
|    value_loss           | 2.76e-07     |
------------------------------------------
Output 100: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 100          |
|    time_elapsed         | 428          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0007444598 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0843      |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00475      |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 8.66e-08     |
------------------------------------------
Output 101: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 478           |
|    iterations           | 101           |
|    time_elapsed         | 432           |
|    total_timesteps      | 206848        |
| train/                  |               |
|    approx_kl            | 0.00094139844 |
|    clip_fraction        | 0.0117        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0749       |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00274      |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000447     |
|    value_loss           | 3.04e-11      |
-------------------------------------------
Output 102: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 479         |
|    iterations           | 102         |
|    time_elapsed         | 436         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.001352916 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0769     |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000165   |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.00585     |
|    value_loss           | 4.59e-07    |
-----------------------------------------
Output 103: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 103          |
|    time_elapsed         | 439          |
|    total_timesteps      | 210944       |
| train/                  |              |
|    approx_kl            | 0.0047310484 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.087       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.005       |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00784     |
|    value_loss           | 7.87e-07     |
------------------------------------------
Output 104: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 480         |
|    iterations           | 104         |
|    time_elapsed         | 443         |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.008482906 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0935     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 1.28e-10    |
-----------------------------------------
Output 105: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 480        |
|    iterations           | 105        |
|    time_elapsed         | 447        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.07966387 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0553    |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0531    |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0348    |
|    value_loss           | 3.87e-05   |
----------------------------------------
Output 106: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 106          |
|    time_elapsed         | 451          |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 0.0015962933 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0297      |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00555      |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.000508    |
|    value_loss           | 1.29e-07     |
------------------------------------------
Output 107: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 107         |
|    time_elapsed         | 454         |
|    total_timesteps      | 219136      |
| train/                  |             |
|    approx_kl            | 0.047669947 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.044      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0429     |
|    n_updates            | 1060        |
|    policy_gradient_loss | 0.000455    |
|    value_loss           | 3.25e-10    |
-----------------------------------------
Output 108: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 108          |
|    time_elapsed         | 458          |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 0.0091649555 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.086       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0196      |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 2.52e-06     |
------------------------------------------
Output 109: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 109          |
|    time_elapsed         | 462          |
|    total_timesteps      | 223232       |
| train/                  |              |
|    approx_kl            | 0.0017429048 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0719      |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00486     |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00451     |
|    value_loss           | 2.21e-07     |
------------------------------------------
Output 110: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 483          |
|    iterations           | 110          |
|    time_elapsed         | 466          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.0047817044 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0875      |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0054      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00649     |
|    value_loss           | 3.49e-10     |
------------------------------------------
Output 111: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 111         |
|    time_elapsed         | 469         |
|    total_timesteps      | 227328      |
| train/                  |             |
|    approx_kl            | 0.070237555 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0527     |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0481     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 2.21e-05    |
-----------------------------------------
Output 112: Average over 100 episodes - Reward: 0.09
----------------------------------------
| time/                   |            |
|    fps                  | 484        |
|    iterations           | 112        |
|    time_elapsed         | 473        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.27668828 |
|    clip_fraction        | 0.0639     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0475    |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0373    |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 4.14e-07   |
----------------------------------------
Output 113: Average over 100 episodes - Reward: 0.64
----------------------------------------
| time/                   |            |
|    fps                  | 485        |
|    iterations           | 113        |
|    time_elapsed         | 477        |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.16493645 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.248     |
|    explained_variance   | -0.0128    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00746   |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.0733     |
----------------------------------------
Output 114: Average over 100 episodes - Reward: 0.93
----------------------------------------
| time/                   |            |
|    fps                  | 485        |
|    iterations           | 114        |
|    time_elapsed         | 480        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.07129785 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | -0.024     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0483     |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.165      |
----------------------------------------
Output 115: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 486         |
|    iterations           | 115         |
|    time_elapsed         | 484         |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.036115795 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | -0.237      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00566    |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.0423      |
-----------------------------------------
Output 116: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 486         |
|    iterations           | 116         |
|    time_elapsed         | 487         |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.020508397 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0577     |
|    explained_variance   | -2.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0256      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.000447   |
|    value_loss           | 0.000125    |
-----------------------------------------
Output 117: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 487         |
|    iterations           | 117         |
|    time_elapsed         | 491         |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.014928468 |
|    clip_fraction        | 0.00913     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0219     |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0206     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 0.00192     |
-----------------------------------------
Output 118: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 487        |
|    iterations           | 118        |
|    time_elapsed         | 495        |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.16267842 |
|    clip_fraction        | 0.0333     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0323    |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0655    |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 7.39e-07   |
----------------------------------------
Output 119: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 119         |
|    time_elapsed         | 498         |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.035688713 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.373      |
|    explained_variance   | 0.092       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0289     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0502     |
|    value_loss           | 0.00122     |
-----------------------------------------
Output 120: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 120         |
|    time_elapsed         | 502         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.012931242 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.268      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0386     |
|    value_loss           | 0.000531    |
-----------------------------------------
Output 121: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 489          |
|    iterations           | 121          |
|    time_elapsed         | 506          |
|    total_timesteps      | 247808       |
| train/                  |              |
|    approx_kl            | 0.0083519835 |
|    clip_fraction        | 0.27         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0464      |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.0468      |
|    value_loss           | 0.000169     |
------------------------------------------
Output 122: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 122         |
|    time_elapsed         | 510         |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.033867113 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0495     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0422     |
|    value_loss           | 0.000103    |
-----------------------------------------
Output 123: Average over 100 episodes - Reward: 0.75
----------------------------------------
| time/                   |            |
|    fps                  | 489        |
|    iterations           | 123        |
|    time_elapsed         | 514        |
|    total_timesteps      | 251904     |
| train/                  |            |
|    approx_kl            | 0.13825582 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0754    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0477    |
|    value_loss           | 2.8e-05    |
----------------------------------------
Output 124: Average over 100 episodes - Reward: 0.96
----------------------------------------
| time/                   |            |
|    fps                  | 490        |
|    iterations           | 124        |
|    time_elapsed         | 517        |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.05686716 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.0105     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0324     |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.11       |
----------------------------------------
Output 125: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 490         |
|    iterations           | 125         |
|    time_elapsed         | 521         |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.026103962 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | -0.127      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 0.0172      |
-----------------------------------------
Output 126: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 491          |
|    iterations           | 126          |
|    time_elapsed         | 525          |
|    total_timesteps      | 258048       |
| train/                  |              |
|    approx_kl            | 0.0056073964 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.238       |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.027       |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.0157      |
|    value_loss           | 9.94e-05     |
------------------------------------------
Output 127: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 491         |
|    iterations           | 127         |
|    time_elapsed         | 529         |
|    total_timesteps      | 260096      |
| train/                  |             |
|    approx_kl            | 0.038914233 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0457     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 6.65e-05    |
-----------------------------------------
Output 128: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 491        |
|    iterations           | 128        |
|    time_elapsed         | 532        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.08222622 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0991    |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0278    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 1.91e-05   |
----------------------------------------
Output 129: Average over 100 episodes - Reward: 0.77
-----------------------------------------
| time/                   |             |
|    fps                  | 492         |
|    iterations           | 129         |
|    time_elapsed         | 536         |
|    total_timesteps      | 264192      |
| train/                  |             |
|    approx_kl            | 0.006312862 |
|    clip_fraction        | 0.079       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0303     |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 1.01e-06    |
-----------------------------------------
Output 130: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 493         |
|    iterations           | 130         |
|    time_elapsed         | 539         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.055939585 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.0444      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0485     |
|    value_loss           | 0.0519      |
-----------------------------------------
Output 131: Average over 100 episodes - Reward: 0.87
-----------------------------------------
| time/                   |             |
|    fps                  | 493         |
|    iterations           | 131         |
|    time_elapsed         | 543         |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.020254336 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | -0.9        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00144     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.00451     |
-----------------------------------------
Output 132: Average over 100 episodes - Reward: 0.97
-----------------------------------------
| time/                   |             |
|    fps                  | 493         |
|    iterations           | 132         |
|    time_elapsed         | 547         |
|    total_timesteps      | 270336      |
| train/                  |             |
|    approx_kl            | 0.044105142 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.0213      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0279     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.044      |
|    value_loss           | 0.0606      |
-----------------------------------------
Output 133: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 494         |
|    iterations           | 133         |
|    time_elapsed         | 551         |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.044174165 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0794     |
|    explained_variance   | -0.726      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0743     |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.0114      |
-----------------------------------------
Output 134: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 494        |
|    iterations           | 134        |
|    time_elapsed         | 554        |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.04088348 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | -0.563     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0191    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 7.6e-05    |
----------------------------------------
Output 135: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 495        |
|    iterations           | 135        |
|    time_elapsed         | 558        |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.04275366 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.186     |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0361    |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 9e-06      |
----------------------------------------
Output 136: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 136         |
|    time_elapsed         | 562         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.011869425 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0499     |
|    value_loss           | 9.89e-05    |
-----------------------------------------
Output 137: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 137         |
|    time_elapsed         | 566         |
|    total_timesteps      | 280576      |
| train/                  |             |
|    approx_kl            | 0.009195291 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.174      |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0477     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0449     |
|    value_loss           | 6.24e-05    |
-----------------------------------------
Output 138: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 138         |
|    time_elapsed         | 569         |
|    total_timesteps      | 282624      |
| train/                  |             |
|    approx_kl            | 0.024919853 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.097      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.068      |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.037      |
|    value_loss           | 8.24e-06    |
-----------------------------------------
Output 139: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 496         |
|    iterations           | 139         |
|    time_elapsed         | 573         |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.009173914 |
|    clip_fraction        | 0.0367      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0693     |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00264    |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00832    |
|    value_loss           | 1.35e-06    |
-----------------------------------------
Output 140: Average over 100 episodes - Reward: 0.24
---------------------------------------
| time/                   |           |
|    fps                  | 496       |
|    iterations           | 140       |
|    time_elapsed         | 577       |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.1379154 |
|    clip_fraction        | 0.0795    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0692   |
|    explained_variance   | 1         |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0327   |
|    n_updates            | 1390      |
|    policy_gradient_loss | -0.0142   |
|    value_loss           | 2.99e-09  |
---------------------------------------
Output 141: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 497        |
|    iterations           | 141        |
|    time_elapsed         | 580        |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.42240155 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.0031     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0206     |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.15       |
----------------------------------------
Output 142: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 497        |
|    iterations           | 142        |
|    time_elapsed         | 584        |
|    total_timesteps      | 290816     |
| train/                  |            |
|    approx_kl            | 0.17623109 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | -1.81      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0554    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.0323     |
----------------------------------------
Output 143: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 498        |
|    iterations           | 143        |
|    time_elapsed         | 587        |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.04505092 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.42      |
|    explained_variance   | -0.799     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0638    |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0572    |
|    value_loss           | 0.00288    |
----------------------------------------
Output 144: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 498         |
|    iterations           | 144         |
|    time_elapsed         | 591         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.018862491 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0544     |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0526     |
|    value_loss           | 0.000628    |
-----------------------------------------
Output 145: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 498         |
|    iterations           | 145         |
|    time_elapsed         | 595         |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.008739142 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.262      |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0541     |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0463     |
|    value_loss           | 0.000206    |
-----------------------------------------
Output 146: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 499         |
|    iterations           | 146         |
|    time_elapsed         | 598         |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.044158593 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0415     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0417     |
|    value_loss           | 8.38e-05    |
-----------------------------------------
Output 147: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 499         |
|    iterations           | 147         |
|    time_elapsed         | 602         |
|    total_timesteps      | 301056      |
| train/                  |             |
|    approx_kl            | 0.069751106 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0657     |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0282     |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 1.82e-05    |
-----------------------------------------
Output 148: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 500          |
|    iterations           | 148          |
|    time_elapsed         | 606          |
|    total_timesteps      | 303104       |
| train/                  |              |
|    approx_kl            | 0.0016602727 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0479      |
|    explained_variance   | -0.11        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00112      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00346     |
|    value_loss           | 0.00219      |
------------------------------------------
Output 149: Average over 100 episodes - Reward: 0.45
-----------------------------------------
| time/                   |             |
|    fps                  | 500         |
|    iterations           | 149         |
|    time_elapsed         | 609         |
|    total_timesteps      | 305152      |
| train/                  |             |
|    approx_kl            | 0.037578475 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0785     |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 1.74e-06    |
-----------------------------------------
Output 150: Average over 100 episodes - Reward: 0.96
----------------------------------------
| time/                   |            |
|    fps                  | 500        |
|    iterations           | 150        |
|    time_elapsed         | 613        |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.19892657 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.144     |
|    explained_variance   | 0.00485    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.029      |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.179      |
----------------------------------------
Output 151: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 501       |
|    iterations           | 151       |
|    time_elapsed         | 616       |
|    total_timesteps      | 309248    |
| train/                  |           |
|    approx_kl            | 0.1480828 |
|    clip_fraction        | 0.0927    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.16     |
|    explained_variance   | -0.837    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0168   |
|    n_updates            | 1500      |
|    policy_gradient_loss | 0.0163    |
|    value_loss           | 0.0391    |
---------------------------------------
Output 152: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 501          |
|    iterations           | 152          |
|    time_elapsed         | 620          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0021774753 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.259       |
|    explained_variance   | -2.76        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0131      |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.000269     |
------------------------------------------
Output 153: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 153        |
|    time_elapsed         | 623        |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.01805758 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.22      |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00688   |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0273    |
|    value_loss           | 0.00163    |
----------------------------------------
Output 154: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 502          |
|    iterations           | 154          |
|    time_elapsed         | 627          |
|    total_timesteps      | 315392       |
| train/                  |              |
|    approx_kl            | 0.0032090722 |
|    clip_fraction        | 0.0557       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00542     |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.00317      |
------------------------------------------
Output 155: Average over 100 episodes - Reward: 0.83
----------------------------------------
| time/                   |            |
|    fps                  | 503        |
|    iterations           | 155        |
|    time_elapsed         | 630        |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.04382356 |
|    clip_fraction        | 0.0944     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.106      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.101     |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0199    |
|    value_loss           | 0.00317    |
----------------------------------------
Output 156: Average over 100 episodes - Reward: 0.96
--------------------------------------
| time/                   |          |
|    fps                  | 503      |
|    iterations           | 156      |
|    time_elapsed         | 634      |
|    total_timesteps      | 319488   |
| train/                  |          |
|    approx_kl            | 0.067389 |
|    clip_fraction        | 0.182    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.158   |
|    explained_variance   | 0.00867  |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0188   |
|    n_updates            | 1550     |
|    policy_gradient_loss | -0.0318  |
|    value_loss           | 0.121    |
--------------------------------------
Output 157: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 503         |
|    iterations           | 157         |
|    time_elapsed         | 638         |
|    total_timesteps      | 321536      |
| train/                  |             |
|    approx_kl            | 0.029381398 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | -0.262      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0339     |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 0.0141      |
-----------------------------------------
Output 158: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 504          |
|    iterations           | 158          |
|    time_elapsed         | 641          |
|    total_timesteps      | 323584       |
| train/                  |              |
|    approx_kl            | 0.0038442463 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.283       |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00546     |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 8.44e-05     |
------------------------------------------
Output 159: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 504       |
|    iterations           | 159       |
|    time_elapsed         | 645       |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 0.0665988 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.188    |
|    explained_variance   | 0.0977    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0199   |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.0278   |
|    value_loss           | 0.0035    |
---------------------------------------
Output 160: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 504         |
|    iterations           | 160         |
|    time_elapsed         | 648         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.060359158 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0952     |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0495     |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 0.00192     |
-----------------------------------------
Output 161: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 505          |
|    iterations           | 161          |
|    time_elapsed         | 652          |
|    total_timesteps      | 329728       |
| train/                  |              |
|    approx_kl            | 0.0007680996 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0729      |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0207       |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 3.38e-06     |
------------------------------------------
Output 162: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 505          |
|    iterations           | 162          |
|    time_elapsed         | 655          |
|    total_timesteps      | 331776       |
| train/                  |              |
|    approx_kl            | 0.0008601211 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0697      |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00192     |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 3e-06        |
------------------------------------------
Output 163: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 506         |
|    iterations           | 163         |
|    time_elapsed         | 659         |
|    total_timesteps      | 333824      |
| train/                  |             |
|    approx_kl            | 0.018945277 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0304     |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 1.15e-06    |
-----------------------------------------
Output 164: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 505           |
|    iterations           | 164           |
|    time_elapsed         | 664           |
|    total_timesteps      | 335872        |
| train/                  |               |
|    approx_kl            | 0.00012755665 |
|    clip_fraction        | 0.0021        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0148       |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000183      |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.00197      |
|    value_loss           | 0.00192       |
-------------------------------------------
Output 165: Average over 100 episodes - Reward: 0.04
----------------------------------------
| time/                   |            |
|    fps                  | 505        |
|    iterations           | 165        |
|    time_elapsed         | 667        |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.46962216 |
|    clip_fraction        | 0.0724     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0484    |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0616    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 6.6e-07    |
----------------------------------------
Output 166: Average over 100 episodes - Reward: 0.49
----------------------------------------
| time/                   |            |
|    fps                  | 506        |
|    iterations           | 166        |
|    time_elapsed         | 671        |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.13352671 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.272     |
|    explained_variance   | -0.0316    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0333    |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0333    |
|    value_loss           | 0.0553     |
----------------------------------------
Output 167: Average over 100 episodes - Reward: 0.92
----------------------------------------
| time/                   |            |
|    fps                  | 506        |
|    iterations           | 167        |
|    time_elapsed         | 675        |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.09713223 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.181     |
|    explained_variance   | 0.0556     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0191     |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.177      |
----------------------------------------
Output 168: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 506         |
|    iterations           | 168         |
|    time_elapsed         | 678         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.034161568 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | -0.321      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00393     |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.0502      |
-----------------------------------------
Output 169: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 507         |
|    iterations           | 169         |
|    time_elapsed         | 682         |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.004047215 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | -0.526      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00119     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 0.00177     |
-----------------------------------------
Output 170: Average over 100 episodes - Reward: 0.43
-----------------------------------------
| time/                   |             |
|    fps                  | 507         |
|    iterations           | 170         |
|    time_elapsed         | 686         |
|    total_timesteps      | 348160      |
| train/                  |             |
|    approx_kl            | 0.053490937 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00808    |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 4.44e-06    |
-----------------------------------------
Output 171: Average over 100 episodes - Reward: 0.84
----------------------------------------
| time/                   |            |
|    fps                  | 507        |
|    iterations           | 171        |
|    time_elapsed         | 689        |
|    total_timesteps      | 350208     |
| train/                  |            |
|    approx_kl            | 0.07950724 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.187     |
|    explained_variance   | 0.00636    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0387     |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.184      |
----------------------------------------
Output 172: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 508        |
|    iterations           | 172        |
|    time_elapsed         | 693        |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.07428414 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | -0.0446    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0099     |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.109      |
----------------------------------------
Output 173: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 508         |
|    iterations           | 173         |
|    time_elapsed         | 696         |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.087308735 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | -1.56       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0489     |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0599     |
|    value_loss           | 0.00301     |
-----------------------------------------
Output 174: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 508         |
|    iterations           | 174         |
|    time_elapsed         | 700         |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.022692215 |
|    clip_fraction        | 0.381       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0499     |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.051      |
|    value_loss           | 0.000264    |
-----------------------------------------
Output 175: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 509        |
|    iterations           | 175        |
|    time_elapsed         | 704        |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.01457241 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0358    |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.000145   |
----------------------------------------
Output 176: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 509         |
|    iterations           | 176         |
|    time_elapsed         | 707         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.024231987 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.152       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0535     |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.00192     |
-----------------------------------------
Output 177: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 509        |
|    iterations           | 177        |
|    time_elapsed         | 711        |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.07179434 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0753    |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0278    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0215    |
|    value_loss           | 2.97e-05   |
----------------------------------------
Output 178: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 509        |
|    iterations           | 178        |
|    time_elapsed         | 715        |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.03737089 |
|    clip_fraction        | 0.0977     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.0558     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0093     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 0.0057     |
----------------------------------------
Output 179: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 510         |
|    iterations           | 179         |
|    time_elapsed         | 718         |
|    total_timesteps      | 366592      |
| train/                  |             |
|    approx_kl            | 0.048015833 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0666     |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.0485     |
|    value_loss           | 1.26e-05    |
-----------------------------------------
Output 180: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 510         |
|    iterations           | 180         |
|    time_elapsed         | 722         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.011211102 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.247      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0419     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0489     |
|    value_loss           | 0.00013     |
-----------------------------------------
Output 181: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 510        |
|    iterations           | 181        |
|    time_elapsed         | 725        |
|    total_timesteps      | 370688     |
| train/                  |            |
|    approx_kl            | 0.08549966 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0505    |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 9.6e-05    |
----------------------------------------
Output 182: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 511        |
|    iterations           | 182        |
|    time_elapsed         | 729        |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.08492035 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00294   |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0266    |
|    value_loss           | 1.15e-05   |
----------------------------------------
Output 183: Average over 100 episodes - Reward: 0.88
-----------------------------------------
| time/                   |             |
|    fps                  | 511         |
|    iterations           | 183         |
|    time_elapsed         | 732         |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.010095494 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0307     |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 2.96e-07    |
-----------------------------------------
Output 184: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 511         |
|    iterations           | 184         |
|    time_elapsed         | 736         |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.031513818 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.0245      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00755    |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0366     |
|    value_loss           | 0.0424      |
-----------------------------------------
Output 185: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 511        |
|    iterations           | 185        |
|    time_elapsed         | 740        |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.06870682 |
|    clip_fraction        | 0.0857     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | -0.237     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00127    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.0063     |
----------------------------------------
Output 186: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 512        |
|    iterations           | 186        |
|    time_elapsed         | 743        |
|    total_timesteps      | 380928     |
| train/                  |            |
|    approx_kl            | 0.00271534 |
|    clip_fraction        | 0.019      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000111   |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.00351   |
|    value_loss           | 3.43e-05   |
----------------------------------------
Output 187: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 512         |
|    iterations           | 187         |
|    time_elapsed         | 747         |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.024892036 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0842     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00638    |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 4.3e-06     |
-----------------------------------------
Output 188: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 512         |
|    iterations           | 188         |
|    time_elapsed         | 751         |
|    total_timesteps      | 385024      |
| train/                  |             |
|    approx_kl            | 0.047911048 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.028      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00944    |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00615    |
|    value_loss           | 2.82e-09    |
-----------------------------------------
Output 189: Average over 100 episodes - Reward: 0.31
-----------------------------------------
| time/                   |             |
|    fps                  | 512         |
|    iterations           | 189         |
|    time_elapsed         | 754         |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.076947555 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0875     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0633     |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0538     |
|    value_loss           | 7.43e-13    |
-----------------------------------------
Output 190: Average over 100 episodes - Reward: 0.98
----------------------------------------
| time/                   |            |
|    fps                  | 513        |
|    iterations           | 190        |
|    time_elapsed         | 758        |
|    total_timesteps      | 389120     |
| train/                  |            |
|    approx_kl            | 0.43040076 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0974    |
|    explained_variance   | 0.00572    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0284     |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.171      |
----------------------------------------
Output 191: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 513        |
|    iterations           | 191        |
|    time_elapsed         | 761        |
|    total_timesteps      | 391168     |
| train/                  |            |
|    approx_kl            | 0.22116108 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.1       |
|    explained_variance   | -3.24      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0575    |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0373    |
|    value_loss           | 0.0191     |
----------------------------------------
Output 192: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 513        |
|    iterations           | 192        |
|    time_elapsed         | 765        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.06276562 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.417     |
|    explained_variance   | -0.85      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0695    |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.0698    |
|    value_loss           | 0.00138    |
----------------------------------------
Output 193: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 193         |
|    time_elapsed         | 769         |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.017435176 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.299      |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0455     |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0466     |
|    value_loss           | 0.000569    |
-----------------------------------------
Output 194: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 194         |
|    time_elapsed         | 772         |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.011248317 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0634     |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.0478     |
|    value_loss           | 0.000264    |
-----------------------------------------
Output 195: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 514        |
|    iterations           | 195        |
|    time_elapsed         | 776        |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.06417176 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0529    |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 9.31e-05   |
----------------------------------------
Output 196: Average over 100 episodes - Reward: 0.81
-----------------------------------------
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 196         |
|    time_elapsed         | 780         |
|    total_timesteps      | 401408      |
| train/                  |             |
|    approx_kl            | 0.091942504 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0588     |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00133     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 1.96e-05    |
-----------------------------------------
Output 197: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 197         |
|    time_elapsed         | 783         |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.052296177 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0274     |
|    explained_variance   | 0.0099      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0252      |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.0976      |
-----------------------------------------
Output 198: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 515        |
|    iterations           | 198        |
|    time_elapsed         | 787        |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.08494451 |
|    clip_fraction        | 0.0445     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0498    |
|    explained_variance   | -0.155     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00245    |
|    n_updates            | 1970       |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 0.0117     |
----------------------------------------
Output 199: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 199         |
|    time_elapsed         | 790         |
|    total_timesteps      | 407552      |
| train/                  |             |
|    approx_kl            | 0.021556333 |
|    clip_fraction        | 0.0672      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0699     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 5.24e-05    |
-----------------------------------------
Output 200: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 515        |
|    iterations           | 200        |
|    time_elapsed         | 794        |
|    total_timesteps      | 409600     |
| train/                  |            |
|    approx_kl            | 0.01801084 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.231     |
|    explained_variance   | 0.124      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0361    |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.002      |
----------------------------------------
Output 201: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 201         |
|    time_elapsed         | 797         |
|    total_timesteps      | 411648      |
| train/                  |             |
|    approx_kl            | 0.042603277 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.108       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000493   |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.00328     |
-----------------------------------------
Output 202: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 516          |
|    iterations           | 202          |
|    time_elapsed         | 801          |
|    total_timesteps      | 413696       |
| train/                  |              |
|    approx_kl            | 0.0021369457 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.082       |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0165      |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.01        |
|    value_loss           | 0.00162      |
------------------------------------------
Output 203: Average over 100 episodes - Reward: 0.88
------------------------------------------
| time/                   |              |
|    fps                  | 516          |
|    iterations           | 203          |
|    time_elapsed         | 805          |
|    total_timesteps      | 415744       |
| train/                  |              |
|    approx_kl            | 0.0008726318 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0707      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0248      |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.00925     |
|    value_loss           | 2.09e-05     |
------------------------------------------
Output 204: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 516         |
|    iterations           | 204         |
|    time_elapsed         | 808         |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.056843445 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0492     |
|    explained_variance   | 0.0113      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0182      |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0691      |
-----------------------------------------
Output 205: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 516         |
|    iterations           | 205         |
|    time_elapsed         | 812         |
|    total_timesteps      | 419840      |
| train/                  |             |
|    approx_kl            | 0.032714747 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | -1.06       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0633     |
|    n_updates            | 2040        |
|    policy_gradient_loss | 0.0152      |
|    value_loss           | 0.000196    |
-----------------------------------------
Output 206: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 206          |
|    time_elapsed         | 815          |
|    total_timesteps      | 421888       |
| train/                  |              |
|    approx_kl            | 0.0023409487 |
|    clip_fraction        | 0.0401       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.705        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0295      |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.0186      |
|    value_loss           | 5.12e-05     |
------------------------------------------
Output 207: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 207         |
|    time_elapsed         | 819         |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.061946306 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0358     |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0295     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 3.3e-05     |
-----------------------------------------
Output 208: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 208          |
|    time_elapsed         | 823          |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0065406384 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0115      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000959    |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00717     |
|    value_loss           | 5.79e-06     |
------------------------------------------
Output 209: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 209           |
|    time_elapsed         | 827           |
|    total_timesteps      | 428032        |
| train/                  |               |
|    approx_kl            | 0.00017625195 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00663      |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.017        |
|    n_updates            | 2080          |
|    policy_gradient_loss | -0.00147      |
|    value_loss           | 0.00192       |
-------------------------------------------
Output 210: Average over 100 episodes - Reward: 0.15
----------------------------------------
| time/                   |            |
|    fps                  | 517        |
|    iterations           | 210        |
|    time_elapsed         | 830        |
|    total_timesteps      | 430080     |
| train/                  |            |
|    approx_kl            | 0.17157266 |
|    clip_fraction        | 0.0749     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0404    |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0366    |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 2.15e-06   |
----------------------------------------
Output 211: Average over 100 episodes - Reward: 0.78
----------------------------------------
| time/                   |            |
|    fps                  | 517        |
|    iterations           | 211        |
|    time_elapsed         | 834        |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.15629897 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | -0.00213   |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00585    |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.037     |
|    value_loss           | 0.117      |
----------------------------------------
Output 212: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 518       |
|    iterations           | 212       |
|    time_elapsed         | 837       |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 0.2676063 |
|    clip_fraction        | 0.237     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0626   |
|    explained_variance   | -0.187    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00806   |
|    n_updates            | 2110      |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.119     |
---------------------------------------
Output 213: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 213         |
|    time_elapsed         | 841         |
|    total_timesteps      | 436224      |
| train/                  |             |
|    approx_kl            | 0.004740798 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | -7.3        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0219      |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.000302    |
|    value_loss           | 0.00231     |
-----------------------------------------
Output 214: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 214        |
|    time_elapsed         | 844        |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.01083165 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.245     |
|    explained_variance   | 0.189      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0427    |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.000268   |
----------------------------------------
Output 215: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 215         |
|    time_elapsed         | 848         |
|    total_timesteps      | 440320      |
| train/                  |             |
|    approx_kl            | 0.009102711 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.0923      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00174    |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.00383     |
-----------------------------------------
Output 216: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 216        |
|    time_elapsed         | 852        |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.08198865 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0703    |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.026     |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0384    |
|    value_loss           | 8.9e-05    |
----------------------------------------
Output 217: Average over 100 episodes - Reward: 0.77
----------------------------------------
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 217        |
|    time_elapsed         | 855        |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.06499056 |
|    clip_fraction        | 0.0759     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0538    |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0322    |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0207    |
|    value_loss           | 1.3e-05    |
----------------------------------------
Output 218: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 218        |
|    time_elapsed         | 859        |
|    total_timesteps      | 446464     |
| train/                  |            |
|    approx_kl            | 0.06747413 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0238    |
|    explained_variance   | 0.00955    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0264     |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 0.135      |
----------------------------------------
Output 219: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 219         |
|    time_elapsed         | 863         |
|    total_timesteps      | 448512      |
| train/                  |             |
|    approx_kl            | 0.010648869 |
|    clip_fraction        | 0.00498     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00539    |
|    explained_variance   | -0.264      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00643    |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00724    |
|    value_loss           | 0.0159      |
-----------------------------------------
Output 220: Average over 100 episodes - Reward: 0.2
-----------------------------------------
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 220         |
|    time_elapsed         | 866         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.094011664 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0161     |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 4.63e-05    |
-----------------------------------------
Output 221: Average over 100 episodes - Reward: 0.87
----------------------------------------
| time/                   |            |
|    fps                  | 520        |
|    iterations           | 221        |
|    time_elapsed         | 870        |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.19480726 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0896    |
|    explained_variance   | -0.00194   |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0298     |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.0378    |
|    value_loss           | 0.129      |
----------------------------------------
Output 222: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 520        |
|    iterations           | 222        |
|    time_elapsed         | 873        |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.38515094 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0469    |
|    explained_variance   | -0.247     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00896   |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0572    |
|    value_loss           | 0.107      |
----------------------------------------
Output 223: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 223         |
|    time_elapsed         | 877         |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.037235193 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | -4.25       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0252     |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.00325     |
-----------------------------------------
Output 224: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 224         |
|    time_elapsed         | 880         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.091179684 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0514     |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.0502     |
|    value_loss           | 0.000465    |
-----------------------------------------
Output 225: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 225        |
|    time_elapsed         | 884        |
|    total_timesteps      | 460800     |
| train/                  |            |
|    approx_kl            | 0.07821904 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0572    |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0529    |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.035     |
|    value_loss           | 6.45e-05   |
----------------------------------------
Output 226: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 226           |
|    time_elapsed         | 887           |
|    total_timesteps      | 462848        |
| train/                  |               |
|    approx_kl            | 0.00053619687 |
|    clip_fraction        | 0.0146        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0338       |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0109       |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.0108       |
|    value_loss           | 1.02e-05      |
-------------------------------------------
Output 227: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 227          |
|    time_elapsed         | 891          |
|    total_timesteps      | 464896       |
| train/                  |              |
|    approx_kl            | 0.0005041722 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0226      |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0104      |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00647     |
|    value_loss           | 5.38e-06     |
------------------------------------------
Output 228: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 228         |
|    time_elapsed         | 895         |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.011099211 |
|    clip_fraction        | 0.00571     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00877    |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00658    |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.00709    |
|    value_loss           | 2.6e-06     |
-----------------------------------------
Output 229: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 229          |
|    time_elapsed         | 898          |
|    total_timesteps      | 468992       |
| train/                  |              |
|    approx_kl            | 0.0033211415 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00467     |
|    explained_variance   | 0.037        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00395     |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.00359     |
|    value_loss           | 0.00947      |
------------------------------------------
Output 230: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 230        |
|    time_elapsed         | 902        |
|    total_timesteps      | 471040     |
| train/                  |            |
|    approx_kl            | 0.19315904 |
|    clip_fraction        | 0.0868     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0405    |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0627    |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0356    |
|    value_loss           | 8.05e-06   |
----------------------------------------
Output 231: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 231         |
|    time_elapsed         | 906         |
|    total_timesteps      | 473088      |
| train/                  |             |
|    approx_kl            | 0.049875185 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.367      |
|    explained_variance   | 0.0627      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0461     |
|    value_loss           | 0.0013      |
-----------------------------------------
Output 232: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 232         |
|    time_elapsed         | 909         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.017960068 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0693     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0437     |
|    value_loss           | 0.00076     |
-----------------------------------------
Output 233: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 233         |
|    time_elapsed         | 913         |
|    total_timesteps      | 477184      |
| train/                  |             |
|    approx_kl            | 0.009585342 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0376     |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.00211     |
-----------------------------------------
Output 234: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 522       |
|    iterations           | 234       |
|    time_elapsed         | 917       |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.0562812 |
|    clip_fraction        | 0.261     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.122    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0547   |
|    n_updates            | 2330      |
|    policy_gradient_loss | -0.0399   |
|    value_loss           | 0.0001    |
---------------------------------------
Output 235: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 522        |
|    iterations           | 235        |
|    time_elapsed         | 920        |
|    total_timesteps      | 481280     |
| train/                  |            |
|    approx_kl            | 0.06223934 |
|    clip_fraction        | 0.0263     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0185    |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0292    |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 1.76e-05   |
----------------------------------------
Output 236: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 236           |
|    time_elapsed         | 924           |
|    total_timesteps      | 483328        |
| train/                  |               |
|    approx_kl            | 0.00012403022 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00578      |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000126      |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.00196      |
|    value_loss           | 1.02e-06      |
-------------------------------------------
Output 237: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 237           |
|    time_elapsed         | 928           |
|    total_timesteps      | 485376        |
| train/                  |               |
|    approx_kl            | 3.8282655e-05 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00526      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.53e-06     |
|    n_updates            | 2360          |
|    policy_gradient_loss | -0.00176      |
|    value_loss           | 6.77e-07      |
-------------------------------------------
Output 238: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 238         |
|    time_elapsed         | 931         |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.000680205 |
|    clip_fraction        | 0.00132     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00273    |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00026    |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 0.00194     |
-----------------------------------------
Output 239: Average over 100 episodes - Reward: 1.0
-------------------------------------------
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 239           |
|    time_elapsed         | 935           |
|    total_timesteps      | 489472        |
| train/                  |               |
|    approx_kl            | 1.4285906e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0032       |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000443     |
|    n_updates            | 2380          |
|    policy_gradient_loss | -0.00015      |
|    value_loss           | 1.68e-06      |
-------------------------------------------
Output 240: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 522        |
|    iterations           | 240        |
|    time_elapsed         | 939        |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.02252782 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0831    |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0487    |
|    n_updates            | 2390       |
|    policy_gradient_loss | 0.172      |
|    value_loss           | 2.44e-07   |
----------------------------------------
Output 241: Average over 100 episodes - Reward: 1.0
--------------------------------------
| time/                   |          |
|    fps                  | 523      |
|    iterations           | 241      |
|    time_elapsed         | 943      |
|    total_timesteps      | 493568   |
| train/                  |          |
|    approx_kl            | 0.082475 |
|    clip_fraction        | 0.196    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0169  |
|    explained_variance   | 0.927    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0505  |
|    n_updates            | 2400     |
|    policy_gradient_loss | -0.0306  |
|    value_loss           | 2.35e-05 |
--------------------------------------
Output 242: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 242          |
|    time_elapsed         | 947          |
|    total_timesteps      | 495616       |
| train/                  |              |
|    approx_kl            | 0.0005344911 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00792     |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000534     |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00379     |
|    value_loss           | 1.9e-06      |
------------------------------------------
Output 243: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 523       |
|    iterations           | 243       |
|    time_elapsed         | 951       |
|    total_timesteps      | 497664    |
| train/                  |           |
|    approx_kl            | 0.0336619 |
|    clip_fraction        | 0.11      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0716   |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0569   |
|    n_updates            | 2420      |
|    policy_gradient_loss | 0.218     |
|    value_loss           | 4.5e-07   |
---------------------------------------
Output 244: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 523        |
|    iterations           | 244        |
|    time_elapsed         | 954        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.10573519 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0165    |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0526    |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.0342    |
|    value_loss           | 3.13e-05   |
----------------------------------------
Output 245: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 245         |
|    time_elapsed         | 958         |
|    total_timesteps      | 501760      |
| train/                  |             |
|    approx_kl            | 0.006045489 |
|    clip_fraction        | 0.00288     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00265    |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00315    |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 1.64e-06    |
-----------------------------------------
Overall: Average Reward: 0.9380408163265306
