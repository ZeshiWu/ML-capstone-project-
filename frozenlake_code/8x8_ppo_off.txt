8x8 frozen lake map PPO - is_slippery off
Using cpu device
Logging to ./PPOtensorboard/PPO_10
Output 1: Average over 66 episodes - Reward: 0.0
-----------------------------
| time/              |      |
|    fps             | 1122 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 2048 |
-----------------------------
Output 2: Average over 63 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 821         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.006044142 |
|    clip_fraction        | 0.0199      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -58.9       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00559    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0073     |
|    value_loss           | 0.00684     |
-----------------------------------------
Output 3: Average over 83 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 745        |
|    iterations           | 3          |
|    time_elapsed         | 8          |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.01052245 |
|    clip_fraction        | 0.0512     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -14.8      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0141    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.000252   |
----------------------------------------
Output 4: Average over 100 episodes - Reward: 0.0
-----------------------------------------
| time/                   |             |
|    fps                  | 709         |
|    iterations           | 4           |
|    time_elapsed         | 11          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.020178944 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | -11.8       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0347     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 4.46e-05    |
-----------------------------------------
Output 5: Average over 100 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 691        |
|    iterations           | 5          |
|    time_elapsed         | 14         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.02010863 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | -4.94      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0316    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.022     |
|    value_loss           | 1.52e-06   |
----------------------------------------
Output 6: Average over 100 episodes - Reward: 0.0
----------------------------------------
| time/                   |            |
|    fps                  | 681        |
|    iterations           | 6          |
|    time_elapsed         | 18         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.00929793 |
|    clip_fraction        | 0.0681     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | -40        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0198    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 1.05e-05   |
----------------------------------------
Output 7: Average over 100 episodes - Reward: 0.05
-----------------------------------------
| time/                   |             |
|    fps                  | 672         |
|    iterations           | 7           |
|    time_elapsed         | 21          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011280677 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.0213      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0279      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.00173     |
-----------------------------------------
Output 8: Average over 100 episodes - Reward: 0.15
-----------------------------------------
| time/                   |             |
|    fps                  | 667         |
|    iterations           | 8           |
|    time_elapsed         | 24          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010722017 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.0143      |
-----------------------------------------
Output 9: Average over 100 episodes - Reward: 0.29
-----------------------------------------
| time/                   |             |
|    fps                  | 663         |
|    iterations           | 9           |
|    time_elapsed         | 27          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.015204789 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00441     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 0.0412      |
-----------------------------------------
Output 10: Average over 97 episodes - Reward: 0.5567010309278351
----------------------------------------
| time/                   |            |
|    fps                  | 660        |
|    iterations           | 10         |
|    time_elapsed         | 31         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.01741536 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.441      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0116     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0343    |
|    value_loss           | 0.0506     |
----------------------------------------
Output 11: Average over 99 episodes - Reward: 0.7676767676767676
-----------------------------------------
| time/                   |             |
|    fps                  | 657         |
|    iterations           | 11          |
|    time_elapsed         | 34          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.016685743 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0168      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.0521      |
-----------------------------------------
Output 12: Average over 100 episodes - Reward: 0.91
-----------------------------------------
| time/                   |             |
|    fps                  | 655         |
|    iterations           | 12          |
|    time_elapsed         | 37          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.018636761 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0324     |
|    value_loss           | 0.0349      |
-----------------------------------------
Output 13: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 654         |
|    iterations           | 13          |
|    time_elapsed         | 40          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.016968064 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.178       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.0232      |
-----------------------------------------
Output 14: Average over 100 episodes - Reward: 0.97
-----------------------------------------
| time/                   |             |
|    fps                  | 653         |
|    iterations           | 14          |
|    time_elapsed         | 43          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.018439129 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0275     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.00392     |
-----------------------------------------
Output 15: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 652         |
|    iterations           | 15          |
|    time_elapsed         | 47          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.005640595 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00191     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00862    |
|    value_loss           | 0.014       |
-----------------------------------------
Output 16: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 651         |
|    iterations           | 16          |
|    time_elapsed         | 50          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.021781374 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.00437     |
-----------------------------------------
Output 17: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 649         |
|    iterations           | 17          |
|    time_elapsed         | 53          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.004223869 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.335      |
|    explained_variance   | 0.126       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0225     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00814    |
|    value_loss           | 0.0106      |
-----------------------------------------
Output 18: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 647        |
|    iterations           | 18         |
|    time_elapsed         | 56         |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.01964724 |
|    clip_fraction        | 0.0828     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0206     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.00673    |
----------------------------------------
Output 19: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 645       |
|    iterations           | 19        |
|    time_elapsed         | 60        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.0025993 |
|    clip_fraction        | 0.0329    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.211    |
|    explained_variance   | 0.32      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00197   |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.00432  |
|    value_loss           | 0.00466   |
---------------------------------------
Output 20: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 643          |
|    iterations           | 20           |
|    time_elapsed         | 63           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0074000983 |
|    clip_fraction        | 0.056        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0131      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00591     |
|    value_loss           | 0.00337      |
------------------------------------------
Output 21: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 643         |
|    iterations           | 21          |
|    time_elapsed         | 66          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.021860879 |
|    clip_fraction        | 0.0648      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0284     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.000912    |
-----------------------------------------
Output 22: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 643          |
|    iterations           | 22           |
|    time_elapsed         | 70           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0035960386 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | 0.735        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00764     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00727     |
|    value_loss           | 0.00129      |
------------------------------------------
Output 23: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 642         |
|    iterations           | 23          |
|    time_elapsed         | 73          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.017487247 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0078     |
|    value_loss           | 4.51e-05    |
-----------------------------------------
Output 24: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 642         |
|    iterations           | 24          |
|    time_elapsed         | 76          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008658483 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0228     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00712    |
|    value_loss           | 0.00478     |
-----------------------------------------
Output 25: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 640         |
|    iterations           | 25          |
|    time_elapsed         | 79          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.008308111 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00174    |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00357    |
|    value_loss           | 7.03e-06    |
-----------------------------------------
Output 26: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 640         |
|    iterations           | 26          |
|    time_elapsed         | 83          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.010199228 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | -0.393      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00872    |
|    value_loss           | 0.000458    |
-----------------------------------------
Output 27: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 640         |
|    iterations           | 27          |
|    time_elapsed         | 86          |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.006220735 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00489     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 8.94e-06    |
-----------------------------------------
Output 28: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 639         |
|    iterations           | 28          |
|    time_elapsed         | 89          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.008631999 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00287     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 2.12e-06    |
-----------------------------------------
Output 29: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 639         |
|    iterations           | 29          |
|    time_elapsed         | 92          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.007676951 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.223      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00637    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00451    |
|    value_loss           | 2.72e-06    |
-----------------------------------------
Output 30: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 638         |
|    iterations           | 30          |
|    time_elapsed         | 96          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.005912216 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00322    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 1.78e-06    |
-----------------------------------------
Output 31: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 637         |
|    iterations           | 31          |
|    time_elapsed         | 99          |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.015208446 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.234      |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0307     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 0.000267    |
-----------------------------------------
Output 32: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 637         |
|    iterations           | 32          |
|    time_elapsed         | 102         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.020784054 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.221      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.017      |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 1.33e-06    |
-----------------------------------------
Output 33: Average over 100 episodes - Reward: 0.97
------------------------------------------
| time/                   |              |
|    fps                  | 637          |
|    iterations           | 33           |
|    time_elapsed         | 106          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0100774625 |
|    clip_fraction        | 0.0754       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00601     |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00629     |
|    value_loss           | 1e-06        |
------------------------------------------
Output 34: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 636         |
|    iterations           | 34          |
|    time_elapsed         | 109         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.007767453 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.247      |
|    explained_variance   | 0.199       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00162     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00967    |
|    value_loss           | 0.0122      |
-----------------------------------------
Output 35: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 636        |
|    iterations           | 35         |
|    time_elapsed         | 112        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.02042374 |
|    clip_fraction        | 0.0977     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.203     |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00181    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0067    |
|    value_loss           | 4.73e-05   |
----------------------------------------
Output 36: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 636          |
|    iterations           | 36           |
|    time_elapsed         | 115          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0034842214 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.2         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000963     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 1.83e-06     |
------------------------------------------
Output 37: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 635          |
|    iterations           | 37           |
|    time_elapsed         | 119          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0035356716 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00695      |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00316     |
|    value_loss           | 1.66e-06     |
------------------------------------------
Output 38: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 635         |
|    iterations           | 38          |
|    time_elapsed         | 122         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.015597098 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 9.4e-07     |
-----------------------------------------
Output 39: Average over 100 episodes - Reward: 0.96
-----------------------------------------
| time/                   |             |
|    fps                  | 634         |
|    iterations           | 39          |
|    time_elapsed         | 125         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.007140465 |
|    clip_fraction        | 0.0411      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000838   |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 1.03e-06    |
-----------------------------------------
Output 40: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 634          |
|    iterations           | 40           |
|    time_elapsed         | 129          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0047314093 |
|    clip_fraction        | 0.0571       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0.237        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00626      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 0.0107       |
------------------------------------------
Output 41: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 634         |
|    iterations           | 41          |
|    time_elapsed         | 132         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.014603989 |
|    clip_fraction        | 0.0472      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0447      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.000355    |
-----------------------------------------
Output 42: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 634         |
|    iterations           | 42          |
|    time_elapsed         | 135         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.007660769 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0101     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 4.8e-06     |
-----------------------------------------
Output 43: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 634         |
|    iterations           | 43          |
|    time_elapsed         | 138         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.020111684 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00786    |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 2.55e-05    |
-----------------------------------------
Output 44: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 633         |
|    iterations           | 44          |
|    time_elapsed         | 142         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.019707493 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 1.86e-05    |
-----------------------------------------
Output 45: Average over 100 episodes - Reward: 0.96
-----------------------------------------
| time/                   |             |
|    fps                  | 633         |
|    iterations           | 45          |
|    time_elapsed         | 145         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.020352392 |
|    clip_fraction        | 0.0849      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00878    |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.34e-06    |
-----------------------------------------
Output 46: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 632         |
|    iterations           | 46          |
|    time_elapsed         | 148         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.012783332 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.131       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00444    |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00871    |
|    value_loss           | 0.0113      |
-----------------------------------------
Output 47: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 632        |
|    iterations           | 47         |
|    time_elapsed         | 152        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.01726351 |
|    clip_fraction        | 0.0585     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.217     |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.024     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00646   |
|    value_loss           | 2.63e-05   |
----------------------------------------
Output 48: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 632         |
|    iterations           | 48          |
|    time_elapsed         | 155         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.006487972 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00303     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 4.15e-06    |
-----------------------------------------
Output 49: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 631         |
|    iterations           | 49          |
|    time_elapsed         | 158         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.003638769 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00304     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0051     |
|    value_loss           | 3.16e-06    |
-----------------------------------------
Output 50: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 632          |
|    iterations           | 50           |
|    time_elapsed         | 162          |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0052190265 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.406        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00517      |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00595     |
|    value_loss           | 0.00243      |
------------------------------------------
Output 51: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 631         |
|    iterations           | 51          |
|    time_elapsed         | 165         |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.008047351 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0134     |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00994    |
|    value_loss           | 0.00299     |
-----------------------------------------
Output 52: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 631         |
|    iterations           | 52          |
|    time_elapsed         | 168         |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.012332376 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0345     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 9.92e-06    |
-----------------------------------------
Output 53: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 631         |
|    iterations           | 53          |
|    time_elapsed         | 171         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.011181392 |
|    clip_fraction        | 0.0739      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00183    |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0083     |
|    value_loss           | 9.23e-07    |
-----------------------------------------
Output 54: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 630        |
|    iterations           | 54         |
|    time_elapsed         | 175        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.02854899 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.192     |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.02      |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0229    |
|    value_loss           | 6.06e-05   |
----------------------------------------
Output 55: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 630         |
|    iterations           | 55          |
|    time_elapsed         | 178         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.010869084 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.018      |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 8e-06       |
-----------------------------------------
Output 56: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 630          |
|    iterations           | 56           |
|    time_elapsed         | 181          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0042408183 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00606      |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00556     |
|    value_loss           | 3.32e-06     |
------------------------------------------
Output 57: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 630        |
|    iterations           | 57         |
|    time_elapsed         | 185        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.00647962 |
|    clip_fraction        | 0.0407     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.146     |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000279  |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.00508   |
|    value_loss           | 6.41e-06   |
----------------------------------------
Output 58: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 630         |
|    iterations           | 58          |
|    time_elapsed         | 188         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.003985335 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0057     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00338    |
|    value_loss           | 6.38e-07    |
-----------------------------------------
Output 59: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 630         |
|    iterations           | 59          |
|    time_elapsed         | 191         |
|    total_timesteps      | 120832      |
| train/                  |             |
|    approx_kl            | 0.009548348 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.017       |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00703    |
|    value_loss           | 9.59e-07    |
-----------------------------------------
Output 60: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 630         |
|    iterations           | 60          |
|    time_elapsed         | 195         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.003754215 |
|    clip_fraction        | 0.0561      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00263     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 9.68e-07    |
-----------------------------------------
Output 61: Average over 100 episodes - Reward: 0.98
------------------------------------------
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 61           |
|    time_elapsed         | 198          |
|    total_timesteps      | 124928       |
| train/                  |              |
|    approx_kl            | 0.0047538546 |
|    clip_fraction        | 0.0468       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.138       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0211      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00838     |
|    value_loss           | 3.75e-06     |
------------------------------------------
Output 62: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 62           |
|    time_elapsed         | 201          |
|    total_timesteps      | 126976       |
| train/                  |              |
|    approx_kl            | 0.0016242198 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00992     |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 0.00427      |
------------------------------------------
Output 63: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 63          |
|    time_elapsed         | 204         |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.018207148 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | -0.196      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00829    |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00889    |
|    value_loss           | 0.000151    |
-----------------------------------------
Output 64: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 64          |
|    time_elapsed         | 208         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.003830106 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00848    |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00368    |
|    value_loss           | 1.46e-06    |
-----------------------------------------
Output 65: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 65           |
|    time_elapsed         | 211          |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.0039985245 |
|    clip_fraction        | 0.063        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00137     |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 1.65e-06     |
------------------------------------------
Output 66: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 66          |
|    time_elapsed         | 214         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.010159313 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00367     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 0.00308     |
-----------------------------------------
Output 67: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 67          |
|    time_elapsed         | 217         |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.012787975 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0127     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 0.00301     |
-----------------------------------------
Output 68: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 68          |
|    time_elapsed         | 221         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.017299969 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 3.21e-05    |
-----------------------------------------
Output 69: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 69          |
|    time_elapsed         | 224         |
|    total_timesteps      | 141312      |
| train/                  |             |
|    approx_kl            | 0.005522758 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00394     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00762    |
|    value_loss           | 9.66e-06    |
-----------------------------------------
Output 70: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 70           |
|    time_elapsed         | 227          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0056260955 |
|    clip_fraction        | 0.0829       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00817     |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00828     |
|    value_loss           | 4.04e-06     |
------------------------------------------
Output 71: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 629        |
|    iterations           | 71         |
|    time_elapsed         | 230        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.02106181 |
|    clip_fraction        | 0.0824     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.205     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00539   |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.00804   |
|    value_loss           | 3.38e-06   |
----------------------------------------
Output 72: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 72          |
|    time_elapsed         | 234         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.006749413 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00366    |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 7.57e-07    |
-----------------------------------------
Output 73: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 73          |
|    time_elapsed         | 237         |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.016271126 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0111     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 4e-08       |
-----------------------------------------
Output 74: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 74          |
|    time_elapsed         | 240         |
|    total_timesteps      | 151552      |
| train/                  |             |
|    approx_kl            | 0.046530414 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.226      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0374     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 5.67e-05    |
-----------------------------------------
Output 75: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 75          |
|    time_elapsed         | 244         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.017686488 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00772    |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 7.39e-06    |
-----------------------------------------
Output 76: Average over 100 episodes - Reward: 1.0
--------------------------------------
| time/                   |          |
|    fps                  | 629      |
|    iterations           | 76       |
|    time_elapsed         | 247      |
|    total_timesteps      | 155648   |
| train/                  |          |
|    approx_kl            | 0.011896 |
|    clip_fraction        | 0.0335   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.197   |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.00423 |
|    n_updates            | 750      |
|    policy_gradient_loss | -0.00436 |
|    value_loss           | 1.14e-06 |
--------------------------------------
Output 77: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 77          |
|    time_elapsed         | 250         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.014850796 |
|    clip_fraction        | 0.0666      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00664    |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 1.12e-06    |
-----------------------------------------
Output 78: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 78          |
|    time_elapsed         | 253         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.022910144 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.197      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0383     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 3.54e-05    |
-----------------------------------------
Output 79: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 629        |
|    iterations           | 79         |
|    time_elapsed         | 257        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.01181376 |
|    clip_fraction        | 0.036      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.172     |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00927   |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.00699   |
|    value_loss           | 5.37e-06   |
----------------------------------------
Output 80: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 80          |
|    time_elapsed         | 260         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.009765888 |
|    clip_fraction        | 0.0421      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000572   |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 6.11e-07    |
-----------------------------------------
Output 81: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 81           |
|    time_elapsed         | 263          |
|    total_timesteps      | 165888       |
| train/                  |              |
|    approx_kl            | 0.0037548125 |
|    clip_fraction        | 0.0378       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.68         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00311     |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00647     |
|    value_loss           | 0.00106      |
------------------------------------------
Output 82: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 82          |
|    time_elapsed         | 266         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.012483539 |
|    clip_fraction        | 0.0666      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 2.88e-06    |
-----------------------------------------
Output 83: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 83          |
|    time_elapsed         | 270         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.033627238 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.218      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0194     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 5.15e-05    |
-----------------------------------------
Output 84: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 629        |
|    iterations           | 84         |
|    time_elapsed         | 273        |
|    total_timesteps      | 172032     |
| train/                  |            |
|    approx_kl            | 0.02603834 |
|    clip_fraction        | 0.0621     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.192     |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0173    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 9.82e-06   |
----------------------------------------
Output 85: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 85           |
|    time_elapsed         | 276          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0066652973 |
|    clip_fraction        | 0.0971       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00384      |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00715     |
|    value_loss           | 1.88e-06     |
------------------------------------------
Output 86: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 86           |
|    time_elapsed         | 279          |
|    total_timesteps      | 176128       |
| train/                  |              |
|    approx_kl            | 0.0030995829 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.202       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0182       |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 4.51e-07     |
------------------------------------------
Output 87: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 87          |
|    time_elapsed         | 283         |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.011357735 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.029      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 3.2e-09     |
-----------------------------------------
Output 88: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 88          |
|    time_elapsed         | 286         |
|    total_timesteps      | 180224      |
| train/                  |             |
|    approx_kl            | 0.023788685 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.209      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00648    |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.00487     |
-----------------------------------------
Output 89: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 627          |
|    iterations           | 89           |
|    time_elapsed         | 290          |
|    total_timesteps      | 182272       |
| train/                  |              |
|    approx_kl            | 0.0047408994 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00631     |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00326     |
|    value_loss           | 8.58e-06     |
------------------------------------------
Output 90: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 627         |
|    iterations           | 90          |
|    time_elapsed         | 293         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.002800039 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00151     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 1.13e-06    |
-----------------------------------------
Output 91: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 627        |
|    iterations           | 91         |
|    time_elapsed         | 296        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.01731095 |
|    clip_fraction        | 0.0962     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0194    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 7.55e-07   |
----------------------------------------
Output 92: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 627         |
|    iterations           | 92          |
|    time_elapsed         | 300         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.057662304 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.237      |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00854    |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.000126    |
-----------------------------------------
Output 93: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 627         |
|    iterations           | 93          |
|    time_elapsed         | 303         |
|    total_timesteps      | 190464      |
| train/                  |             |
|    approx_kl            | 0.037479877 |
|    clip_fraction        | 0.0493      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 7.57e-06    |
-----------------------------------------
Output 94: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 627          |
|    iterations           | 94           |
|    time_elapsed         | 306          |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 0.0037376727 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0203      |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00181     |
|    value_loss           | 7.76e-07     |
------------------------------------------
Output 95: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 627          |
|    iterations           | 95           |
|    time_elapsed         | 309          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.0015756369 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.191       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00673     |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 7.26e-07     |
------------------------------------------
Output 96: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 96          |
|    time_elapsed         | 313         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.004887009 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.201      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0315     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 7.4e-06     |
-----------------------------------------
Output 97: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 97          |
|    time_elapsed         | 316         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.005325071 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00123    |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 1.48e-06    |
-----------------------------------------
Output 98: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 98          |
|    time_elapsed         | 319         |
|    total_timesteps      | 200704      |
| train/                  |             |
|    approx_kl            | 0.011686072 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00804     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 1.13e-06    |
-----------------------------------------
Output 99: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 628          |
|    iterations           | 99           |
|    time_elapsed         | 322          |
|    total_timesteps      | 202752       |
| train/                  |              |
|    approx_kl            | 0.0043611163 |
|    clip_fraction        | 0.0682       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.212       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00727      |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 8.27e-07     |
------------------------------------------
Output 100: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 628          |
|    iterations           | 100          |
|    time_elapsed         | 325          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0074618063 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0169      |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00788     |
|    value_loss           | 4.75e-06     |
------------------------------------------
Output 101: Average over 100 episodes - Reward: 0.58
----------------------------------------
| time/                   |            |
|    fps                  | 628        |
|    iterations           | 101        |
|    time_elapsed         | 329        |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.01660714 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.249     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0334    |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 2.67e-08   |
----------------------------------------
Output 102: Average over 100 episodes - Reward: 0.87
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 102         |
|    time_elapsed         | 332         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.015262932 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.191       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00198    |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.0903      |
-----------------------------------------
Output 103: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 103         |
|    time_elapsed         | 335         |
|    total_timesteps      | 210944      |
| train/                  |             |
|    approx_kl            | 0.026314717 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.232      |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0071     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.0344      |
-----------------------------------------
Output 104: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 104         |
|    time_elapsed         | 338         |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.018563217 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.202      |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00297    |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.00511     |
-----------------------------------------
Output 105: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 105         |
|    time_elapsed         | 342         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.036527883 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 2.86e-05    |
-----------------------------------------
Output 106: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 106         |
|    time_elapsed         | 345         |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.012595559 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.384      |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0522     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0408     |
|    value_loss           | 0.000156    |
-----------------------------------------
Output 107: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 107         |
|    time_elapsed         | 348         |
|    total_timesteps      | 219136      |
| train/                  |             |
|    approx_kl            | 0.031617124 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0593     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.041      |
|    value_loss           | 0.000103    |
-----------------------------------------
Output 108: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 627        |
|    iterations           | 108        |
|    time_elapsed         | 352        |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.03523091 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.245     |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0385    |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 4.06e-05   |
----------------------------------------
Output 109: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 627         |
|    iterations           | 109         |
|    time_elapsed         | 355         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.008148348 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0248      |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 0.00298     |
-----------------------------------------
Output 110: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 627         |
|    iterations           | 110         |
|    time_elapsed         | 358         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.010971322 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0305     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.006      |
|    value_loss           | 8.03e-06    |
-----------------------------------------
Output 111: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 627          |
|    iterations           | 111          |
|    time_elapsed         | 362          |
|    total_timesteps      | 227328       |
| train/                  |              |
|    approx_kl            | 0.0072352365 |
|    clip_fraction        | 0.0675       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.409        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00441     |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00847     |
|    value_loss           | 0.00237      |
------------------------------------------
Output 112: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 627          |
|    iterations           | 112          |
|    time_elapsed         | 365          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0047798553 |
|    clip_fraction        | 0.058        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0079      |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 4.15e-06     |
------------------------------------------
Output 113: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 627         |
|    iterations           | 113         |
|    time_elapsed         | 368         |
|    total_timesteps      | 231424      |
| train/                  |             |
|    approx_kl            | 0.005303782 |
|    clip_fraction        | 0.07        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0249     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 0.00339     |
-----------------------------------------
Output 114: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 627        |
|    iterations           | 114        |
|    time_elapsed         | 372        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.00833849 |
|    clip_fraction        | 0.07       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.189     |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0147    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.00402   |
|    value_loss           | 8.98e-06   |
----------------------------------------
Output 115: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 627          |
|    iterations           | 115          |
|    time_elapsed         | 375          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0046626087 |
|    clip_fraction        | 0.0585       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00554     |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00353     |
|    value_loss           | 1.8e-06      |
------------------------------------------
Output 116: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 627        |
|    iterations           | 116        |
|    time_elapsed         | 378        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.01207222 |
|    clip_fraction        | 0.0827     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.198     |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0044    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.00785   |
|    value_loss           | 4.49e-06   |
----------------------------------------
Output 117: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 117         |
|    time_elapsed         | 382         |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.005132646 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00221    |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 4.21e-06    |
-----------------------------------------
Output 118: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 118         |
|    time_elapsed         | 385         |
|    total_timesteps      | 241664      |
| train/                  |             |
|    approx_kl            | 0.004623325 |
|    clip_fraction        | 0.0639      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0058     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 3.44e-07    |
-----------------------------------------
Output 119: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 119         |
|    time_elapsed         | 388         |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.010614268 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.192      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0225     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 7e-07       |
-----------------------------------------
Output 120: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 120         |
|    time_elapsed         | 392         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.024775255 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.026      |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 4.4e-05     |
-----------------------------------------
Output 121: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 626        |
|    iterations           | 121        |
|    time_elapsed         | 395        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.03223162 |
|    clip_fraction        | 0.0348     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0219    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 1.12e-05   |
----------------------------------------
Output 122: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 626        |
|    iterations           | 122        |
|    time_elapsed         | 398        |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.00498769 |
|    clip_fraction        | 0.058      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.144     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00458   |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.00443   |
|    value_loss           | 1.16e-06   |
----------------------------------------
Output 123: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 626          |
|    iterations           | 123          |
|    time_elapsed         | 402          |
|    total_timesteps      | 251904       |
| train/                  |              |
|    approx_kl            | 0.0028220762 |
|    clip_fraction        | 0.0596       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00187     |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00276     |
|    value_loss           | 1.15e-06     |
------------------------------------------
Output 124: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 124         |
|    time_elapsed         | 405         |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.004558799 |
|    clip_fraction        | 0.0385      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 3.65e-07    |
-----------------------------------------
Output 125: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 125         |
|    time_elapsed         | 408         |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.032339983 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0195     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 4.24e-05    |
-----------------------------------------
Output 126: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 126         |
|    time_elapsed         | 412         |
|    total_timesteps      | 258048      |
| train/                  |             |
|    approx_kl            | 0.018106282 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0088     |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 1.39e-05    |
-----------------------------------------
Output 127: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 625        |
|    iterations           | 127        |
|    time_elapsed         | 415        |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.00857231 |
|    clip_fraction        | 0.0602     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.406      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00353   |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.00321   |
|    value_loss           | 0.00249    |
----------------------------------------
Output 128: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 625          |
|    iterations           | 128          |
|    time_elapsed         | 418          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0066082543 |
|    clip_fraction        | 0.0526       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0611       |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 5.96e-06     |
------------------------------------------
Output 129: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 625         |
|    iterations           | 129         |
|    time_elapsed         | 422         |
|    total_timesteps      | 264192      |
| train/                  |             |
|    approx_kl            | 0.022183508 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 1.12e-06    |
-----------------------------------------
Output 130: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 625         |
|    iterations           | 130         |
|    time_elapsed         | 425         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.007664696 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0238     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.000162    |
-----------------------------------------
Output 131: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 625         |
|    iterations           | 131         |
|    time_elapsed         | 428         |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.013433667 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0412     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.000128    |
-----------------------------------------
Output 132: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 625        |
|    iterations           | 132        |
|    time_elapsed         | 432        |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.01429287 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.222     |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0502    |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0262    |
|    value_loss           | 3.74e-05   |
----------------------------------------
Output 133: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 625          |
|    iterations           | 133          |
|    time_elapsed         | 435          |
|    total_timesteps      | 272384       |
| train/                  |              |
|    approx_kl            | 0.0028554443 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0143      |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00883     |
|    value_loss           | 1.41e-05     |
------------------------------------------
Output 134: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 625         |
|    iterations           | 134         |
|    time_elapsed         | 438         |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.005777232 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00209     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00924    |
|    value_loss           | 9.9e-06     |
-----------------------------------------
Output 135: Average over 100 episodes - Reward: 0.97
------------------------------------------
| time/                   |              |
|    fps                  | 625          |
|    iterations           | 135          |
|    time_elapsed         | 442          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.0065109245 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.199       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0228      |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00691     |
|    value_loss           | 3.13e-06     |
------------------------------------------
Output 136: Average over 100 episodes - Reward: 0.97
------------------------------------------
| time/                   |              |
|    fps                  | 624          |
|    iterations           | 136          |
|    time_elapsed         | 445          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0049226666 |
|    clip_fraction        | 0.078        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0043       |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00799     |
|    value_loss           | 0.0112       |
------------------------------------------
Output 137: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 137         |
|    time_elapsed         | 448         |
|    total_timesteps      | 280576      |
| train/                  |             |
|    approx_kl            | 0.011024905 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00976    |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.00881     |
-----------------------------------------
Output 138: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 138         |
|    time_elapsed         | 452         |
|    total_timesteps      | 282624      |
| train/                  |             |
|    approx_kl            | 0.036285415 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0573     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 4.41e-05    |
-----------------------------------------
Output 139: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 139         |
|    time_elapsed         | 455         |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.023619054 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0695     |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0456     |
|    value_loss           | 0.000157    |
-----------------------------------------
Output 140: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 140         |
|    time_elapsed         | 458         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.017496888 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0455     |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.037      |
|    value_loss           | 0.000179    |
-----------------------------------------
Output 141: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 141         |
|    time_elapsed         | 462         |
|    total_timesteps      | 288768      |
| train/                  |             |
|    approx_kl            | 0.029669184 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 5.53e-05    |
-----------------------------------------
Output 142: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 624          |
|    iterations           | 142          |
|    time_elapsed         | 465          |
|    total_timesteps      | 290816       |
| train/                  |              |
|    approx_kl            | 0.0036815086 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.237       |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00293     |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 2.49e-05     |
------------------------------------------
Output 143: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 624          |
|    iterations           | 143          |
|    time_elapsed         | 468          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0063456725 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.231       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000423    |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 1.62e-05     |
------------------------------------------
Output 144: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 144         |
|    time_elapsed         | 472         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.008981727 |
|    clip_fraction        | 0.0818      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.219      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 7.63e-06    |
-----------------------------------------
Output 145: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 145         |
|    time_elapsed         | 475         |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.006047194 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00131     |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 6.28e-06    |
-----------------------------------------
Output 146: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 146         |
|    time_elapsed         | 478         |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.011358421 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00982    |
|    value_loss           | 4.89e-06    |
-----------------------------------------
Output 147: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 147         |
|    time_elapsed         | 482         |
|    total_timesteps      | 301056      |
| train/                  |             |
|    approx_kl            | 0.025213972 |
|    clip_fraction        | 0.0645      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0409      |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 1.8e-05     |
-----------------------------------------
Output 148: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 624          |
|    iterations           | 148          |
|    time_elapsed         | 485          |
|    total_timesteps      | 303104       |
| train/                  |              |
|    approx_kl            | 0.0045359684 |
|    clip_fraction        | 0.0378       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0017      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00411     |
|    value_loss           | 2.58e-06     |
------------------------------------------
Output 149: Average over 100 episodes - Reward: 0.6
----------------------------------------
| time/                   |            |
|    fps                  | 624        |
|    iterations           | 149        |
|    time_elapsed         | 488        |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.04067952 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.22      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0356    |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0277    |
|    value_loss           | 2.61e-08   |
----------------------------------------
Output 150: Average over 100 episodes - Reward: 0.86
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 150         |
|    time_elapsed         | 492         |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.022152532 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | 0.0869      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0292      |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.0956      |
-----------------------------------------
Output 151: Average over 100 episodes - Reward: 0.96
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 151         |
|    time_elapsed         | 495         |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.011879377 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.00103     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.0345      |
-----------------------------------------
Output 152: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 624         |
|    iterations           | 152         |
|    time_elapsed         | 498         |
|    total_timesteps      | 311296      |
| train/                  |             |
|    approx_kl            | 0.008883102 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.0618      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00446     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 0.01        |
-----------------------------------------
Output 153: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 624        |
|    iterations           | 153        |
|    time_elapsed         | 502        |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.01719114 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.179     |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0198    |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 3.86e-05   |
----------------------------------------
Output 154: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 154         |
|    time_elapsed         | 505         |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.017173786 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00427    |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 4.51e-05    |
-----------------------------------------
Output 155: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 155         |
|    time_elapsed         | 509         |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.010956069 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.228       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00177    |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.00565     |
-----------------------------------------
Output 156: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 156         |
|    time_elapsed         | 512         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.014847808 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0105     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0093     |
|    value_loss           | 0.00133     |
-----------------------------------------
Output 157: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 157          |
|    time_elapsed         | 515          |
|    total_timesteps      | 321536       |
| train/                  |              |
|    approx_kl            | 0.0033315772 |
|    clip_fraction        | 0.0361       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.86e-05     |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 4.81e-06     |
------------------------------------------
Output 158: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 158          |
|    time_elapsed         | 519          |
|    total_timesteps      | 323584       |
| train/                  |              |
|    approx_kl            | 0.0034338394 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000926    |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00762     |
|    value_loss           | 3.06e-06     |
------------------------------------------
Output 159: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 159         |
|    time_elapsed         | 522         |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.005254404 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0121      |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 0.00584     |
-----------------------------------------
Output 160: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 160         |
|    time_elapsed         | 525         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.029291654 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.034      |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00933    |
|    value_loss           | 1.11e-05    |
-----------------------------------------
Output 161: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 161         |
|    time_elapsed         | 529         |
|    total_timesteps      | 329728      |
| train/                  |             |
|    approx_kl            | 0.038414784 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0373     |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.000103    |
-----------------------------------------
Output 162: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 162         |
|    time_elapsed         | 532         |
|    total_timesteps      | 331776      |
| train/                  |             |
|    approx_kl            | 0.026452819 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000825   |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.029      |
|    value_loss           | 1.99e-05    |
-----------------------------------------
Output 163: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 163         |
|    time_elapsed         | 535         |
|    total_timesteps      | 333824      |
| train/                  |             |
|    approx_kl            | 0.019506907 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0202     |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.00356     |
-----------------------------------------
Output 164: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 164         |
|    time_elapsed         | 538         |
|    total_timesteps      | 335872      |
| train/                  |             |
|    approx_kl            | 0.030262168 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 4.49e-05    |
-----------------------------------------
Output 165: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 165         |
|    time_elapsed         | 542         |
|    total_timesteps      | 337920      |
| train/                  |             |
|    approx_kl            | 0.022014316 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 1.86e-05    |
-----------------------------------------
Output 166: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 166          |
|    time_elapsed         | 545          |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.0029576803 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00157      |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00476     |
|    value_loss           | 0.00357      |
------------------------------------------
Output 167: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 167         |
|    time_elapsed         | 548         |
|    total_timesteps      | 342016      |
| train/                  |             |
|    approx_kl            | 0.015184287 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 4.41e-06    |
-----------------------------------------
Output 168: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 168         |
|    time_elapsed         | 552         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.010355068 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.238      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0228     |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 5.11e-05    |
-----------------------------------------
Output 169: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 169         |
|    time_elapsed         | 555         |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.034800146 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.209      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 2.65e-05    |
-----------------------------------------
Output 170: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 170         |
|    time_elapsed         | 558         |
|    total_timesteps      | 348160      |
| train/                  |             |
|    approx_kl            | 0.008912163 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00654    |
|    value_loss           | 0.00596     |
-----------------------------------------
Output 171: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 171          |
|    time_elapsed         | 561          |
|    total_timesteps      | 350208       |
| train/                  |              |
|    approx_kl            | 0.0057249446 |
|    clip_fraction        | 0.0634       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.61e-05     |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00521     |
|    value_loss           | 1.6e-05      |
------------------------------------------
Output 172: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 172         |
|    time_elapsed         | 565         |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.018692195 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0153     |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 3.4e-06     |
-----------------------------------------
Output 173: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 173         |
|    time_elapsed         | 568         |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.009634081 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.000146    |
-----------------------------------------
Output 174: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 174        |
|    time_elapsed         | 572        |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.02499488 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00472   |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 0.000104   |
----------------------------------------
Output 175: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 175        |
|    time_elapsed         | 575        |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.02769589 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0414    |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 3.34e-05   |
----------------------------------------
Output 176: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 176         |
|    time_elapsed         | 578         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.013185325 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0165     |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 1.06e-05    |
-----------------------------------------
Output 177: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 177          |
|    time_elapsed         | 581          |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0054164953 |
|    clip_fraction        | 0.0508       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0287      |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.00415     |
|    value_loss           | 1.17e-06     |
------------------------------------------
Output 178: Average over 100 episodes - Reward: 0.99
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 178          |
|    time_elapsed         | 584          |
|    total_timesteps      | 364544       |
| train/                  |              |
|    approx_kl            | 0.0039504957 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0241      |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.0038      |
|    value_loss           | 2.79e-06     |
------------------------------------------
Output 179: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 179          |
|    time_elapsed         | 588          |
|    total_timesteps      | 366592       |
| train/                  |              |
|    approx_kl            | 0.0039996617 |
|    clip_fraction        | 0.0794       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0273      |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.00138      |
------------------------------------------
Output 180: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 180         |
|    time_elapsed         | 591         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.015252473 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.202      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0138     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 0.00281     |
-----------------------------------------
Output 181: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 181          |
|    time_elapsed         | 594          |
|    total_timesteps      | 370688       |
| train/                  |              |
|    approx_kl            | 0.0045168884 |
|    clip_fraction        | 0.0548       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0109       |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.0037      |
|    value_loss           | 0.00341      |
------------------------------------------
Output 182: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 182         |
|    time_elapsed         | 598         |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.010895927 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00985    |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00543    |
|    value_loss           | 8.5e-06     |
-----------------------------------------
Output 183: Average over 100 episodes - Reward: 0.94
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 183         |
|    time_elapsed         | 601         |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.010791905 |
|    clip_fraction        | 0.0681      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 1.01e-06    |
-----------------------------------------
Output 184: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 184         |
|    time_elapsed         | 604         |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.009743955 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.0849      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0184     |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0208      |
-----------------------------------------
Output 185: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 185         |
|    time_elapsed         | 607         |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.013972007 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.236      |
|    explained_variance   | 0.227       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.00871     |
-----------------------------------------
Output 186: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 186         |
|    time_elapsed         | 611         |
|    total_timesteps      | 380928      |
| train/                  |             |
|    approx_kl            | 0.027355144 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0683     |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.000116    |
-----------------------------------------
Output 187: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 187         |
|    time_elapsed         | 614         |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.041929644 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0588     |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0512     |
|    value_loss           | 0.000126    |
-----------------------------------------
Output 188: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 188         |
|    time_elapsed         | 617         |
|    total_timesteps      | 385024      |
| train/                  |             |
|    approx_kl            | 0.012592504 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 9.64e-05    |
-----------------------------------------
Output 189: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 623       |
|    iterations           | 189       |
|    time_elapsed         | 621       |
|    total_timesteps      | 387072    |
| train/                  |           |
|    approx_kl            | 0.0268552 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.211    |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0368   |
|    n_updates            | 1880      |
|    policy_gradient_loss | -0.0309   |
|    value_loss           | 2.89e-05  |
---------------------------------------
Output 190: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 190         |
|    time_elapsed         | 624         |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.017449602 |
|    clip_fraction        | 0.0823      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 7.15e-06    |
-----------------------------------------
Output 191: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 191         |
|    time_elapsed         | 627         |
|    total_timesteps      | 391168      |
| train/                  |             |
|    approx_kl            | 0.011145588 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0346     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 1.41e-05    |
-----------------------------------------
Output 192: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 623        |
|    iterations           | 192        |
|    time_elapsed         | 631        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.03159365 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.192     |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0467    |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.029     |
|    value_loss           | 2.08e-05   |
----------------------------------------
Output 193: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 193         |
|    time_elapsed         | 634         |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.026830237 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0128     |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.00995    |
|    value_loss           | 2.91e-06    |
-----------------------------------------
Output 194: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 194         |
|    time_elapsed         | 637         |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.008356645 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00816    |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.00303     |
-----------------------------------------
Output 195: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 195         |
|    time_elapsed         | 640         |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.031113362 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.226      |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 6.59e-05    |
-----------------------------------------
Output 196: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 196         |
|    time_elapsed         | 644         |
|    total_timesteps      | 401408      |
| train/                  |             |
|    approx_kl            | 0.021190654 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0219     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 8.25e-06    |
-----------------------------------------
Output 197: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 197         |
|    time_elapsed         | 647         |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.027671063 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.257      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0613     |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 1.49e-06    |
-----------------------------------------
Output 198: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 198         |
|    time_elapsed         | 650         |
|    total_timesteps      | 405504      |
| train/                  |             |
|    approx_kl            | 0.010707444 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0435     |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.00011     |
-----------------------------------------
Output 199: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 199         |
|    time_elapsed         | 653         |
|    total_timesteps      | 407552      |
| train/                  |             |
|    approx_kl            | 0.032035697 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0273     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 8.17e-05    |
-----------------------------------------
Output 200: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 200         |
|    time_elapsed         | 657         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.024275383 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0424     |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 1.11e-05    |
-----------------------------------------
Output 201: Average over 100 episodes - Reward: 0.7
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 201         |
|    time_elapsed         | 660         |
|    total_timesteps      | 411648      |
| train/                  |             |
|    approx_kl            | 0.011773294 |
|    clip_fraction        | 0.0548      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 3.32e-06    |
-----------------------------------------
Output 202: Average over 100 episodes - Reward: 0.93
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 202         |
|    time_elapsed         | 663         |
|    total_timesteps      | 413696      |
| train/                  |             |
|    approx_kl            | 0.022873618 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00526    |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.0332     |
|    value_loss           | 0.0391      |
-----------------------------------------
Output 203: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 203         |
|    time_elapsed         | 667         |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.014840322 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | -0.00535    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.00989     |
-----------------------------------------
Output 204: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 623          |
|    iterations           | 204          |
|    time_elapsed         | 670          |
|    total_timesteps      | 417792       |
| train/                  |              |
|    approx_kl            | 0.0051274756 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.493        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000846    |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00445     |
|    value_loss           | 0.00138      |
------------------------------------------
Output 205: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 205         |
|    time_elapsed         | 673         |
|    total_timesteps      | 419840      |
| train/                  |             |
|    approx_kl            | 0.021854416 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000406   |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 5.46e-06    |
-----------------------------------------
Output 206: Average over 100 episodes - Reward: 0.59
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 206         |
|    time_elapsed         | 677         |
|    total_timesteps      | 421888      |
| train/                  |             |
|    approx_kl            | 0.015980028 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00919    |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 1.12e-06    |
-----------------------------------------
Output 207: Average over 100 episodes - Reward: 0.84
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 207         |
|    time_elapsed         | 680         |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.016615197 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.0683      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00848     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.0904      |
-----------------------------------------
Output 208: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 208         |
|    time_elapsed         | 683         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.015758574 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.251      |
|    explained_variance   | 0.0885      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0376     |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.0401      |
-----------------------------------------
Output 209: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 209         |
|    time_elapsed         | 686         |
|    total_timesteps      | 428032      |
| train/                  |             |
|    approx_kl            | 0.008202482 |
|    clip_fraction        | 0.0435      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.269      |
|    explained_variance   | -0.2        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00522    |
|    value_loss           | 0.00287     |
-----------------------------------------
Output 210: Average over 100 episodes - Reward: 0.91
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 210         |
|    time_elapsed         | 690         |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.029093735 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.269      |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0601     |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 4.7e-05     |
-----------------------------------------
Output 211: Average over 100 episodes - Reward: 0.93
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 211         |
|    time_elapsed         | 693         |
|    total_timesteps      | 432128      |
| train/                  |             |
|    approx_kl            | 0.014222979 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.00891     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0158      |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0491      |
-----------------------------------------
Output 212: Average over 100 episodes - Reward: 0.97
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 212         |
|    time_elapsed         | 696         |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 0.011535384 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.33       |
|    explained_variance   | 0.131       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.0231      |
-----------------------------------------
Output 213: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 213         |
|    time_elapsed         | 700         |
|    total_timesteps      | 436224      |
| train/                  |             |
|    approx_kl            | 0.021758694 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.308      |
|    explained_variance   | 0.0712      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0365     |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 0.0127      |
-----------------------------------------
Output 214: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 214         |
|    time_elapsed         | 703         |
|    total_timesteps      | 438272      |
| train/                  |             |
|    approx_kl            | 0.013192996 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0259      |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.00363     |
-----------------------------------------
Output 215: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 215         |
|    time_elapsed         | 706         |
|    total_timesteps      | 440320      |
| train/                  |             |
|    approx_kl            | 0.024685927 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00247    |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.00332     |
-----------------------------------------
Output 216: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 216        |
|    time_elapsed         | 710        |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.02892821 |
|    clip_fraction        | 0.0793     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.26      |
|    explained_variance   | 0.302      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0389    |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 0.00348    |
----------------------------------------
Output 217: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 217         |
|    time_elapsed         | 713         |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.024525978 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.263      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0366     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 3.73e-05    |
-----------------------------------------
Output 218: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 218         |
|    time_elapsed         | 716         |
|    total_timesteps      | 446464      |
| train/                  |             |
|    approx_kl            | 0.010867902 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.234      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0127     |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00895    |
|    value_loss           | 6.58e-06    |
-----------------------------------------
Output 219: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 219         |
|    time_elapsed         | 719         |
|    total_timesteps      | 448512      |
| train/                  |             |
|    approx_kl            | 0.013966848 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.034      |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 6.33e-07    |
-----------------------------------------
Output 220: Average over 100 episodes - Reward: 0.99
----------------------------------------
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 220        |
|    time_elapsed         | 723        |
|    total_timesteps      | 450560     |
| train/                  |            |
|    approx_kl            | 0.02661833 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0255    |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0288    |
|    value_loss           | 5.37e-05   |
----------------------------------------
Output 221: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 221         |
|    time_elapsed         | 726         |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.026514713 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.238      |
|    explained_variance   | 0.224       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0197     |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.00571     |
-----------------------------------------
Output 222: Average over 100 episodes - Reward: 1.0
---------------------------------------
| time/                   |           |
|    fps                  | 622       |
|    iterations           | 222       |
|    time_elapsed         | 729       |
|    total_timesteps      | 454656    |
| train/                  |           |
|    approx_kl            | 0.0078857 |
|    clip_fraction        | 0.0674    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.215    |
|    explained_variance   | 0.27      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0441   |
|    n_updates            | 2210      |
|    policy_gradient_loss | -0.00565  |
|    value_loss           | 0.00353   |
---------------------------------------
Output 223: Average over 100 episodes - Reward: 0.99
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 223         |
|    time_elapsed         | 733         |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.021478161 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 1.74e-05    |
-----------------------------------------
Output 224: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 224         |
|    time_elapsed         | 736         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.008960294 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.02       |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.00236     |
-----------------------------------------
Output 225: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 225         |
|    time_elapsed         | 739         |
|    total_timesteps      | 460800      |
| train/                  |             |
|    approx_kl            | 0.012944078 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.00907    |
|    value_loss           | 2.15e-05    |
-----------------------------------------
Output 226: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 226         |
|    time_elapsed         | 743         |
|    total_timesteps      | 462848      |
| train/                  |             |
|    approx_kl            | 0.031699877 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.232      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00557     |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 1.72e-05    |
-----------------------------------------
Output 227: Average over 100 episodes - Reward: 0.97
----------------------------------------
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 227        |
|    time_elapsed         | 746        |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.01269073 |
|    clip_fraction        | 0.086      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.212     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0113    |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.00725   |
|    value_loss           | 3.25e-06   |
----------------------------------------
Output 228: Average over 100 episodes - Reward: 0.96
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 228         |
|    time_elapsed         | 749         |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.011801967 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.217      |
|    explained_variance   | 0.0736      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0108      |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.0244      |
-----------------------------------------
Output 229: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 229         |
|    time_elapsed         | 752         |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.008972487 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000593    |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.0141      |
-----------------------------------------
Output 230: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 230         |
|    time_elapsed         | 756         |
|    total_timesteps      | 471040      |
| train/                  |             |
|    approx_kl            | 0.031823687 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0164     |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 8.19e-05    |
-----------------------------------------
Output 231: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 231         |
|    time_elapsed         | 759         |
|    total_timesteps      | 473088      |
| train/                  |             |
|    approx_kl            | 0.032283712 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.303      |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0296     |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.000102    |
-----------------------------------------
Output 232: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 232         |
|    time_elapsed         | 762         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.026271766 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.236      |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 4.62e-05    |
-----------------------------------------
Output 233: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 233         |
|    time_elapsed         | 766         |
|    total_timesteps      | 477184      |
| train/                  |             |
|    approx_kl            | 0.011042044 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.025      |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 1.6e-05     |
-----------------------------------------
Output 234: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 234         |
|    time_elapsed         | 769         |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.012194102 |
|    clip_fraction        | 0.0668      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 0.00296     |
-----------------------------------------
Output 235: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 622          |
|    iterations           | 235          |
|    time_elapsed         | 773          |
|    total_timesteps      | 481280       |
| train/                  |              |
|    approx_kl            | 0.0063280063 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0261      |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.00599     |
|    value_loss           | 8.47e-06     |
------------------------------------------
Output 236: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 236         |
|    time_elapsed         | 776         |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.025784908 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.221      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0336     |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 2.08e-06    |
-----------------------------------------
Output 237: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 237        |
|    time_elapsed         | 779        |
|    total_timesteps      | 485376     |
| train/                  |            |
|    approx_kl            | 0.05569321 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.242     |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0379    |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.0289    |
|    value_loss           | 0.000117   |
----------------------------------------
Output 238: Average over 100 episodes - Reward: 1.0
----------------------------------------
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 238        |
|    time_elapsed         | 783        |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.04468733 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.209     |
|    explained_variance   | 0.281      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000755  |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0213    |
|    value_loss           | 0.00369    |
----------------------------------------
Output 239: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 239         |
|    time_elapsed         | 786         |
|    total_timesteps      | 489472      |
| train/                  |             |
|    approx_kl            | 0.023086127 |
|    clip_fraction        | 0.0556      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.219      |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0183     |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 4.03e-06    |
-----------------------------------------
Output 240: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 240         |
|    time_elapsed         | 789         |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.031867083 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0474     |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.000145    |
-----------------------------------------
Output 241: Average over 100 episodes - Reward: 0.98
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 241         |
|    time_elapsed         | 793         |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.048743255 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0184     |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 3.2e-05     |
-----------------------------------------
Output 242: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 242         |
|    time_elapsed         | 796         |
|    total_timesteps      | 495616      |
| train/                  |             |
|    approx_kl            | 0.010392413 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00404     |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 0.00462     |
-----------------------------------------
Output 243: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 243         |
|    time_elapsed         | 800         |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.003956821 |
|    clip_fraction        | 0.0705      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00513    |
|    value_loss           | 5.52e-06    |
-----------------------------------------
Output 244: Average over 100 episodes - Reward: 1.0
------------------------------------------
| time/                   |              |
|    fps                  | 621          |
|    iterations           | 244          |
|    time_elapsed         | 803          |
|    total_timesteps      | 499712       |
| train/                  |              |
|    approx_kl            | 0.0064020893 |
|    clip_fraction        | 0.0704       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.017       |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.0049      |
|    value_loss           | 8.48e-07     |
------------------------------------------
Output 245: Average over 100 episodes - Reward: 1.0
-----------------------------------------
| time/                   |             |
|    fps                  | 621         |
|    iterations           | 245         |
|    time_elapsed         | 806         |
|    total_timesteps      | 501760      |
| train/                  |             |
|    approx_kl            | 0.008786172 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.217      |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0134     |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 9.22e-06    |
-----------------------------------------
Overall: Average Reward: 0.953416912487709
