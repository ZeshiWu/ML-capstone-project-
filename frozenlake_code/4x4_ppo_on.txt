4x4 frozen lake map PPO - is_slippery on
Using cpu device
Logging to ./PPOtensorboard/PPO_9
Output 1: Average over 100 episodes - Reward: 0.0
-----------------------------
| time/              |      |
|    fps             | 1089 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 2048 |
-----------------------------
Output 2: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 773         |
|    iterations           | 2           |
|    time_elapsed         | 5           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.016806401 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -11.3       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.016      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 0.0048      |
-----------------------------------------
Output 3: Average over 100 episodes - Reward: 0.04
-----------------------------------------
| time/                   |             |
|    fps                  | 707         |
|    iterations           | 3           |
|    time_elapsed         | 8           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.010882476 |
|    clip_fraction        | 0.0762      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.0146      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.01       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00846    |
|    value_loss           | 0.0183      |
-----------------------------------------
Output 4: Average over 100 episodes - Reward: 0.04
----------------------------------------
| time/                   |            |
|    fps                  | 670        |
|    iterations           | 4          |
|    time_elapsed         | 12         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01217748 |
|    clip_fraction        | 0.0783     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0147     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00824   |
|    value_loss           | 0.0187     |
----------------------------------------
Output 5: Average over 100 episodes - Reward: 0.03
-----------------------------------------
| time/                   |             |
|    fps                  | 651         |
|    iterations           | 5           |
|    time_elapsed         | 15          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.007310791 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000729   |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00646    |
|    value_loss           | 0.0165      |
-----------------------------------------
Output 6: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 630         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012754362 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.0948      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00373     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00961    |
|    value_loss           | 0.0245      |
-----------------------------------------
Output 7: Average over 100 episodes - Reward: 0.02
-----------------------------------------
| time/                   |             |
|    fps                  | 617         |
|    iterations           | 7           |
|    time_elapsed         | 23          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012667261 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00658     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00961    |
|    value_loss           | 0.0209      |
-----------------------------------------
Output 8: Average over 100 episodes - Reward: 0.01
-----------------------------------------
| time/                   |             |
|    fps                  | 611         |
|    iterations           | 8           |
|    time_elapsed         | 26          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.008555341 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00563     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.0196      |
-----------------------------------------
Output 9: Average over 100 episodes - Reward: 0.05
------------------------------------------
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 9            |
|    time_elapsed         | 30           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0077200504 |
|    clip_fraction        | 0.0849       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0158      |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00964     |
|    value_loss           | 0.015        |
------------------------------------------
Output 10: Average over 100 episodes - Reward: 0.04
-----------------------------------------
| time/                   |             |
|    fps                  | 602         |
|    iterations           | 10          |
|    time_elapsed         | 33          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.009101567 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.129       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0111     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.0329      |
-----------------------------------------
Output 11: Average over 100 episodes - Reward: 0.03
-----------------------------------------
| time/                   |             |
|    fps                  | 599         |
|    iterations           | 11          |
|    time_elapsed         | 37          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.013797207 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00893     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.0274      |
-----------------------------------------
Output 12: Average over 100 episodes - Reward: 0.04
-----------------------------------------
| time/                   |             |
|    fps                  | 596         |
|    iterations           | 12          |
|    time_elapsed         | 41          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011359693 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.164       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0232      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 0.0259      |
-----------------------------------------
Output 13: Average over 100 episodes - Reward: 0.08
-----------------------------------------
| time/                   |             |
|    fps                  | 595         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.011398627 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.0868      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00616     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.0193      |
-----------------------------------------
Output 14: Average over 100 episodes - Reward: 0.05
----------------------------------------
| time/                   |            |
|    fps                  | 594        |
|    iterations           | 14         |
|    time_elapsed         | 48         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.01084634 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00609   |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00923   |
|    value_loss           | 0.0434     |
----------------------------------------
Output 15: Average over 100 episodes - Reward: 0.06
----------------------------------------
| time/                   |            |
|    fps                  | 592        |
|    iterations           | 15         |
|    time_elapsed         | 51         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.01833167 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.226      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0115    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.0361     |
----------------------------------------
Output 16: Average over 100 episodes - Reward: 0.15
-----------------------------------------
| time/                   |             |
|    fps                  | 592         |
|    iterations           | 16          |
|    time_elapsed         | 55          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.012228652 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00299    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.0455      |
-----------------------------------------
Output 17: Average over 100 episodes - Reward: 0.07
-----------------------------------------
| time/                   |             |
|    fps                  | 591         |
|    iterations           | 17          |
|    time_elapsed         | 58          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.013663464 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0122      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.0527      |
-----------------------------------------
Output 18: Average over 100 episodes - Reward: 0.12
-----------------------------------------
| time/                   |             |
|    fps                  | 591         |
|    iterations           | 18          |
|    time_elapsed         | 62          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.014314977 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | -0.074      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00877    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00874    |
|    value_loss           | 0.0392      |
-----------------------------------------
Output 19: Average over 100 episodes - Reward: 0.19
-----------------------------------------
| time/                   |             |
|    fps                  | 590         |
|    iterations           | 19          |
|    time_elapsed         | 65          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.008643083 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.87       |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0157      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 0.0444      |
-----------------------------------------
Output 20: Average over 100 episodes - Reward: 0.23
-----------------------------------------
| time/                   |             |
|    fps                  | 589         |
|    iterations           | 20          |
|    time_elapsed         | 69          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.008589075 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.127       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00941    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00669    |
|    value_loss           | 0.0598      |
-----------------------------------------
Output 21: Average over 98 episodes - Reward: 0.21428571428571427
------------------------------------------
| time/                   |              |
|    fps                  | 589          |
|    iterations           | 21           |
|    time_elapsed         | 72           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0047319056 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.76        |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0555       |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00474     |
|    value_loss           | 0.0605       |
------------------------------------------
Output 22: Average over 87 episodes - Reward: 0.26436781609195403
-----------------------------------------
| time/                   |             |
|    fps                  | 588         |
|    iterations           | 22          |
|    time_elapsed         | 76          |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.008064672 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0118      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00879    |
|    value_loss           | 0.0539      |
-----------------------------------------
Output 23: Average over 82 episodes - Reward: 0.34146341463414637
-----------------------------------------
| time/                   |             |
|    fps                  | 586         |
|    iterations           | 23          |
|    time_elapsed         | 80          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.006780672 |
|    clip_fraction        | 0.0886      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.157       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00385    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00893    |
|    value_loss           | 0.0588      |
-----------------------------------------
Output 24: Average over 80 episodes - Reward: 0.4
------------------------------------------
| time/                   |              |
|    fps                  | 585          |
|    iterations           | 24           |
|    time_elapsed         | 83           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0043834597 |
|    clip_fraction        | 0.0584       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0298       |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00665     |
|    value_loss           | 0.0655       |
------------------------------------------
Output 25: Average over 72 episodes - Reward: 0.3888888888888889
------------------------------------------
| time/                   |              |
|    fps                  | 584          |
|    iterations           | 25           |
|    time_elapsed         | 87           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0031976185 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.453       |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0309       |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 0.067        |
------------------------------------------
Output 26: Average over 67 episodes - Reward: 0.3880597014925373
------------------------------------------
| time/                   |              |
|    fps                  | 583          |
|    iterations           | 26           |
|    time_elapsed         | 91           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0026409123 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0208       |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00606     |
|    value_loss           | 0.0614       |
------------------------------------------
Output 27: Average over 72 episodes - Reward: 0.4722222222222222
------------------------------------------
| time/                   |              |
|    fps                  | 582          |
|    iterations           | 27           |
|    time_elapsed         | 94           |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0035475926 |
|    clip_fraction        | 0.0611       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.348       |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0351       |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00626     |
|    value_loss           | 0.0591       |
------------------------------------------
Output 28: Average over 63 episodes - Reward: 0.49206349206349204
------------------------------------------
| time/                   |              |
|    fps                  | 583          |
|    iterations           | 28           |
|    time_elapsed         | 98           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0019835841 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.313       |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0306       |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00372     |
|    value_loss           | 0.0624       |
------------------------------------------
Output 29: Average over 63 episodes - Reward: 0.49206349206349204
------------------------------------------
| time/                   |              |
|    fps                  | 583          |
|    iterations           | 29           |
|    time_elapsed         | 101          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0013272243 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.279       |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0273       |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.0592       |
------------------------------------------
Output 30: Average over 57 episodes - Reward: 0.543859649122807
------------------------------------------
| time/                   |              |
|    fps                  | 583          |
|    iterations           | 30           |
|    time_elapsed         | 105          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0025562586 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.253       |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0269       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00344     |
|    value_loss           | 0.0564       |
------------------------------------------
Output 31: Average over 61 episodes - Reward: 0.6557377049180327
------------------------------------------
| time/                   |              |
|    fps                  | 583          |
|    iterations           | 31           |
|    time_elapsed         | 108          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0017375496 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0191       |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00374     |
|    value_loss           | 0.0517       |
------------------------------------------
Output 32: Average over 57 episodes - Reward: 0.5614035087719298
------------------------------------------
| time/                   |              |
|    fps                  | 581          |
|    iterations           | 32           |
|    time_elapsed         | 112          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0017214397 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.202       |
|    explained_variance   | 0.219        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0279       |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00447     |
|    value_loss           | 0.053        |
------------------------------------------
Output 33: Average over 64 episodes - Reward: 0.546875
------------------------------------------
| time/                   |              |
|    fps                  | 582          |
|    iterations           | 33           |
|    time_elapsed         | 116          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0008285908 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0282       |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 0.0539       |
------------------------------------------
Output 34: Average over 51 episodes - Reward: 0.5294117647058824
------------------------------------------
| time/                   |              |
|    fps                  | 581          |
|    iterations           | 34           |
|    time_elapsed         | 119          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0013710867 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0239       |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 0.0572       |
------------------------------------------
Output 35: Average over 61 episodes - Reward: 0.6065573770491803
------------------------------------------
| time/                   |              |
|    fps                  | 581          |
|    iterations           | 35           |
|    time_elapsed         | 123          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0012494994 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00725      |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 0.0413       |
------------------------------------------
Output 36: Average over 46 episodes - Reward: 0.6304347826086957
------------------------------------------
| time/                   |              |
|    fps                  | 580          |
|    iterations           | 36           |
|    time_elapsed         | 126          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0007904496 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0237       |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 0.0555       |
------------------------------------------
Output 37: Average over 61 episodes - Reward: 0.5737704918032787
------------------------------------------
| time/                   |              |
|    fps                  | 580          |
|    iterations           | 37           |
|    time_elapsed         | 130          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0020851665 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.14        |
|    explained_variance   | 0.0763       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0127       |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 0.0409       |
------------------------------------------
Output 38: Average over 49 episodes - Reward: 0.6122448979591837
------------------------------------------
| time/                   |              |
|    fps                  | 581          |
|    iterations           | 38           |
|    time_elapsed         | 133          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0021763616 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.108       |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0202       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 0.0536       |
------------------------------------------
Output 39: Average over 42 episodes - Reward: 0.5714285714285714
-------------------------------------------
| time/                   |               |
|    fps                  | 581           |
|    iterations           | 39            |
|    time_elapsed         | 137           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00097445695 |
|    clip_fraction        | 0.0119        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.088        |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0153        |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.00203      |
|    value_loss           | 0.0484        |
-------------------------------------------
Output 40: Average over 52 episodes - Reward: 0.6538461538461539
------------------------------------------
| time/                   |              |
|    fps                  | 581          |
|    iterations           | 40           |
|    time_elapsed         | 140          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0006203038 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0827      |
|    explained_variance   | 0.0658       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0129       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000783    |
|    value_loss           | 0.0408       |
------------------------------------------
Output 41: Average over 62 episodes - Reward: 0.7258064516129032
-------------------------------------------
| time/                   |               |
|    fps                  | 579           |
|    iterations           | 41            |
|    time_elapsed         | 144           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00038151254 |
|    clip_fraction        | 0.0083        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0689       |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0241        |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.00114      |
|    value_loss           | 0.0463        |
-------------------------------------------
Output 42: Average over 50 episodes - Reward: 0.66
------------------------------------------
| time/                   |              |
|    fps                  | 578          |
|    iterations           | 42           |
|    time_elapsed         | 148          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0006789861 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0729      |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0255       |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 0.0499       |
------------------------------------------
Output 43: Average over 52 episodes - Reward: 0.6153846153846154
-------------------------------------------
| time/                   |               |
|    fps                  | 578           |
|    iterations           | 43            |
|    time_elapsed         | 152           |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00058706786 |
|    clip_fraction        | 0.00518       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0767       |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0104        |
|    n_updates            | 420           |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 0.0412        |
-------------------------------------------
Output 44: Average over 55 episodes - Reward: 0.7090909090909091
-------------------------------------------
| time/                   |               |
|    fps                  | 578           |
|    iterations           | 44            |
|    time_elapsed         | 155           |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00035412202 |
|    clip_fraction        | 0.0062        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0798       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0203        |
|    n_updates            | 430           |
|    policy_gradient_loss | -0.00146      |
|    value_loss           | 0.0417        |
-------------------------------------------
Output 45: Average over 55 episodes - Reward: 0.7090909090909091
------------------------------------------
| time/                   |              |
|    fps                  | 578          |
|    iterations           | 45           |
|    time_elapsed         | 159          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0019204768 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0903      |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0223       |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00279     |
|    value_loss           | 0.0486       |
------------------------------------------
Output 46: Average over 61 episodes - Reward: 0.6721311475409836
------------------------------------------
| time/                   |              |
|    fps                  | 577          |
|    iterations           | 46           |
|    time_elapsed         | 163          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0024093008 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.11        |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0165       |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 0.0451       |
------------------------------------------
Output 47: Average over 51 episodes - Reward: 0.6078431372549019
-----------------------------------------
| time/                   |             |
|    fps                  | 577         |
|    iterations           | 47          |
|    time_elapsed         | 166         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.003199719 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.168       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0238      |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0032     |
|    value_loss           | 0.0501      |
-----------------------------------------
Output 48: Average over 42 episodes - Reward: 0.7380952380952381
------------------------------------------
| time/                   |              |
|    fps                  | 577          |
|    iterations           | 48           |
|    time_elapsed         | 170          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0007270281 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 0.063        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0192       |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000193    |
|    value_loss           | 0.0423       |
------------------------------------------
Output 49: Average over 48 episodes - Reward: 0.6666666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 576           |
|    iterations           | 49            |
|    time_elapsed         | 174           |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.00030948163 |
|    clip_fraction        | 0.00698       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.104        |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0155        |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.000458     |
|    value_loss           | 0.0306        |
-------------------------------------------
Output 50: Average over 50 episodes - Reward: 0.7
------------------------------------------
| time/                   |              |
|    fps                  | 575          |
|    iterations           | 50           |
|    time_elapsed         | 177          |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0003943604 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.106       |
|    explained_variance   | 0.209        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0222       |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.000175    |
|    value_loss           | 0.0359       |
------------------------------------------
Output 51: Average over 47 episodes - Reward: 0.6808510638297872
------------------------------------------
| time/                   |              |
|    fps                  | 575          |
|    iterations           | 51           |
|    time_elapsed         | 181          |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.0043431707 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0175       |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.0412       |
------------------------------------------
Output 52: Average over 53 episodes - Reward: 0.7547169811320755
------------------------------------------
| time/                   |              |
|    fps                  | 575          |
|    iterations           | 52           |
|    time_elapsed         | 185          |
|    total_timesteps      | 106496       |
| train/                  |              |
|    approx_kl            | 0.0014110103 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0134       |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.0362       |
------------------------------------------
Output 53: Average over 55 episodes - Reward: 0.6727272727272727
-----------------------------------------
| time/                   |             |
|    fps                  | 575         |
|    iterations           | 53          |
|    time_elapsed         | 188         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.001229261 |
|    clip_fraction        | 0.0228      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.196       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0105      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 0.0369      |
-----------------------------------------
Output 54: Average over 53 episodes - Reward: 0.6226415094339622
------------------------------------------
| time/                   |              |
|    fps                  | 575          |
|    iterations           | 54           |
|    time_elapsed         | 192          |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 0.0019487934 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.11        |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0188       |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 0.045        |
------------------------------------------
Output 55: Average over 50 episodes - Reward: 0.6
------------------------------------------
| time/                   |              |
|    fps                  | 575          |
|    iterations           | 55           |
|    time_elapsed         | 195          |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.0006890403 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0901      |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.017        |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 0.0433       |
------------------------------------------
Output 56: Average over 60 episodes - Reward: 0.6333333333333333
-------------------------------------------
| time/                   |               |
|    fps                  | 575           |
|    iterations           | 56            |
|    time_elapsed         | 199           |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.00051105587 |
|    clip_fraction        | 0.00674       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0791       |
|    explained_variance   | 0.262         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0215        |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.00135      |
|    value_loss           | 0.0406        |
-------------------------------------------
Output 57: Average over 48 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 574          |
|    iterations           | 57           |
|    time_elapsed         | 203          |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.0007322659 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0745      |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0174       |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.000997    |
|    value_loss           | 0.0524       |
------------------------------------------
Output 58: Average over 47 episodes - Reward: 0.723404255319149
------------------------------------------
| time/                   |              |
|    fps                  | 573          |
|    iterations           | 58           |
|    time_elapsed         | 207          |
|    total_timesteps      | 118784       |
| train/                  |              |
|    approx_kl            | 0.0004992838 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0684      |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0164       |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 0.0371       |
------------------------------------------
Output 59: Average over 72 episodes - Reward: 0.7222222222222222
-------------------------------------------
| time/                   |               |
|    fps                  | 573           |
|    iterations           | 59            |
|    time_elapsed         | 210           |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 0.00058930466 |
|    clip_fraction        | 0.00942       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0603       |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0119        |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 0.0357        |
-------------------------------------------
Output 60: Average over 56 episodes - Reward: 0.7142857142857143
------------------------------------------
| time/                   |              |
|    fps                  | 573          |
|    iterations           | 60           |
|    time_elapsed         | 214          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0013913875 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0563      |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0203       |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 0.0519       |
------------------------------------------
Output 61: Average over 53 episodes - Reward: 0.6981132075471698
-------------------------------------------
| time/                   |               |
|    fps                  | 573           |
|    iterations           | 61            |
|    time_elapsed         | 217           |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.00063035614 |
|    clip_fraction        | 0.00679       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0525       |
|    explained_variance   | 0.102         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0298        |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 0.047         |
-------------------------------------------
Output 62: Average over 53 episodes - Reward: 0.7169811320754716
------------------------------------------
| time/                   |              |
|    fps                  | 573          |
|    iterations           | 62           |
|    time_elapsed         | 221          |
|    total_timesteps      | 126976       |
| train/                  |              |
|    approx_kl            | 0.0014875315 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0598      |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0176       |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00419     |
|    value_loss           | 0.038        |
------------------------------------------
Output 63: Average over 51 episodes - Reward: 0.6470588235294118
-------------------------------------------
| time/                   |               |
|    fps                  | 573           |
|    iterations           | 63            |
|    time_elapsed         | 225           |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.00043116155 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0583       |
|    explained_variance   | 0.219         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0257        |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.00156      |
|    value_loss           | 0.0394        |
-------------------------------------------
Output 64: Average over 46 episodes - Reward: 0.6739130434782609
------------------------------------------
| time/                   |              |
|    fps                  | 573          |
|    iterations           | 64           |
|    time_elapsed         | 228          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0010812697 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0576      |
|    explained_variance   | 0.0182       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0175       |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 0.0447       |
------------------------------------------
Output 65: Average over 57 episodes - Reward: 0.631578947368421
------------------------------------------
| time/                   |              |
|    fps                  | 572          |
|    iterations           | 65           |
|    time_elapsed         | 232          |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.0016664534 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0512      |
|    explained_variance   | 0.178        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00742      |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 0.0346       |
------------------------------------------
Output 66: Average over 52 episodes - Reward: 0.8076923076923077
-----------------------------------------
| time/                   |             |
|    fps                  | 572         |
|    iterations           | 66          |
|    time_elapsed         | 235         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.002765651 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0502     |
|    explained_variance   | 0.0741      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0304      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 0.0517      |
-----------------------------------------
Output 67: Average over 53 episodes - Reward: 0.6981132075471698
------------------------------------------
| time/                   |              |
|    fps                  | 572          |
|    iterations           | 67           |
|    time_elapsed         | 239          |
|    total_timesteps      | 137216       |
| train/                  |              |
|    approx_kl            | 0.0002811646 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0435      |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0127       |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.000854    |
|    value_loss           | 0.0293       |
------------------------------------------
Output 68: Average over 51 episodes - Reward: 0.7647058823529411
------------------------------------------
| time/                   |              |
|    fps                  | 572          |
|    iterations           | 68           |
|    time_elapsed         | 243          |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0007429962 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.044       |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0275       |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.000886    |
|    value_loss           | 0.0457       |
------------------------------------------
Output 69: Average over 48 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 572          |
|    iterations           | 69           |
|    time_elapsed         | 246          |
|    total_timesteps      | 141312       |
| train/                  |              |
|    approx_kl            | 0.0002786439 |
|    clip_fraction        | 0.00322      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0419      |
|    explained_variance   | 0.178        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0221       |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00032     |
|    value_loss           | 0.0356       |
------------------------------------------
Output 70: Average over 46 episodes - Reward: 0.6304347826086957
-----------------------------------------
| time/                   |             |
|    fps                  | 572         |
|    iterations           | 70          |
|    time_elapsed         | 250         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.000512972 |
|    clip_fraction        | 0.0041      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0282     |
|    explained_variance   | 0.211       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0136      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.000849   |
|    value_loss           | 0.0337      |
-----------------------------------------
Output 71: Average over 50 episodes - Reward: 0.78
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 71            |
|    time_elapsed         | 253           |
|    total_timesteps      | 145408        |
| train/                  |               |
|    approx_kl            | 0.00056233624 |
|    clip_fraction        | 0.00444       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0272       |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0194        |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000677     |
|    value_loss           | 0.0387        |
-------------------------------------------
Output 72: Average over 50 episodes - Reward: 0.64
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 72            |
|    time_elapsed         | 257           |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 0.00019190292 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0173        |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000429     |
|    value_loss           | 0.0347        |
-------------------------------------------
Output 73: Average over 49 episodes - Reward: 0.7959183673469388
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 73            |
|    time_elapsed         | 261           |
|    total_timesteps      | 149504        |
| train/                  |               |
|    approx_kl            | 0.00031313338 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0253       |
|    explained_variance   | 0.235         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0143        |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.000231     |
|    value_loss           | 0.0403        |
-------------------------------------------
Output 74: Average over 49 episodes - Reward: 0.4897959183673469
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 74            |
|    time_elapsed         | 264           |
|    total_timesteps      | 151552        |
| train/                  |               |
|    approx_kl            | 0.00040834444 |
|    clip_fraction        | 0.00273       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0285       |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0212        |
|    n_updates            | 730           |
|    policy_gradient_loss | -0.000425     |
|    value_loss           | 0.0375        |
-------------------------------------------
Output 75: Average over 59 episodes - Reward: 0.8135593220338984
-----------------------------------------
| time/                   |             |
|    fps                  | 572         |
|    iterations           | 75          |
|    time_elapsed         | 268         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.002209754 |
|    clip_fraction        | 0.01        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0344     |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0221      |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 0.0444      |
-----------------------------------------
Output 76: Average over 50 episodes - Reward: 0.64
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 76            |
|    time_elapsed         | 272           |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 0.00087461167 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0443       |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0218        |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.00215      |
|    value_loss           | 0.0439        |
-------------------------------------------
Output 77: Average over 48 episodes - Reward: 0.6666666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 77            |
|    time_elapsed         | 275           |
|    total_timesteps      | 157696        |
| train/                  |               |
|    approx_kl            | 0.00095800305 |
|    clip_fraction        | 0.00796       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0624       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0204        |
|    n_updates            | 760           |
|    policy_gradient_loss | -0.000883     |
|    value_loss           | 0.0418        |
-------------------------------------------
Output 78: Average over 46 episodes - Reward: 0.717391304347826
------------------------------------------
| time/                   |              |
|    fps                  | 572          |
|    iterations           | 78           |
|    time_elapsed         | 279          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0005649087 |
|    clip_fraction        | 0.00894      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0549      |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.015        |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.0358       |
------------------------------------------
Output 79: Average over 51 episodes - Reward: 0.6862745098039216
------------------------------------------
| time/                   |              |
|    fps                  | 572          |
|    iterations           | 79           |
|    time_elapsed         | 282          |
|    total_timesteps      | 161792       |
| train/                  |              |
|    approx_kl            | 0.0008125907 |
|    clip_fraction        | 0.00776      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0546      |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0158       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.0339       |
------------------------------------------
Output 80: Average over 47 episodes - Reward: 0.5957446808510638
------------------------------------------
| time/                   |              |
|    fps                  | 571          |
|    iterations           | 80           |
|    time_elapsed         | 286          |
|    total_timesteps      | 163840       |
| train/                  |              |
|    approx_kl            | 0.0009996962 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0713      |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0176       |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.000961    |
|    value_loss           | 0.0417       |
------------------------------------------
Output 81: Average over 53 episodes - Reward: 0.7924528301886793
-------------------------------------------
| time/                   |               |
|    fps                  | 571           |
|    iterations           | 81            |
|    time_elapsed         | 290           |
|    total_timesteps      | 165888        |
| train/                  |               |
|    approx_kl            | 0.00087090954 |
|    clip_fraction        | 0.00479       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0545       |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0295        |
|    n_updates            | 800           |
|    policy_gradient_loss | -0.00128      |
|    value_loss           | 0.039         |
-------------------------------------------
Output 82: Average over 49 episodes - Reward: 0.7551020408163265
-------------------------------------------
| time/                   |               |
|    fps                  | 570           |
|    iterations           | 82            |
|    time_elapsed         | 294           |
|    total_timesteps      | 167936        |
| train/                  |               |
|    approx_kl            | 0.00056931644 |
|    clip_fraction        | 0.00654       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0438       |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0198        |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.000946     |
|    value_loss           | 0.0384        |
-------------------------------------------
Output 83: Average over 48 episodes - Reward: 0.7083333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 569           |
|    iterations           | 83            |
|    time_elapsed         | 298           |
|    total_timesteps      | 169984        |
| train/                  |               |
|    approx_kl            | 0.00048534718 |
|    clip_fraction        | 0.00571       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0413       |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0109        |
|    n_updates            | 820           |
|    policy_gradient_loss | -0.000993     |
|    value_loss           | 0.0365        |
-------------------------------------------
Output 84: Average over 43 episodes - Reward: 0.5581395348837209
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 84            |
|    time_elapsed         | 302           |
|    total_timesteps      | 172032        |
| train/                  |               |
|    approx_kl            | 0.00013608247 |
|    clip_fraction        | 0.00415       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0406       |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0179        |
|    n_updates            | 830           |
|    policy_gradient_loss | -0.0004       |
|    value_loss           | 0.0375        |
-------------------------------------------
Output 85: Average over 48 episodes - Reward: 0.7083333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 85            |
|    time_elapsed         | 305           |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 0.00041888937 |
|    clip_fraction        | 0.00493       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0337       |
|    explained_variance   | 0.152         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00818       |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000788     |
|    value_loss           | 0.0305        |
-------------------------------------------
Output 86: Average over 48 episodes - Reward: 0.6666666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 86           |
|    time_elapsed         | 309          |
|    total_timesteps      | 176128       |
| train/                  |              |
|    approx_kl            | 0.0011330727 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.037       |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.014        |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 0.0374       |
------------------------------------------
Output 87: Average over 51 episodes - Reward: 0.7058823529411765
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 87           |
|    time_elapsed         | 313          |
|    total_timesteps      | 178176       |
| train/                  |              |
|    approx_kl            | 0.0006592063 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0259      |
|    explained_variance   | 0.19         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0195       |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 0.0417       |
------------------------------------------
Output 88: Average over 56 episodes - Reward: 0.6964285714285714
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 88            |
|    time_elapsed         | 317           |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 0.00016978238 |
|    clip_fraction        | 0.002         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0211       |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0157        |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 0.0414        |
-------------------------------------------
Output 89: Average over 49 episodes - Reward: 0.7755102040816326
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 89            |
|    time_elapsed         | 321           |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 0.00044719057 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0219       |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0149        |
|    n_updates            | 880           |
|    policy_gradient_loss | -0.000228     |
|    value_loss           | 0.044         |
-------------------------------------------
Output 90: Average over 56 episodes - Reward: 0.8392857142857143
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 90            |
|    time_elapsed         | 325           |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 0.00012543041 |
|    clip_fraction        | 0.00278       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0192        |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.000616     |
|    value_loss           | 0.0375        |
-------------------------------------------
Output 91: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 91            |
|    time_elapsed         | 328           |
|    total_timesteps      | 186368        |
| train/                  |               |
|    approx_kl            | 0.00014031472 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0257       |
|    explained_variance   | 0.247         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0182        |
|    n_updates            | 900           |
|    policy_gradient_loss | 0.000169      |
|    value_loss           | 0.0339        |
-------------------------------------------
Output 92: Average over 44 episodes - Reward: 0.7954545454545454
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 92            |
|    time_elapsed         | 332           |
|    total_timesteps      | 188416        |
| train/                  |               |
|    approx_kl            | 0.00014753445 |
|    clip_fraction        | 0.00278       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.02         |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0133        |
|    n_updates            | 910           |
|    policy_gradient_loss | -0.000229     |
|    value_loss           | 0.0343        |
-------------------------------------------
Output 93: Average over 53 episodes - Reward: 0.6981132075471698
------------------------------------------
| time/                   |              |
|    fps                  | 566          |
|    iterations           | 93           |
|    time_elapsed         | 336          |
|    total_timesteps      | 190464       |
| train/                  |              |
|    approx_kl            | 0.0012192301 |
|    clip_fraction        | 0.00776      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0177      |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0155       |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000953    |
|    value_loss           | 0.028        |
------------------------------------------
Output 94: Average over 52 episodes - Reward: 0.6153846153846154
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 94            |
|    time_elapsed         | 339           |
|    total_timesteps      | 192512        |
| train/                  |               |
|    approx_kl            | 0.00033328793 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 0.188         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0154        |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.000418     |
|    value_loss           | 0.0379        |
-------------------------------------------
Output 95: Average over 44 episodes - Reward: 0.6363636363636364
-----------------------------------------
| time/                   |             |
|    fps                  | 566         |
|    iterations           | 95          |
|    time_elapsed         | 343         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.000510911 |
|    clip_fraction        | 0.00532     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0185     |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00965     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00088    |
|    value_loss           | 0.0457      |
-----------------------------------------
Output 96: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 96            |
|    time_elapsed         | 347           |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00039175246 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0.0701        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00892       |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.000762     |
|    value_loss           | 0.0338        |
-------------------------------------------
Output 97: Average over 50 episodes - Reward: 0.7
------------------------------------------
| time/                   |              |
|    fps                  | 566          |
|    iterations           | 97           |
|    time_elapsed         | 350          |
|    total_timesteps      | 198656       |
| train/                  |              |
|    approx_kl            | 0.0011972953 |
|    clip_fraction        | 0.00698      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0159      |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0311       |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 0.0426       |
------------------------------------------
Output 98: Average over 55 episodes - Reward: 0.6909090909090909
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 98            |
|    time_elapsed         | 354           |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 0.00084737386 |
|    clip_fraction        | 0.00249       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0.22          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0175        |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.000646     |
|    value_loss           | 0.0363        |
-------------------------------------------
Output 99: Average over 49 episodes - Reward: 0.7346938775510204
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 99            |
|    time_elapsed         | 357           |
|    total_timesteps      | 202752        |
| train/                  |               |
|    approx_kl            | 0.00036394535 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00828      |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0221        |
|    n_updates            | 980           |
|    policy_gradient_loss | -0.000564     |
|    value_loss           | 0.0405        |
-------------------------------------------
Output 100: Average over 49 episodes - Reward: 0.6530612244897959
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 100           |
|    time_elapsed         | 361           |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 0.00031424363 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.0863        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0354        |
|    n_updates            | 990           |
|    policy_gradient_loss | -0.00036      |
|    value_loss           | 0.0408        |
-------------------------------------------
Output 101: Average over 47 episodes - Reward: 0.5957446808510638
------------------------------------------
| time/                   |              |
|    fps                  | 566          |
|    iterations           | 101          |
|    time_elapsed         | 364          |
|    total_timesteps      | 206848       |
| train/                  |              |
|    approx_kl            | 0.0006776056 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00733     |
|    explained_variance   | 0.0727       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0229       |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.000598    |
|    value_loss           | 0.0409       |
------------------------------------------
Output 102: Average over 48 episodes - Reward: 0.6041666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 566          |
|    iterations           | 102          |
|    time_elapsed         | 368          |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 7.976382e-05 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0064      |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0269       |
|    n_updates            | 1010         |
|    policy_gradient_loss | -7.22e-05    |
|    value_loss           | 0.0369       |
------------------------------------------
Output 103: Average over 47 episodes - Reward: 0.8085106382978723
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 103           |
|    time_elapsed         | 372           |
|    total_timesteps      | 210944        |
| train/                  |               |
|    approx_kl            | 0.00018341068 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00665      |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0239        |
|    n_updates            | 1020          |
|    policy_gradient_loss | -0.000188     |
|    value_loss           | 0.0406        |
-------------------------------------------
Output 104: Average over 53 episodes - Reward: 0.7547169811320755
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 104           |
|    time_elapsed         | 375           |
|    total_timesteps      | 212992        |
| train/                  |               |
|    approx_kl            | 2.1816231e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00482      |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0159        |
|    n_updates            | 1030          |
|    policy_gradient_loss | -4.88e-05     |
|    value_loss           | 0.0304        |
-------------------------------------------
Output 105: Average over 48 episodes - Reward: 0.8333333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 105           |
|    time_elapsed         | 379           |
|    total_timesteps      | 215040        |
| train/                  |               |
|    approx_kl            | 0.00021521104 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00529      |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0225        |
|    n_updates            | 1040          |
|    policy_gradient_loss | -0.000115     |
|    value_loss           | 0.04          |
-------------------------------------------
Output 106: Average over 46 episodes - Reward: 0.782608695652174
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 106          |
|    time_elapsed         | 382          |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 0.0001870963 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00628     |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0127       |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.000349    |
|    value_loss           | 0.0331       |
------------------------------------------
Output 107: Average over 45 episodes - Reward: 0.8222222222222222
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 107           |
|    time_elapsed         | 386           |
|    total_timesteps      | 219136        |
| train/                  |               |
|    approx_kl            | 0.00016983756 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00598      |
|    explained_variance   | 0.208         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00429       |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.000177     |
|    value_loss           | 0.0292        |
-------------------------------------------
Output 108: Average over 49 episodes - Reward: 0.8163265306122449
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 108           |
|    time_elapsed         | 389           |
|    total_timesteps      | 221184        |
| train/                  |               |
|    approx_kl            | 0.00032587335 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00423      |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0108        |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.000207     |
|    value_loss           | 0.026         |
-------------------------------------------
Output 109: Average over 50 episodes - Reward: 0.66
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 109          |
|    time_elapsed         | 393          |
|    total_timesteps      | 223232       |
| train/                  |              |
|    approx_kl            | 0.0010157045 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00668     |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0201       |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.000607    |
|    value_loss           | 0.029        |
------------------------------------------
Output 110: Average over 46 episodes - Reward: 0.6739130434782609
-----------------------------------------
| time/                   |             |
|    fps                  | 567         |
|    iterations           | 110         |
|    time_elapsed         | 396         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.002593034 |
|    clip_fraction        | 0.00747     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0229     |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0156      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.000388   |
|    value_loss           | 0.0395      |
-----------------------------------------
Output 111: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 111           |
|    time_elapsed         | 400           |
|    total_timesteps      | 227328        |
| train/                  |               |
|    approx_kl            | 0.00040575696 |
|    clip_fraction        | 0.00732       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.03         |
|    explained_variance   | 0.153         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0208        |
|    n_updates            | 1100          |
|    policy_gradient_loss | 7.44e-05      |
|    value_loss           | 0.0385        |
-------------------------------------------
Output 112: Average over 48 episodes - Reward: 0.6041666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 112          |
|    time_elapsed         | 404          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0004769207 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0265      |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0101       |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.000444    |
|    value_loss           | 0.0357       |
------------------------------------------
Output 113: Average over 54 episodes - Reward: 0.7222222222222222
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 113          |
|    time_elapsed         | 407          |
|    total_timesteps      | 231424       |
| train/                  |              |
|    approx_kl            | 0.0011054457 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0311      |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0142       |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 0.0448       |
------------------------------------------
Output 114: Average over 55 episodes - Reward: 0.6727272727272727
----------------------------------------
| time/                   |            |
|    fps                  | 567        |
|    iterations           | 114        |
|    time_elapsed         | 411        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.00304094 |
|    clip_fraction        | 0.0161     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0288    |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0327     |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.00261   |
|    value_loss           | 0.0407     |
----------------------------------------
Output 115: Average over 48 episodes - Reward: 0.7083333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 115           |
|    time_elapsed         | 414           |
|    total_timesteps      | 235520        |
| train/                  |               |
|    approx_kl            | 0.00087254855 |
|    clip_fraction        | 0.00933       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.042        |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0226        |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.000274     |
|    value_loss           | 0.0457        |
-------------------------------------------
Output 116: Average over 51 episodes - Reward: 0.6470588235294118
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 116          |
|    time_elapsed         | 418          |
|    total_timesteps      | 237568       |
| train/                  |              |
|    approx_kl            | 0.0006627308 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0231      |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0192       |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 0.0377       |
------------------------------------------
Output 117: Average over 50 episodes - Reward: 0.8
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 117          |
|    time_elapsed         | 421          |
|    total_timesteps      | 239616       |
| train/                  |              |
|    approx_kl            | 0.0005287107 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0247      |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.023        |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.000688    |
|    value_loss           | 0.041        |
------------------------------------------
Output 118: Average over 53 episodes - Reward: 0.7735849056603774
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 118           |
|    time_elapsed         | 425           |
|    total_timesteps      | 241664        |
| train/                  |               |
|    approx_kl            | 0.00012640242 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0152        |
|    n_updates            | 1170          |
|    policy_gradient_loss | -0.000212     |
|    value_loss           | 0.0321        |
-------------------------------------------
Output 119: Average over 50 episodes - Reward: 0.68
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 119           |
|    time_elapsed         | 428           |
|    total_timesteps      | 243712        |
| train/                  |               |
|    approx_kl            | 0.00033227136 |
|    clip_fraction        | 0.00454       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0253       |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0171        |
|    n_updates            | 1180          |
|    policy_gradient_loss | -0.000117     |
|    value_loss           | 0.0407        |
-------------------------------------------
Output 120: Average over 53 episodes - Reward: 0.6981132075471698
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 120           |
|    time_elapsed         | 432           |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.00067595334 |
|    clip_fraction        | 0.00977       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0206       |
|    explained_variance   | 0.0658        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0236        |
|    n_updates            | 1190          |
|    policy_gradient_loss | -0.00246      |
|    value_loss           | 0.0415        |
-------------------------------------------
Output 121: Average over 48 episodes - Reward: 0.7083333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 121           |
|    time_elapsed         | 435           |
|    total_timesteps      | 247808        |
| train/                  |               |
|    approx_kl            | 0.00088325253 |
|    clip_fraction        | 0.00952       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0283       |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00669      |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.00143      |
|    value_loss           | 0.0378        |
-------------------------------------------
Output 122: Average over 45 episodes - Reward: 0.8222222222222222
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 122           |
|    time_elapsed         | 439           |
|    total_timesteps      | 249856        |
| train/                  |               |
|    approx_kl            | 0.00031054474 |
|    clip_fraction        | 0.00435       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0148       |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0162        |
|    n_updates            | 1210          |
|    policy_gradient_loss | -0.00113      |
|    value_loss           | 0.0331        |
-------------------------------------------
Output 123: Average over 48 episodes - Reward: 0.7291666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 123          |
|    time_elapsed         | 442          |
|    total_timesteps      | 251904       |
| train/                  |              |
|    approx_kl            | 0.0003038453 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0152      |
|    explained_variance   | 0.284        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0182       |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.000316    |
|    value_loss           | 0.0278       |
------------------------------------------
Output 124: Average over 50 episodes - Reward: 0.68
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 124           |
|    time_elapsed         | 446           |
|    total_timesteps      | 253952        |
| train/                  |               |
|    approx_kl            | 0.00035776148 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0201       |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0143        |
|    n_updates            | 1230          |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 0.0347        |
-------------------------------------------
Output 125: Average over 36 episodes - Reward: 0.6111111111111112
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 125          |
|    time_elapsed         | 450          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.0005216509 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0166      |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0123       |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.000258    |
|    value_loss           | 0.0374       |
------------------------------------------
Output 126: Average over 48 episodes - Reward: 0.7291666666666666
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 126          |
|    time_elapsed         | 454          |
|    total_timesteps      | 258048       |
| train/                  |              |
|    approx_kl            | 0.0003410581 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0.0847       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0184       |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.000447    |
|    value_loss           | 0.031        |
------------------------------------------
Output 127: Average over 59 episodes - Reward: 0.7627118644067796
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 127          |
|    time_elapsed         | 457          |
|    total_timesteps      | 260096       |
| train/                  |              |
|    approx_kl            | 0.0008080896 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0145       |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.000915    |
|    value_loss           | 0.0354       |
------------------------------------------
Output 128: Average over 53 episodes - Reward: 0.7547169811320755
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 128           |
|    time_elapsed         | 461           |
|    total_timesteps      | 262144        |
| train/                  |               |
|    approx_kl            | 0.00043479018 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0257        |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.000431     |
|    value_loss           | 0.0439        |
-------------------------------------------
Output 129: Average over 50 episodes - Reward: 0.76
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 129          |
|    time_elapsed         | 464          |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 0.0004165969 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0154      |
|    explained_variance   | 0.146        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0163       |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.000571    |
|    value_loss           | 0.0403       |
------------------------------------------
Output 130: Average over 51 episodes - Reward: 0.7843137254901961
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 130           |
|    time_elapsed         | 468           |
|    total_timesteps      | 266240        |
| train/                  |               |
|    approx_kl            | 0.00039965537 |
|    clip_fraction        | 0.0021        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0119       |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0143        |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.00063      |
|    value_loss           | 0.0383        |
-------------------------------------------
Output 131: Average over 45 episodes - Reward: 0.6444444444444445
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 131          |
|    time_elapsed         | 471          |
|    total_timesteps      | 268288       |
| train/                  |              |
|    approx_kl            | 0.0002432767 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0102      |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.02         |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.000292    |
|    value_loss           | 0.0287       |
------------------------------------------
Output 132: Average over 54 episodes - Reward: 0.7592592592592593
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 132           |
|    time_elapsed         | 475           |
|    total_timesteps      | 270336        |
| train/                  |               |
|    approx_kl            | 0.00046433407 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0072       |
|    explained_variance   | 0.22          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0222        |
|    n_updates            | 1310          |
|    policy_gradient_loss | -0.000244     |
|    value_loss           | 0.0282        |
-------------------------------------------
Output 133: Average over 52 episodes - Reward: 0.6923076923076923
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 133          |
|    time_elapsed         | 478          |
|    total_timesteps      | 272384       |
| train/                  |              |
|    approx_kl            | 0.0013336991 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.219        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0101       |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 0.0364       |
------------------------------------------
Output 134: Average over 48 episodes - Reward: 0.6875
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 134           |
|    time_elapsed         | 482           |
|    total_timesteps      | 274432        |
| train/                  |               |
|    approx_kl            | 0.00056532584 |
|    clip_fraction        | 0.00796       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0232       |
|    explained_variance   | 0.138         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0217        |
|    n_updates            | 1330          |
|    policy_gradient_loss | -0.000556     |
|    value_loss           | 0.0423        |
-------------------------------------------
Output 135: Average over 52 episodes - Reward: 0.6538461538461539
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 135           |
|    time_elapsed         | 485           |
|    total_timesteps      | 276480        |
| train/                  |               |
|    approx_kl            | 0.00027244329 |
|    clip_fraction        | 0.00161       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0147        |
|    n_updates            | 1340          |
|    policy_gradient_loss | -0.000337     |
|    value_loss           | 0.0329        |
-------------------------------------------
Output 136: Average over 53 episodes - Reward: 0.6981132075471698
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 136          |
|    time_elapsed         | 489          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0025859487 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0182      |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.033        |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.000807    |
|    value_loss           | 0.0401       |
------------------------------------------
Output 137: Average over 43 episodes - Reward: 0.6976744186046512
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 137          |
|    time_elapsed         | 493          |
|    total_timesteps      | 280576       |
| train/                  |              |
|    approx_kl            | 0.0007037554 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0286      |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0214       |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 0.0374       |
------------------------------------------
Output 138: Average over 53 episodes - Reward: 0.660377358490566
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 138          |
|    time_elapsed         | 496          |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0005622427 |
|    clip_fraction        | 0.00737      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0301      |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0293       |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.000579    |
|    value_loss           | 0.0349       |
------------------------------------------
Output 139: Average over 48 episodes - Reward: 0.6458333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 139           |
|    time_elapsed         | 500           |
|    total_timesteps      | 284672        |
| train/                  |               |
|    approx_kl            | 0.00041470292 |
|    clip_fraction        | 0.0061        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0206       |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0163        |
|    n_updates            | 1380          |
|    policy_gradient_loss | -0.00159      |
|    value_loss           | 0.0431        |
-------------------------------------------
Output 140: Average over 52 episodes - Reward: 0.6730769230769231
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 140           |
|    time_elapsed         | 504           |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 0.00067931524 |
|    clip_fraction        | 0.00679       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0215       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.016         |
|    n_updates            | 1390          |
|    policy_gradient_loss | -0.00027      |
|    value_loss           | 0.0384        |
-------------------------------------------
Output 141: Average over 55 episodes - Reward: 0.6363636363636364
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 141           |
|    time_elapsed         | 507           |
|    total_timesteps      | 288768        |
| train/                  |               |
|    approx_kl            | 0.00014870687 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0229       |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0198        |
|    n_updates            | 1400          |
|    policy_gradient_loss | 0.000183      |
|    value_loss           | 0.0414        |
-------------------------------------------
Output 142: Average over 57 episodes - Reward: 0.7368421052631579
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 142          |
|    time_elapsed         | 511          |
|    total_timesteps      | 290816       |
| train/                  |              |
|    approx_kl            | 0.0015894789 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0248      |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0088       |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00236     |
|    value_loss           | 0.0451       |
------------------------------------------
Output 143: Average over 67 episodes - Reward: 0.7611940298507462
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 143          |
|    time_elapsed         | 515          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0005810471 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.029       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00499      |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.000561    |
|    value_loss           | 0.0446       |
------------------------------------------
Output 144: Average over 46 episodes - Reward: 0.6521739130434783
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 144           |
|    time_elapsed         | 518           |
|    total_timesteps      | 294912        |
| train/                  |               |
|    approx_kl            | 0.00037609122 |
|    clip_fraction        | 0.00762       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0193       |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0311        |
|    n_updates            | 1430          |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 0.0503        |
-------------------------------------------
Output 145: Average over 48 episodes - Reward: 0.6458333333333334
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 145          |
|    time_elapsed         | 522          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0003497127 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0173      |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0193       |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.000694    |
|    value_loss           | 0.0367       |
------------------------------------------
Output 146: Average over 52 episodes - Reward: 0.7692307692307693
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 146          |
|    time_elapsed         | 525          |
|    total_timesteps      | 299008       |
| train/                  |              |
|    approx_kl            | 7.570791e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0207       |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.000254    |
|    value_loss           | 0.0387       |
------------------------------------------
Output 147: Average over 64 episodes - Reward: 0.6875
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 147          |
|    time_elapsed         | 529          |
|    total_timesteps      | 301056       |
| train/                  |              |
|    approx_kl            | 0.0006490729 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0109      |
|    explained_variance   | 0.191        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0211       |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.000581    |
|    value_loss           | 0.0364       |
------------------------------------------
Output 148: Average over 46 episodes - Reward: 0.782608695652174
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 148           |
|    time_elapsed         | 532           |
|    total_timesteps      | 303104        |
| train/                  |               |
|    approx_kl            | 0.00081355846 |
|    clip_fraction        | 0.00361       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0117       |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0405        |
|    n_updates            | 1470          |
|    policy_gradient_loss | -0.00114      |
|    value_loss           | 0.0516        |
-------------------------------------------
Output 149: Average over 51 episodes - Reward: 0.7843137254901961
-----------------------------------------
| time/                   |             |
|    fps                  | 569         |
|    iterations           | 149         |
|    time_elapsed         | 536         |
|    total_timesteps      | 305152      |
| train/                  |             |
|    approx_kl            | 0.001211207 |
|    clip_fraction        | 0.00303     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0089     |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0171      |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 0.0323      |
-----------------------------------------
Output 150: Average over 54 episodes - Reward: 0.6296296296296297
------------------------------------------
| time/                   |              |
|    fps                  | 569          |
|    iterations           | 150          |
|    time_elapsed         | 539          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 9.869257e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00566     |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0113       |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.000352    |
|    value_loss           | 0.0325       |
------------------------------------------
Output 151: Average over 60 episodes - Reward: 0.65
-------------------------------------------
| time/                   |               |
|    fps                  | 569           |
|    iterations           | 151           |
|    time_elapsed         | 543           |
|    total_timesteps      | 309248        |
| train/                  |               |
|    approx_kl            | 6.5168715e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00639      |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0201        |
|    n_updates            | 1500          |
|    policy_gradient_loss | -0.000349     |
|    value_loss           | 0.0432        |
-------------------------------------------
Output 152: Average over 41 episodes - Reward: 0.7317073170731707
------------------------------------------
| time/                   |              |
|    fps                  | 569          |
|    iterations           | 152          |
|    time_elapsed         | 547          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0010357611 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0148      |
|    explained_variance   | 0.233        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0165       |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.000352    |
|    value_loss           | 0.0445       |
------------------------------------------
Output 153: Average over 46 episodes - Reward: 0.7608695652173914
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 153          |
|    time_elapsed         | 551          |
|    total_timesteps      | 313344       |
| train/                  |              |
|    approx_kl            | 0.0010058131 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0193      |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0156       |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.000685    |
|    value_loss           | 0.0356       |
------------------------------------------
Output 154: Average over 50 episodes - Reward: 0.74
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 154           |
|    time_elapsed         | 554           |
|    total_timesteps      | 315392        |
| train/                  |               |
|    approx_kl            | 0.00028778886 |
|    clip_fraction        | 0.00464       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0132       |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0195        |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.00138      |
|    value_loss           | 0.033         |
-------------------------------------------
Output 155: Average over 46 episodes - Reward: 0.6956521739130435
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 155          |
|    time_elapsed         | 558          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0003184912 |
|    clip_fraction        | 0.00493      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.014       |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0192       |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 0.0348       |
------------------------------------------
Output 156: Average over 46 episodes - Reward: 0.6086956521739131
-----------------------------------------
| time/                   |             |
|    fps                  | 568         |
|    iterations           | 156         |
|    time_elapsed         | 562         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.002308763 |
|    clip_fraction        | 0.00601     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0168     |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0186      |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.000935   |
|    value_loss           | 0.0334      |
-----------------------------------------
Output 157: Average over 46 episodes - Reward: 0.6304347826086957
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 157          |
|    time_elapsed         | 565          |
|    total_timesteps      | 321536       |
| train/                  |              |
|    approx_kl            | 0.0012069024 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0258      |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0206       |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.000975    |
|    value_loss           | 0.0378       |
------------------------------------------
Output 158: Average over 53 episodes - Reward: 0.7924528301886793
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 158          |
|    time_elapsed         | 569          |
|    total_timesteps      | 323584       |
| train/                  |              |
|    approx_kl            | 0.0005006208 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0188      |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0237       |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.000467    |
|    value_loss           | 0.0415       |
------------------------------------------
Output 159: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 159           |
|    time_elapsed         | 573           |
|    total_timesteps      | 325632        |
| train/                  |               |
|    approx_kl            | 0.00044126695 |
|    clip_fraction        | 0.00342       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0157        |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.00072      |
|    value_loss           | 0.0388        |
-------------------------------------------
Output 160: Average over 49 episodes - Reward: 0.6530612244897959
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 160           |
|    time_elapsed         | 576           |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.00067222887 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.197         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0229        |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.000637     |
|    value_loss           | 0.0334        |
-------------------------------------------
Output 161: Average over 56 episodes - Reward: 0.7857142857142857
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 161           |
|    time_elapsed         | 580           |
|    total_timesteps      | 329728        |
| train/                  |               |
|    approx_kl            | 0.00029407482 |
|    clip_fraction        | 0.00454       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0227       |
|    explained_variance   | 0.0907        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0196        |
|    n_updates            | 1600          |
|    policy_gradient_loss | -0.000966     |
|    value_loss           | 0.0434        |
-------------------------------------------
Output 162: Average over 41 episodes - Reward: 0.7560975609756098
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 162           |
|    time_elapsed         | 583           |
|    total_timesteps      | 331776        |
| train/                  |               |
|    approx_kl            | 0.00012700359 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0117       |
|    explained_variance   | 0.133         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0203        |
|    n_updates            | 1610          |
|    policy_gradient_loss | -8.31e-05     |
|    value_loss           | 0.0386        |
-------------------------------------------
Output 163: Average over 52 episodes - Reward: 0.7692307692307693
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 163           |
|    time_elapsed         | 587           |
|    total_timesteps      | 333824        |
| train/                  |               |
|    approx_kl            | 0.00012721439 |
|    clip_fraction        | 0.00107       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0113        |
|    n_updates            | 1620          |
|    policy_gradient_loss | -0.000316     |
|    value_loss           | 0.0313        |
-------------------------------------------
Output 164: Average over 52 episodes - Reward: 0.7692307692307693
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 164           |
|    time_elapsed         | 591           |
|    total_timesteps      | 335872        |
| train/                  |               |
|    approx_kl            | 0.00039157487 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0132       |
|    explained_variance   | 0.181         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0154        |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.000464     |
|    value_loss           | 0.0361        |
-------------------------------------------
Output 165: Average over 41 episodes - Reward: 0.7073170731707317
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 165          |
|    time_elapsed         | 595          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0003186455 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0134      |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0131       |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.000753    |
|    value_loss           | 0.0349       |
------------------------------------------
Output 166: Average over 54 episodes - Reward: 0.7407407407407407
-----------------------------------------
| time/                   |             |
|    fps                  | 567         |
|    iterations           | 166         |
|    time_elapsed         | 598         |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.000280567 |
|    clip_fraction        | 0.00151     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.015      |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00864     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.000386   |
|    value_loss           | 0.0247      |
-----------------------------------------
Output 167: Average over 49 episodes - Reward: 0.7346938775510204
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 167          |
|    time_elapsed         | 602          |
|    total_timesteps      | 342016       |
| train/                  |              |
|    approx_kl            | 0.0013369406 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0166      |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0113       |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.000901    |
|    value_loss           | 0.0387       |
------------------------------------------
Output 168: Average over 51 episodes - Reward: 0.6862745098039216
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 168           |
|    time_elapsed         | 605           |
|    total_timesteps      | 344064        |
| train/                  |               |
|    approx_kl            | 0.00054440706 |
|    clip_fraction        | 0.00684       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0228        |
|    n_updates            | 1670          |
|    policy_gradient_loss | -0.00034      |
|    value_loss           | 0.0383        |
-------------------------------------------
Output 169: Average over 46 episodes - Reward: 0.6739130434782609
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 169          |
|    time_elapsed         | 608          |
|    total_timesteps      | 346112       |
| train/                  |              |
|    approx_kl            | 0.0006984146 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0207      |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0161       |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 0.04         |
------------------------------------------
Output 170: Average over 55 episodes - Reward: 0.7272727272727273
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 170           |
|    time_elapsed         | 612           |
|    total_timesteps      | 348160        |
| train/                  |               |
|    approx_kl            | 0.00031782483 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0227        |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.000568     |
|    value_loss           | 0.0382        |
-------------------------------------------
Output 171: Average over 49 episodes - Reward: 0.7346938775510204
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 171          |
|    time_elapsed         | 616          |
|    total_timesteps      | 350208       |
| train/                  |              |
|    approx_kl            | 0.0004994272 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0153      |
|    explained_variance   | 0.152        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0173       |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00059     |
|    value_loss           | 0.0452       |
------------------------------------------
Output 172: Average over 49 episodes - Reward: 0.673469387755102
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 172          |
|    time_elapsed         | 619          |
|    total_timesteps      | 352256       |
| train/                  |              |
|    approx_kl            | 0.0002810109 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0124      |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0273       |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.000533    |
|    value_loss           | 0.0351       |
------------------------------------------
Output 173: Average over 43 episodes - Reward: 0.6976744186046512
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 173          |
|    time_elapsed         | 623          |
|    total_timesteps      | 354304       |
| train/                  |              |
|    approx_kl            | 8.714985e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.01        |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0214       |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.000284    |
|    value_loss           | 0.0376       |
------------------------------------------
Output 174: Average over 42 episodes - Reward: 0.6904761904761905
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 174           |
|    time_elapsed         | 627           |
|    total_timesteps      | 356352        |
| train/                  |               |
|    approx_kl            | 0.00049272616 |
|    clip_fraction        | 0.0082        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.0941        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0189        |
|    n_updates            | 1730          |
|    policy_gradient_loss | -0.000976     |
|    value_loss           | 0.033         |
-------------------------------------------
Output 175: Average over 53 episodes - Reward: 0.7358490566037735
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 175           |
|    time_elapsed         | 630           |
|    total_timesteps      | 358400        |
| train/                  |               |
|    approx_kl            | 0.00026329255 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00808      |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0151        |
|    n_updates            | 1740          |
|    policy_gradient_loss | -0.000575     |
|    value_loss           | 0.0282        |
-------------------------------------------
Output 176: Average over 52 episodes - Reward: 0.7307692307692307
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 176           |
|    time_elapsed         | 634           |
|    total_timesteps      | 360448        |
| train/                  |               |
|    approx_kl            | 0.00020913477 |
|    clip_fraction        | 0.00396       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00872      |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0161        |
|    n_updates            | 1750          |
|    policy_gradient_loss | -0.00115      |
|    value_loss           | 0.0402        |
-------------------------------------------
Output 177: Average over 52 episodes - Reward: 0.8461538461538461
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 177          |
|    time_elapsed         | 637          |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0004811451 |
|    clip_fraction        | 0.00669      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.017       |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0192       |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 0.0364       |
------------------------------------------
Output 178: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 178           |
|    time_elapsed         | 641           |
|    total_timesteps      | 364544        |
| train/                  |               |
|    approx_kl            | 0.00016507081 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0121       |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0144        |
|    n_updates            | 1770          |
|    policy_gradient_loss | -0.000393     |
|    value_loss           | 0.0324        |
-------------------------------------------
Output 179: Average over 62 episodes - Reward: 0.7580645161290323
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 179           |
|    time_elapsed         | 645           |
|    total_timesteps      | 366592        |
| train/                  |               |
|    approx_kl            | 0.00090112805 |
|    clip_fraction        | 0.00405       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00832      |
|    explained_variance   | 0.197         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00748       |
|    n_updates            | 1780          |
|    policy_gradient_loss | -0.00109      |
|    value_loss           | 0.0371        |
-------------------------------------------
Output 180: Average over 48 episodes - Reward: 0.6666666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 180           |
|    time_elapsed         | 648           |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.00031894207 |
|    clip_fraction        | 0.00532       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0156        |
|    n_updates            | 1790          |
|    policy_gradient_loss | -0.000506     |
|    value_loss           | 0.0437        |
-------------------------------------------
Output 181: Average over 53 episodes - Reward: 0.7169811320754716
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 181           |
|    time_elapsed         | 652           |
|    total_timesteps      | 370688        |
| train/                  |               |
|    approx_kl            | 0.00023712509 |
|    clip_fraction        | 0.00244       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0201       |
|    explained_variance   | 0.242         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0176        |
|    n_updates            | 1800          |
|    policy_gradient_loss | -8.44e-05     |
|    value_loss           | 0.0363        |
-------------------------------------------
Output 182: Average over 51 episodes - Reward: 0.7254901960784313
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 182          |
|    time_elapsed         | 656          |
|    total_timesteps      | 372736       |
| train/                  |              |
|    approx_kl            | 0.0012699286 |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0316      |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0195       |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.000942    |
|    value_loss           | 0.0425       |
------------------------------------------
Output 183: Average over 48 episodes - Reward: 0.7708333333333334
------------------------------------------
| time/                   |              |
|    fps                  | 567          |
|    iterations           | 183          |
|    time_elapsed         | 660          |
|    total_timesteps      | 374784       |
| train/                  |              |
|    approx_kl            | 0.0012602247 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00996     |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.024        |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 0.0399       |
------------------------------------------
Output 184: Average over 50 episodes - Reward: 0.76
-------------------------------------------
| time/                   |               |
|    fps                  | 567           |
|    iterations           | 184           |
|    time_elapsed         | 663           |
|    total_timesteps      | 376832        |
| train/                  |               |
|    approx_kl            | 0.00040087942 |
|    clip_fraction        | 0.00552       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00337       |
|    n_updates            | 1830          |
|    policy_gradient_loss | -0.00083      |
|    value_loss           | 0.0273        |
-------------------------------------------
Output 185: Average over 44 episodes - Reward: 0.7045454545454546
-----------------------------------------
| time/                   |             |
|    fps                  | 568         |
|    iterations           | 185         |
|    time_elapsed         | 666         |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.001574667 |
|    clip_fraction        | 0.00562     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0201     |
|    explained_variance   | 0.156       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0126      |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 0.0383      |
-----------------------------------------
Output 186: Average over 54 episodes - Reward: 0.7037037037037037
-------------------------------------------
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 186           |
|    time_elapsed         | 670           |
|    total_timesteps      | 380928        |
| train/                  |               |
|    approx_kl            | 0.00081529166 |
|    clip_fraction        | 0.00557       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0201       |
|    explained_variance   | 0.126         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00991       |
|    n_updates            | 1850          |
|    policy_gradient_loss | -0.000335     |
|    value_loss           | 0.0315        |
-------------------------------------------
Output 187: Average over 48 episodes - Reward: 0.5833333333333334
------------------------------------------
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 187          |
|    time_elapsed         | 673          |
|    total_timesteps      | 382976       |
| train/                  |              |
|    approx_kl            | 0.0006172149 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0221      |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0283       |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.000894    |
|    value_loss           | 0.0409       |
------------------------------------------
Output 188: Average over 49 episodes - Reward: 0.6938775510204082
----------------------------------------
| time/                   |            |
|    fps                  | 569        |
|    iterations           | 188        |
|    time_elapsed         | 676        |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.00493148 |
|    clip_fraction        | 0.00898    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0243    |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0134     |
|    n_updates            | 1870       |
|    policy_gradient_loss | 0.000672   |
|    value_loss           | 0.0383     |
----------------------------------------
Output 189: Average over 54 episodes - Reward: 0.6851851851851852
------------------------------------------
| time/                   |              |
|    fps                  | 569          |
|    iterations           | 189          |
|    time_elapsed         | 679          |
|    total_timesteps      | 387072       |
| train/                  |              |
|    approx_kl            | 0.0038213371 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0268      |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00477      |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00338     |
|    value_loss           | 0.0363       |
------------------------------------------
Output 190: Average over 43 episodes - Reward: 0.7209302325581395
------------------------------------------
| time/                   |              |
|    fps                  | 569          |
|    iterations           | 190          |
|    time_elapsed         | 683          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.0017725977 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0258      |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0113       |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.000899    |
|    value_loss           | 0.0423       |
------------------------------------------
Output 191: Average over 46 episodes - Reward: 0.717391304347826
-------------------------------------------
| time/                   |               |
|    fps                  | 569           |
|    iterations           | 191           |
|    time_elapsed         | 686           |
|    total_timesteps      | 391168        |
| train/                  |               |
|    approx_kl            | 0.00022266174 |
|    clip_fraction        | 0.0062        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0282       |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0005        |
|    n_updates            | 1900          |
|    policy_gradient_loss | -5.46e-05     |
|    value_loss           | 0.0308        |
-------------------------------------------
Output 192: Average over 50 episodes - Reward: 0.58
------------------------------------------
| time/                   |              |
|    fps                  | 570          |
|    iterations           | 192          |
|    time_elapsed         | 689          |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0004531764 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0261      |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00958      |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.000707    |
|    value_loss           | 0.0366       |
------------------------------------------
Output 193: Average over 48 episodes - Reward: 0.6458333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 570           |
|    iterations           | 193           |
|    time_elapsed         | 692           |
|    total_timesteps      | 395264        |
| train/                  |               |
|    approx_kl            | 0.00043001818 |
|    clip_fraction        | 0.00742       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0332       |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.018         |
|    n_updates            | 1920          |
|    policy_gradient_loss | -0.000248     |
|    value_loss           | 0.0445        |
-------------------------------------------
Output 194: Average over 52 episodes - Reward: 0.6153846153846154
-------------------------------------------
| time/                   |               |
|    fps                  | 570           |
|    iterations           | 194           |
|    time_elapsed         | 696           |
|    total_timesteps      | 397312        |
| train/                  |               |
|    approx_kl            | 0.00043454193 |
|    clip_fraction        | 0.00684       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0256       |
|    explained_variance   | 0.175         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.014         |
|    n_updates            | 1930          |
|    policy_gradient_loss | -0.00142      |
|    value_loss           | 0.0401        |
-------------------------------------------
Output 195: Average over 53 episodes - Reward: 0.7169811320754716
------------------------------------------
| time/                   |              |
|    fps                  | 570          |
|    iterations           | 195          |
|    time_elapsed         | 699          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.0009288568 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.028       |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0226       |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00174     |
|    value_loss           | 0.0465       |
------------------------------------------
Output 196: Average over 52 episodes - Reward: 0.6153846153846154
------------------------------------------
| time/                   |              |
|    fps                  | 571          |
|    iterations           | 196          |
|    time_elapsed         | 702          |
|    total_timesteps      | 401408       |
| train/                  |              |
|    approx_kl            | 0.0013009063 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0305      |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00654      |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 0.0417       |
------------------------------------------
Output 197: Average over 55 episodes - Reward: 0.7818181818181819
-------------------------------------------
| time/                   |               |
|    fps                  | 571           |
|    iterations           | 197           |
|    time_elapsed         | 705           |
|    total_timesteps      | 403456        |
| train/                  |               |
|    approx_kl            | 0.00048079225 |
|    clip_fraction        | 0.00654       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0333       |
|    explained_variance   | 0.216         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0352        |
|    n_updates            | 1960          |
|    policy_gradient_loss | -0.00049      |
|    value_loss           | 0.042         |
-------------------------------------------
Output 198: Average over 54 episodes - Reward: 0.6851851851851852
-------------------------------------------
| time/                   |               |
|    fps                  | 571           |
|    iterations           | 198           |
|    time_elapsed         | 709           |
|    total_timesteps      | 405504        |
| train/                  |               |
|    approx_kl            | 0.00022498862 |
|    clip_fraction        | 0.00337       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0165        |
|    n_updates            | 1970          |
|    policy_gradient_loss | -0.000196     |
|    value_loss           | 0.0374        |
-------------------------------------------
Output 199: Average over 51 episodes - Reward: 0.6274509803921569
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 199           |
|    time_elapsed         | 712           |
|    total_timesteps      | 407552        |
| train/                  |               |
|    approx_kl            | 0.00030491065 |
|    clip_fraction        | 0.0064        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0245       |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0264        |
|    n_updates            | 1980          |
|    policy_gradient_loss | -0.00092      |
|    value_loss           | 0.0444        |
-------------------------------------------
Output 200: Average over 54 episodes - Reward: 0.7962962962962963
------------------------------------------
| time/                   |              |
|    fps                  | 572          |
|    iterations           | 200          |
|    time_elapsed         | 715          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0004007937 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.019       |
|    explained_variance   | 0.0874       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0327       |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 0.0446       |
------------------------------------------
Output 201: Average over 46 episodes - Reward: 0.6956521739130435
-------------------------------------------
| time/                   |               |
|    fps                  | 572           |
|    iterations           | 201           |
|    time_elapsed         | 718           |
|    total_timesteps      | 411648        |
| train/                  |               |
|    approx_kl            | 0.00020790476 |
|    clip_fraction        | 0.0042        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0.188         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0174        |
|    n_updates            | 2000          |
|    policy_gradient_loss | -0.000382     |
|    value_loss           | 0.0348        |
-------------------------------------------
Output 202: Average over 56 episodes - Reward: 0.6964285714285714
------------------------------------------
| time/                   |              |
|    fps                  | 573          |
|    iterations           | 202          |
|    time_elapsed         | 721          |
|    total_timesteps      | 413696       |
| train/                  |              |
|    approx_kl            | 0.0003509872 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0263      |
|    explained_variance   | 0.166        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0137       |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.000519    |
|    value_loss           | 0.0343       |
------------------------------------------
Output 203: Average over 57 episodes - Reward: 0.7894736842105263
-------------------------------------------
| time/                   |               |
|    fps                  | 573           |
|    iterations           | 203           |
|    time_elapsed         | 725           |
|    total_timesteps      | 415744        |
| train/                  |               |
|    approx_kl            | 0.00042823786 |
|    clip_fraction        | 0.00898       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0181       |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0159        |
|    n_updates            | 2020          |
|    policy_gradient_loss | -0.00241      |
|    value_loss           | 0.0425        |
-------------------------------------------
Output 204: Average over 43 episodes - Reward: 0.6511627906976745
------------------------------------------
| time/                   |              |
|    fps                  | 573          |
|    iterations           | 204          |
|    time_elapsed         | 728          |
|    total_timesteps      | 417792       |
| train/                  |              |
|    approx_kl            | 0.0002106987 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0178      |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0141       |
|    n_updates            | 2030         |
|    policy_gradient_loss | 4.05e-05     |
|    value_loss           | 0.0405       |
------------------------------------------
Output 205: Average over 44 episodes - Reward: 0.5909090909090909
-------------------------------------------
| time/                   |               |
|    fps                  | 573           |
|    iterations           | 205           |
|    time_elapsed         | 731           |
|    total_timesteps      | 419840        |
| train/                  |               |
|    approx_kl            | 0.00039844395 |
|    clip_fraction        | 0.00537       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0248       |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0216        |
|    n_updates            | 2040          |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 0.0359        |
-------------------------------------------
Output 206: Average over 54 episodes - Reward: 0.7037037037037037
-------------------------------------------
| time/                   |               |
|    fps                  | 574           |
|    iterations           | 206           |
|    time_elapsed         | 734           |
|    total_timesteps      | 421888        |
| train/                  |               |
|    approx_kl            | 0.00044396665 |
|    clip_fraction        | 0.00718       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0296        |
|    n_updates            | 2050          |
|    policy_gradient_loss | -0.001        |
|    value_loss           | 0.0405        |
-------------------------------------------
Output 207: Average over 46 episodes - Reward: 0.8478260869565217
-------------------------------------------
| time/                   |               |
|    fps                  | 574           |
|    iterations           | 207           |
|    time_elapsed         | 737           |
|    total_timesteps      | 423936        |
| train/                  |               |
|    approx_kl            | 0.00025884574 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0182       |
|    explained_variance   | 0.175         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0205        |
|    n_updates            | 2060          |
|    policy_gradient_loss | -0.00051      |
|    value_loss           | 0.0405        |
-------------------------------------------
Output 208: Average over 52 episodes - Reward: 0.7115384615384616
-------------------------------------------
| time/                   |               |
|    fps                  | 574           |
|    iterations           | 208           |
|    time_elapsed         | 741           |
|    total_timesteps      | 425984        |
| train/                  |               |
|    approx_kl            | 0.00018759584 |
|    clip_fraction        | 0.00205       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0127        |
|    n_updates            | 2070          |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 0.0248        |
-------------------------------------------
Output 209: Average over 39 episodes - Reward: 0.717948717948718
-------------------------------------------
| time/                   |               |
|    fps                  | 574           |
|    iterations           | 209           |
|    time_elapsed         | 744           |
|    total_timesteps      | 428032        |
| train/                  |               |
|    approx_kl            | 0.00020082289 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.022        |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0143        |
|    n_updates            | 2080          |
|    policy_gradient_loss | -0.000212     |
|    value_loss           | 0.0338        |
-------------------------------------------
Output 210: Average over 50 episodes - Reward: 0.82
-------------------------------------------
| time/                   |               |
|    fps                  | 575           |
|    iterations           | 210           |
|    time_elapsed         | 747           |
|    total_timesteps      | 430080        |
| train/                  |               |
|    approx_kl            | 0.00048002577 |
|    clip_fraction        | 0.00767       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0177       |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0086        |
|    n_updates            | 2090          |
|    policy_gradient_loss | -0.000639     |
|    value_loss           | 0.0255        |
-------------------------------------------
Output 211: Average over 50 episodes - Reward: 0.66
-------------------------------------------
| time/                   |               |
|    fps                  | 575           |
|    iterations           | 211           |
|    time_elapsed         | 750           |
|    total_timesteps      | 432128        |
| train/                  |               |
|    approx_kl            | 0.00036914094 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0.206         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0183        |
|    n_updates            | 2100          |
|    policy_gradient_loss | -0.000304     |
|    value_loss           | 0.028         |
-------------------------------------------
Output 212: Average over 46 episodes - Reward: 0.8043478260869565
------------------------------------------
| time/                   |              |
|    fps                  | 575          |
|    iterations           | 212          |
|    time_elapsed         | 754          |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 0.0007434521 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0232       |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 0.0361       |
------------------------------------------
Output 213: Average over 48 episodes - Reward: 0.75
-------------------------------------------
| time/                   |               |
|    fps                  | 576           |
|    iterations           | 213           |
|    time_elapsed         | 757           |
|    total_timesteps      | 436224        |
| train/                  |               |
|    approx_kl            | 0.00019489197 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00767      |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0161        |
|    n_updates            | 2120          |
|    policy_gradient_loss | -0.000533     |
|    value_loss           | 0.0278        |
-------------------------------------------
Output 214: Average over 45 episodes - Reward: 0.6888888888888889
-------------------------------------------
| time/                   |               |
|    fps                  | 576           |
|    iterations           | 214           |
|    time_elapsed         | 760           |
|    total_timesteps      | 438272        |
| train/                  |               |
|    approx_kl            | 0.00014262754 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00866      |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0118        |
|    n_updates            | 2130          |
|    policy_gradient_loss | -0.000354     |
|    value_loss           | 0.0326        |
-------------------------------------------
Output 215: Average over 49 episodes - Reward: 0.7551020408163265
-------------------------------------------
| time/                   |               |
|    fps                  | 576           |
|    iterations           | 215           |
|    time_elapsed         | 763           |
|    total_timesteps      | 440320        |
| train/                  |               |
|    approx_kl            | 5.0572475e-05 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.01         |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0181        |
|    n_updates            | 2140          |
|    policy_gradient_loss | 3.07e-06      |
|    value_loss           | 0.0353        |
-------------------------------------------
Output 216: Average over 48 episodes - Reward: 0.7291666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 576           |
|    iterations           | 216           |
|    time_elapsed         | 766           |
|    total_timesteps      | 442368        |
| train/                  |               |
|    approx_kl            | 0.00011075538 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0233        |
|    n_updates            | 2150          |
|    policy_gradient_loss | -0.000309     |
|    value_loss           | 0.034         |
-------------------------------------------
Output 217: Average over 52 episodes - Reward: 0.75
------------------------------------------
| time/                   |              |
|    fps                  | 577          |
|    iterations           | 217          |
|    time_elapsed         | 770          |
|    total_timesteps      | 444416       |
| train/                  |              |
|    approx_kl            | 8.307191e-05 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0.224        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0193       |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.000259    |
|    value_loss           | 0.0348       |
------------------------------------------
Output 218: Average over 54 episodes - Reward: 0.7592592592592593
-------------------------------------------
| time/                   |               |
|    fps                  | 577           |
|    iterations           | 218           |
|    time_elapsed         | 773           |
|    total_timesteps      | 446464        |
| train/                  |               |
|    approx_kl            | 0.00029883263 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00986      |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0162        |
|    n_updates            | 2170          |
|    policy_gradient_loss | -0.000178     |
|    value_loss           | 0.036         |
-------------------------------------------
Output 219: Average over 46 episodes - Reward: 0.6956521739130435
------------------------------------------
| time/                   |              |
|    fps                  | 577          |
|    iterations           | 219          |
|    time_elapsed         | 776          |
|    total_timesteps      | 448512       |
| train/                  |              |
|    approx_kl            | 0.0005422255 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0162       |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00053     |
|    value_loss           | 0.0395       |
------------------------------------------
Output 220: Average over 53 episodes - Reward: 0.7358490566037735
-------------------------------------------
| time/                   |               |
|    fps                  | 577           |
|    iterations           | 220           |
|    time_elapsed         | 779           |
|    total_timesteps      | 450560        |
| train/                  |               |
|    approx_kl            | 0.00024958738 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0167       |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0192        |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000452     |
|    value_loss           | 0.0349        |
-------------------------------------------
Output 221: Average over 45 episodes - Reward: 0.6444444444444445
-------------------------------------------
| time/                   |               |
|    fps                  | 577           |
|    iterations           | 221           |
|    time_elapsed         | 783           |
|    total_timesteps      | 452608        |
| train/                  |               |
|    approx_kl            | 0.00037843717 |
|    clip_fraction        | 0.00381       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00954      |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0276        |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.000363     |
|    value_loss           | 0.0366        |
-------------------------------------------
Output 222: Average over 52 episodes - Reward: 0.6923076923076923
-------------------------------------------
| time/                   |               |
|    fps                  | 578           |
|    iterations           | 222           |
|    time_elapsed         | 786           |
|    total_timesteps      | 454656        |
| train/                  |               |
|    approx_kl            | 0.00026269603 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0164        |
|    n_updates            | 2210          |
|    policy_gradient_loss | 1.98e-05      |
|    value_loss           | 0.0346        |
-------------------------------------------
Output 223: Average over 54 episodes - Reward: 0.7407407407407407
------------------------------------------
| time/                   |              |
|    fps                  | 578          |
|    iterations           | 223          |
|    time_elapsed         | 789          |
|    total_timesteps      | 456704       |
| train/                  |              |
|    approx_kl            | 0.0007835609 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0171      |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0114       |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 0.0392       |
------------------------------------------
Output 224: Average over 49 episodes - Reward: 0.6938775510204082
-----------------------------------------
| time/                   |             |
|    fps                  | 578         |
|    iterations           | 224         |
|    time_elapsed         | 793         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.002112319 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0121     |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0234      |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 0.0432      |
-----------------------------------------
Output 225: Average over 49 episodes - Reward: 0.6938775510204082
-----------------------------------------
| time/                   |             |
|    fps                  | 578         |
|    iterations           | 225         |
|    time_elapsed         | 796         |
|    total_timesteps      | 460800      |
| train/                  |             |
|    approx_kl            | 0.000362787 |
|    clip_fraction        | 0.00693     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.019      |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0204      |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.000533   |
|    value_loss           | 0.0375      |
-----------------------------------------
Output 226: Average over 54 episodes - Reward: 0.7962962962962963
-------------------------------------------
| time/                   |               |
|    fps                  | 578           |
|    iterations           | 226           |
|    time_elapsed         | 799           |
|    total_timesteps      | 462848        |
| train/                  |               |
|    approx_kl            | 0.00041701502 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0192       |
|    explained_variance   | 0.149         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.02          |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.000615     |
|    value_loss           | 0.04          |
-------------------------------------------
Output 227: Average over 47 episodes - Reward: 0.6170212765957447
-------------------------------------------
| time/                   |               |
|    fps                  | 579           |
|    iterations           | 227           |
|    time_elapsed         | 802           |
|    total_timesteps      | 464896        |
| train/                  |               |
|    approx_kl            | 0.00023098913 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.26          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0179        |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.000495     |
|    value_loss           | 0.0329        |
-------------------------------------------
Output 228: Average over 55 episodes - Reward: 0.6909090909090909
-------------------------------------------
| time/                   |               |
|    fps                  | 579           |
|    iterations           | 228           |
|    time_elapsed         | 806           |
|    total_timesteps      | 466944        |
| train/                  |               |
|    approx_kl            | 0.00042521983 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.02         |
|    explained_variance   | 0.0951        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0254        |
|    n_updates            | 2270          |
|    policy_gradient_loss | -0.000275     |
|    value_loss           | 0.0375        |
-------------------------------------------
Output 229: Average over 51 episodes - Reward: 0.7254901960784313
-------------------------------------------
| time/                   |               |
|    fps                  | 579           |
|    iterations           | 229           |
|    time_elapsed         | 809           |
|    total_timesteps      | 468992        |
| train/                  |               |
|    approx_kl            | 0.00041893544 |
|    clip_fraction        | 0.00635       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0333       |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0215        |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.000443     |
|    value_loss           | 0.0414        |
-------------------------------------------
Output 230: Average over 53 episodes - Reward: 0.7358490566037735
-------------------------------------------
| time/                   |               |
|    fps                  | 579           |
|    iterations           | 230           |
|    time_elapsed         | 812           |
|    total_timesteps      | 471040        |
| train/                  |               |
|    approx_kl            | 0.00045841545 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0269       |
|    explained_variance   | 0.176         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0129        |
|    n_updates            | 2290          |
|    policy_gradient_loss | -0.00227      |
|    value_loss           | 0.0363        |
-------------------------------------------
Output 231: Average over 45 episodes - Reward: 0.7333333333333333
------------------------------------------
| time/                   |              |
|    fps                  | 579          |
|    iterations           | 231          |
|    time_elapsed         | 815          |
|    total_timesteps      | 473088       |
| train/                  |              |
|    approx_kl            | 0.0005784326 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0313      |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0249       |
|    n_updates            | 2300         |
|    policy_gradient_loss | -8.83e-06    |
|    value_loss           | 0.0413       |
------------------------------------------
Output 232: Average over 50 episodes - Reward: 0.64
-------------------------------------------
| time/                   |               |
|    fps                  | 580           |
|    iterations           | 232           |
|    time_elapsed         | 818           |
|    total_timesteps      | 475136        |
| train/                  |               |
|    approx_kl            | 0.00045075448 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0301       |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.011         |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.000713     |
|    value_loss           | 0.0342        |
-------------------------------------------
Output 233: Average over 47 episodes - Reward: 0.6595744680851063
-------------------------------------------
| time/                   |               |
|    fps                  | 580           |
|    iterations           | 233           |
|    time_elapsed         | 822           |
|    total_timesteps      | 477184        |
| train/                  |               |
|    approx_kl            | 0.00050693436 |
|    clip_fraction        | 0.00522       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0261       |
|    explained_variance   | 0.0798        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0192        |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.000344     |
|    value_loss           | 0.0442        |
-------------------------------------------
Output 234: Average over 54 episodes - Reward: 0.7777777777777778
-------------------------------------------
| time/                   |               |
|    fps                  | 580           |
|    iterations           | 234           |
|    time_elapsed         | 825           |
|    total_timesteps      | 479232        |
| train/                  |               |
|    approx_kl            | 0.00010278783 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0364       |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0211        |
|    n_updates            | 2330          |
|    policy_gradient_loss | 0.000373      |
|    value_loss           | 0.0391        |
-------------------------------------------
Output 235: Average over 57 episodes - Reward: 0.6842105263157895
------------------------------------------
| time/                   |              |
|    fps                  | 580          |
|    iterations           | 235          |
|    time_elapsed         | 828          |
|    total_timesteps      | 481280       |
| train/                  |              |
|    approx_kl            | 0.0011482086 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0299      |
|    explained_variance   | 0.0956       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0178       |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.000532    |
|    value_loss           | 0.0402       |
------------------------------------------
Output 236: Average over 47 episodes - Reward: 0.7021276595744681
------------------------------------------
| time/                   |              |
|    fps                  | 580          |
|    iterations           | 236          |
|    time_elapsed         | 831          |
|    total_timesteps      | 483328       |
| train/                  |              |
|    approx_kl            | 0.0008445576 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0224      |
|    explained_variance   | 0.19         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0162       |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 0.0398       |
------------------------------------------
Output 237: Average over 46 episodes - Reward: 0.6956521739130435
-------------------------------------------
| time/                   |               |
|    fps                  | 581           |
|    iterations           | 237           |
|    time_elapsed         | 835           |
|    total_timesteps      | 485376        |
| train/                  |               |
|    approx_kl            | 0.00041323568 |
|    clip_fraction        | 0.00654       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0193       |
|    explained_variance   | 0.208         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0282        |
|    n_updates            | 2360          |
|    policy_gradient_loss | -0.00144      |
|    value_loss           | 0.0325        |
-------------------------------------------
Output 238: Average over 48 episodes - Reward: 0.7083333333333334
------------------------------------------
| time/                   |              |
|    fps                  | 581          |
|    iterations           | 238          |
|    time_elapsed         | 838          |
|    total_timesteps      | 487424       |
| train/                  |              |
|    approx_kl            | 0.0007161294 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0156      |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0211       |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 0.0352       |
------------------------------------------
Output 239: Average over 44 episodes - Reward: 0.7727272727272727
-------------------------------------------
| time/                   |               |
|    fps                  | 581           |
|    iterations           | 239           |
|    time_elapsed         | 841           |
|    total_timesteps      | 489472        |
| train/                  |               |
|    approx_kl            | 0.00023564315 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.029        |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0181        |
|    n_updates            | 2380          |
|    policy_gradient_loss | -2.49e-05     |
|    value_loss           | 0.0342        |
-------------------------------------------
Output 240: Average over 51 episodes - Reward: 0.7450980392156863
-------------------------------------------
| time/                   |               |
|    fps                  | 581           |
|    iterations           | 240           |
|    time_elapsed         | 844           |
|    total_timesteps      | 491520        |
| train/                  |               |
|    approx_kl            | 0.00027487267 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.024        |
|    explained_variance   | 0.167         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0157        |
|    n_updates            | 2390          |
|    policy_gradient_loss | -0.000396     |
|    value_loss           | 0.0299        |
-------------------------------------------
Output 241: Average over 58 episodes - Reward: 0.8620689655172413
------------------------------------------
| time/                   |              |
|    fps                  | 582          |
|    iterations           | 241          |
|    time_elapsed         | 847          |
|    total_timesteps      | 493568       |
| train/                  |              |
|    approx_kl            | 0.0007439206 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0229      |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.016        |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.000425    |
|    value_loss           | 0.034        |
------------------------------------------
Output 242: Average over 49 episodes - Reward: 0.6938775510204082
------------------------------------------
| time/                   |              |
|    fps                  | 582          |
|    iterations           | 242          |
|    time_elapsed         | 851          |
|    total_timesteps      | 495616       |
| train/                  |              |
|    approx_kl            | 0.0003864362 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0269      |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00811      |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 0.0373       |
------------------------------------------
Output 243: Average over 48 episodes - Reward: 0.5833333333333334
-------------------------------------------
| time/                   |               |
|    fps                  | 582           |
|    iterations           | 243           |
|    time_elapsed         | 854           |
|    total_timesteps      | 497664        |
| train/                  |               |
|    approx_kl            | 0.00039080143 |
|    clip_fraction        | 0.00796       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0182       |
|    explained_variance   | 0.216         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.027         |
|    n_updates            | 2420          |
|    policy_gradient_loss | -0.000936     |
|    value_loss           | 0.0366        |
-------------------------------------------
Output 244: Average over 45 episodes - Reward: 0.6666666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 582           |
|    iterations           | 244           |
|    time_elapsed         | 857           |
|    total_timesteps      | 499712        |
| train/                  |               |
|    approx_kl            | 0.00020595797 |
|    clip_fraction        | 0.00439       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0282       |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.021         |
|    n_updates            | 2430          |
|    policy_gradient_loss | -0.00012      |
|    value_loss           | 0.0416        |
-------------------------------------------
Output 245: Average over 48 episodes - Reward: 0.7291666666666666
-------------------------------------------
| time/                   |               |
|    fps                  | 582           |
|    iterations           | 245           |
|    time_elapsed         | 860           |
|    total_timesteps      | 501760        |
| train/                  |               |
|    approx_kl            | 0.00026427206 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.008         |
|    n_updates            | 2440          |
|    policy_gradient_loss | 0.000155      |
|    value_loss           | 0.0338        |
-------------------------------------------
Overall: Average Reward: 0.5911464078241047
